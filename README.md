### Do it! LLMì„ í™œìš©í•œ AIì—ì´ì „íŠ¸ ê°œë°œ ì…ë¬¸ ì±… ë³´ê³  ê³µë¶€í•œ ë‚´ìš©ì„ ê¸°ë¡í•©ë‹ˆë‹¤.

<img width="458" height="626" alt="image" src="https://github.com/user-attachments/assets/5402108c-45a6-428a-999d-cec95ace52b8" />


### 1ì¥ LLMìœ¼ë¡œ ì–´ë–¤ ì¼ì„ í•  ìˆ˜ ìˆì„ê¹Œ?

### 01-1 ì±—GPTë¡œ ì‹œì‘ëœ ìƒì„±í˜• AIì‹œëŒ€

LLMì€ ë¬´ì—‡ì¼ê¹Œ?

- ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM): ë°©ëŒ€í•œ ì–‘ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„± (ex. GPT, ì œë¯¸ë‚˜ì´, í´ë¡œë“œ, ë¼ë§ˆ ..ë“±)

GPT, ì±— GPTì˜ ì°¨ì´

- GPT: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ê·¸ ìì²´
- ì±—GPT: GPTë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•„ìš”í•œ ê¸°ëŠ¥ì„ ë§ë¶™ì—¬ ì œê³µí•˜ëŠ” ì±„íŒ… í˜•íƒœ ì„œë¹„ìŠ¤

LLMì˜ ì¢…ë¥˜

- GPT: ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë¡œ ë°œì „
- ì œë¯¸ë‚˜ì´
- ë¼ë§ˆ
- í´ë¡œë“œ
- ë”¥ì‹œí¬

LLMì„ í™œìš©í•œ ìƒì„±í˜• AIì„œë¹„ìŠ¤ì˜ ì¢…ë¥˜

- ì±—GPT
- í¼í”Œë ‰ì‹œí‹°
- SKT ì—ì´ë‹·: í†µí™” ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ì •ë¦¬
ì˜¨ë¼ì¸ ë¯¸íŒ… íšŒì˜ë¡ ìë™ ì‘ì„± í›„ ìš”ì•½ë³¸ ì´ë©”ì¼ë¡œ ì „ì†¡ ì„œë¹„ìŠ¤
- AI Companion: ì¤Œì—ì„œ ê°œë°œ
- Knoxë¯¸íŒ…: ì‚¼ì„± SDS
- ì¸í”„ëŸ° AI ì¸í„´: ê°•ì˜ ì§ˆë¬¸ì— ë‹µë³€

### 01-2 LLMì„ ì™œ ê³µë¶€í•´ì•¼ í• ê¹Œ?

LLM í”„ë¡œê·¸ë˜ë° ê²½í—˜ì´ í•„ìš”í•œ ì´ìœ  

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•´ ì—…ë¬´ ìë™í™”, ì±—ë´‡ ë§Œë“œëŠ” ê³¼ì •ì„ ì§ì ‘ í•˜ë©´ì„œ ìƒì„±í˜• AIì™€ ì–¸ì–´ ëª¨ë¸ì˜ ì¥ì ê³¼ í•œê³„ ì´í•´í•˜ê³  ì´ë¥¼ í†µí•´ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ê¸°ìˆ ì„ ì•Œì•„ë³´ê³  ì¡°í•©í•´ì„œ ì²´ê³„í™”

ì–´ë–¤ ì–¸ì–´ ëª¨ë¸ì„ ì„ íƒí•´ì•¼í• ê¹Œ?
í˜„ì¬ GPT ì–¸ì–´  ëª¨ë¸ì„ ê°€ì¥ ë§ì´ ì‚¬ìš©

ë³´ì•ˆ, ë¹„ìš©ì„ ê³ ë ¤í•˜ë©´ ì†Œê·œëª¨ ì–¸ì–´ ëª¨ë¸(SLM) ê³ ë ¤ 

LLMì˜ í•œê³„ë¥¼ ë³´ì™„í•˜ëŠ” ê¸°ìˆ  6ê°€ì§€

- í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§: ì–¸ì–´ ëª¨ë¸ì˜ ë‹µë³€ì„ ìµœì í™”í•˜ê¸° ìœ„í•´ ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ê³„
- íŒŒì¸ íŠœë‹: ì´ë¯¸ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì— ì›í•˜ëŠ” ë¶„ì•¼ë‚˜ íŠ¹ì • ìš©ë„ì— ë§ê²Œ ì¶”ê°€ ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê¸°ë²•, ì¼ë°˜ PCì—ì„œ êµ¬ë™x, ëŒ€ë¶€ë¶„ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ê³¼ RAGë¥¼ ì´ìš©í•´ ê°œì„ 
- RAG: í•„ìš”í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ì„œ ë‹µë³€í•  ë•Œ í™œìš©í•˜ë„ë¡ ë•ëŠ” ê¸°ìˆ 
- í‘ì…˜ ì½œë§ê³¼ ë„êµ¬ í˜¸ì¶œ: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ ë‹¨ìˆœ ë‹µë³€ì— ê·¸ì¹˜ì§€ ì•Šê³  ì™¸ë¶€APIë‚˜ ì§ì ‘ ë§Œë“  í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ê·¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê¸°ìˆ 
- ë­ì²´ì¸: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ëŠ” í”„ë ˆì„ì›Œí¬
- ë­ê·¸ë˜í”„: ì—¬ëŸ¬ AI ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ì–´ í˜‘ì—…í•˜ë„ë¡ ì‹œìŠ¤í…œì„ êµ¬ì„±í•˜ëŠ” ë°©ì‹ì¸ ë©€í‹° ì—ì´ì „íŠ¸ë¥¼ êµ¬í˜„í•˜ëŠ” í”„ë ˆì„ ì›Œí¬ ì¤‘ í•˜ë‚˜

### 2ì¥ í™˜ê²½ ì„¤ì •í•˜ê³  GPT API ì‹œì‘í•˜ê¸°

ì˜¤í”ˆAIì˜ API í‚¤ë¡œ ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°

```jsx
pip install openai==1.58.1 // pip install open aië„ OK
```

- ë„·í”Œë¦­ìŠ¤ì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ì˜í™”/ë“œë¼ë§ˆ top10ì„ ì•Œë ¤ì¤˜

```jsx
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)

response = client.chat.completions.create(
  model="gpt-4o",
  temperature=0.1,
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "ë„·í”Œë¦­ìŠ¤ì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ì˜í™”/ë“œë¼ë§ˆ top10ì„ ì•Œë ¤ì¤˜"},
  ]
)

print(response)
print('------')
print(response.choices[0].message.content)

```

```jsx
í˜„ì¬ ì‹œì ì—ì„œ ë„·í”Œë¦­ìŠ¤ì˜ ì¸ê¸° ì½˜í…ì¸ ëŠ” ì§€ì—­ê³¼ ì‹œê°„ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ë„·í”Œë¦­ìŠ¤ëŠ” ì •ê¸°ì ìœ¼ë¡œ ì¸ê¸° ìˆëŠ” ì˜í™”ì™€ ë“œë¼ë§ˆì˜ ìˆœìœ„ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. 
ê°€ì¥ ì •í™•í•œ ì •ë³´ë¥¼ ì–»ìœ¼ë ¤ë©´ ë„·í”Œë¦­ìŠ¤ ì•±ì´ë‚˜ ì›¹ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ì—¬ "Top 10" ì„¹ì…˜ì„ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.    
ì¼ë°˜ì ìœ¼ë¡œ ë„·í”Œë¦­ìŠ¤ì˜ ì¸ê¸° ì½˜í…ì¸ ëŠ” ìƒˆë¡œìš´ ì˜¤ë¦¬ì§€ë„ ì‹œë¦¬ì¦ˆ, í™”ì œì˜ ì˜í™”, ê·¸ë¦¬ê³  ìµœê·¼ì— ì¶”ê°€ëœ ì¸ê¸° ìˆëŠ” ì‘í’ˆë“¤ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. 
íŠ¹ì • ì‹œì ì˜ ì¸ê¸° ì½˜í…ì¸ ë¥¼ ì•Œê³  ì‹¶ë‹¤ë©´ ë„·í”Œë¦­ìŠ¤ì˜ ê³µì‹ ë°œí‘œë‚˜ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¸ê³ í•˜ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤.
```

- (ë²”ìœ„ ì¢íˆê¸°) 2025ë…„ 10ì›”, í•œêµ­ì—ì„œ ê°€ì¥ í‰ì ì´ ë†’ì€ ì˜í™”, ë“œë¼ë§ˆ top10ì„ ì•Œë ¤ì¤˜

```jsx
ì£„ì†¡í•˜ì§€ë§Œ, 2025ë…„ 10ì›”ì˜ ë°ì´í„°ëŠ” í˜„ì¬ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 
ì œ ë°ì´í„°ëŠ” 2023ë…„ 10ì›”ê¹Œì§€ì˜ ì •ë³´ë¡œ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. 
ê·¸ëŸ¬ë‚˜ í•œêµ­ì—ì„œ ì¸ê¸° ìˆëŠ” ì˜í™”ì™€ ë“œë¼ë§ˆë¥¼ ì°¾ìœ¼ì‹œë ¤ë©´, ë„¤ì´ë²„ ì˜í™”, ë‹¤ìŒ ì˜í™”, ë˜ëŠ” ë„·í”Œë¦­ìŠ¤, ì™“ì± ì™€ ê°™ì€ 
ìŠ¤íŠ¸ë¦¬ë° ì„œë¹„ìŠ¤ì˜ ìµœì‹  ìˆœìœ„ë¥¼ ì°¸ê³  í•˜ì‹œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. 
ì´ëŸ¬í•œ í”Œë«í¼ë“¤ì€ ì‚¬ìš©ì ë¦¬ë·°ì™€ í‰ì ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì‹  ì¸ê¸° ì½˜í…ì¸ ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
```

ê´œíˆ ë¬¼ì–´ë´¤ë”°â€¦ 2023ë…„ 10ì›”ê¹Œì§€ ì •ë³´ë§Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆì–´ì„œ ìµœì‹  ì •ë³´ë¥¼ ë¬¼ì–´ë³´ëŠ” ê±´ í•œê³„ê°€ ìˆìŒ!

### 3ì¥ ì˜¤í”ˆAIì˜ APIë¡œ ì±—ë´‡ ë§Œë“¤ê¸°

03-1 í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì•Œì•„ë³´ê¸°

- ì›ìƒ· í”„ë¡¬í”„íŒ…: GPTê°€ ì›í•˜ëŠ” íŒ¨í„´ì— ë§ì¶° ë‹µë³€í•˜ë„ë¡ ì˜ˆì‹œë¥¼ í•œë²ˆ ì œì‹œí•´ì„œ ìœ ë„í•˜ëŠ” ë°©ì‹
- í“¨ì‚¿ í”„ë¡¬í”„íŒ…: ì˜ˆì‹œë¥¼ ì—¬ëŸ¬ ë²ˆ ì•Œë ¤ ì£¼ëŠ” ë°©ì‹

- no prompting

```jsx
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')
client = OpenAI(api_key=api_key)
 
response = client.chat.completions.create(
  model="gpt-4o",
  temperature=0.9, 
  messages=[
    {"role": "system", "content": "ë„ˆëŠ” ìœ ì¹˜ì› í•™ìƒì´ì•¼. ìœ ì¹˜ì›ìƒì²˜ëŸ¼ ë‹µë³€í•´ì¤˜."},
    {"role": "user", "content": "ì˜¤ë¦¬"},
  ]	
)
print(response.choices[0].message.content) 

// ê½¥ê½¥! ì˜¤ë¦¬ ì¬ë°Œì–´! ì˜¤ë¦¬ëŠ” ë¬¼ì—ì„œ í—¤ì—„ì¹˜ê³  ê½¥ê½¥ ì†Œë¦¬ ë‚´ëŠ” ê·€ì—¬ìš´ ë™ë¬¼ì´ì•¼! ë„ˆ ì˜¤ë¦¬ ì¢‹ì•„í•´? ğŸ¦†
```

- one shot prompt

```jsx
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')
client = OpenAI(api_key=api_key)

response = client.chat.completions.create(
  model="gpt-4o",
  temperature=0.9,
  messages=[
    {"role": "system", "content": "ë„ˆëŠ” ìœ ì¹˜ì› í•™ìƒì´ì•¼. ìœ ì¹˜ì›ìƒì²˜ëŸ¼ ë‹µë³€í•´ì¤˜."},
    {"role": "user", "content": "ì°¸ìƒˆ"},
    {"role": "assistant", "content": "ì§¹ì§¹"},
    {"role": "user", "content": "ì˜¤ë¦¬"},
  ]		
)

print(response.choices[0].message.content) 

// ê½¥ê½¥!
```

- ê°•ì•„ì§€: ë©ë©! ê·€ì—¬ì›Œ!
- ì—¬ìš°: ì—¬ìš°ëŠ” ê¼¬ë¦¬ê°€ ê¸¸ê³  ë˜‘ë˜‘í•´! ìƒ‰ê¹”ì€ ì£¼í™©ìƒ‰ì´ì—ìš”, ê·¼ë° í•˜ì–€ ì—¬ìš°ë„ ìˆì–´ìš”! ğŸ¦Š
- ì†Œ: ìŒë©”! ì†ŒëŠ” ìš°ìœ  ì¤˜! ë§›ìˆì–´!
- ë™ë¬¼ì´ ì•„ë‹Œ ê±¸ ì…ë ¥í–ˆì„ ë•Œ
    - ë„ê¹¨ë¹„: ë„ê¹¨ë¹„ëŠ” ë¬´ì„œìš´ ê±° ë§ê³  ì°©í•œ ê²ƒë„ ìˆì–´! ë„ê¹¨ë¹„ ë°©ë§ì´ë¡œ ë‘ë“¤ê¸°ë©´ ë–¡ì´ ë‚˜ì˜¨ë‹¤ê³  í•´! ì¬ë°Œì§€?
- few shot prompt

```jsx
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')
client = OpenAI(api_key=api_key)
 
response = client.chat.completions.create(
  model="gpt-4o",
  temperature=0.9, 
  messages=[
    {"role": "system", "content": "ë„ˆëŠ” ìœ ì¹˜ì› í•™ìƒì´ì•¼. ìœ ì¹˜ì›ìƒì²˜ëŸ¼ ë‹µë³€í•´ì¤˜."},
    {"role": "user", "content": "ì°¸ìƒˆ"},
    {"role": "assistant", "content": "ì§¹ì§¹"},
    {"role": "user", "content": "ë§"},
    {"role": "assistant", "content": "íˆì´ì‰"},
    {"role": "user", "content": "ê°œêµ¬ë¦¬"},
    {"role": "assistant", "content": "ê°œêµ´ê°œêµ´"},
    {"role": "user", "content": "ë±€"},
  ]		
)
print(response.choices[0].message.content) 

//ìŠ¤ë¥´ë¥´ë¥´~
```

- ëŠ‘ëŒ€: ì•„ìš°ìš°~~!
- ë‹­: ê¼¬ë¼ì˜¤! ğŸ”
- ë¹„ë‘˜ê¸°: êµ¬êµ¬! êµ¬êµ¬!

03-2 GPTì™€ ë©€í‹°í„´ ëŒ€í™”í•˜ê¸°

- ë©€í‹°í„´: ì—¬ëŸ¬ ë²ˆ ëŒ€í™”(í„´)í•  ë•Œ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ì ì ˆí•˜ê²Œ ë°˜ì‘í•˜ëŠ” ê²ƒ
- ê³¼ê±° ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µ ëª»í•˜ëŠ” GPT

```jsx
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°

client = OpenAI(api_key=api_key)  # ì˜¤í”ˆAI í´ë¼ì´ì–¸íŠ¸ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

while True:
    user_input = input("ì‚¬ìš©ì: ")

    if user_input == "exit":
        break

    response = client.chat.completions.create(
        model="gpt-4o",
        temperature=0.9,
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."},
            {"role": "user", "content": user_input},
        ],
    )
    print("AI: " + response.choices[0].message.content)

```

```jsx
ì‚¬ìš©ì: ì•ˆë…•? ë‚´ ì´ë¦„ì€ ë¹„ë¹„ì´ê³  ë‚˜ì´ëŠ” 33ì‚´ì´ì•¼
AI: ì•ˆë…•í•˜ì„¸ìš”, ë¹„ë¹„ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?
ì‚¬ìš©ì: ë‚´ ì´ë¦„ì´ ë­ê²Œ?
AI: ë‹¹ì‹ ì˜ ì´ë¦„ì„ ëª¨ë¥´ì§€ë§Œ, ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ì§ˆë¬¸ì´ë‚˜ ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!
ì‚¬ìš©ì: ë‚´ ë‚˜ì´ê°€ ëª‡ì‚´ì´ê²Œ?
AI: ë‹¹ì‹ ì˜ ë‚˜ì´ë¥¼ ì¶”ì¸¡í•˜ê¸° ìœ„í•´ì„œëŠ” ë” ë§ì€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ìƒë…„ì›”ì¼ì´ë‚˜ ë‚˜ì´ì— ëŒ€í•œ íŒíŠ¸ë¥¼ ì£¼ì‹œë©´ ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.
ì‚¬ìš©ì: exit
```

- ë©€í‹°í„´ ëŒ€í™” ë§Œë“¤ê¸°

```jsx
from openai import OpenAI  # ì˜¤í”ˆAI ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ì ¸ì˜¤ê¸°
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°

client = OpenAI(api_key=api_key)  # ì˜¤í”ˆAI í´ë¼ì´ì–¸íŠ¸ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

def get_ai_response(messages):
    response = client.chat.completions.create(
        model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
        temperature=0.9,  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  temperature ì„¤ì •
        messages=messages,  # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
    )
    return response.choices[0].message.content  # ìƒì„±ëœ ì‘ë‹µì˜ ë‚´ìš© ë°˜í™˜

messages = [
    {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."},  # ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€
]

while True:
    user_input = input("ì‚¬ìš©ì: ")  # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°

    if user_input == "exit":  # ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ë ¤ëŠ”ì§€ í™•ì¸ì¸
        break
    
    messages.append({"role": "user", "content": user_input})  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€ 
    ai_response = get_ai_response(messages)  # ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ AI ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°
    messages.append({"role": "assistant", "content": ai_response})  # AI ì‘ë‹µ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•˜ê¸°

    print("AI: " + ai_response)  # AI ì‘ë‹µ ì¶œë ¥

```

```jsx
ì‚¬ìš©ì: ì•ˆë…•? ë‚´ ì´ë¦„ì€ ë¹„ë¹„ì•¼. ë‚˜ì´ëŠ” 33ì‚´ì´ì•¼
AI: ì•ˆë…•í•˜ì„¸ìš”, ë¹„ë¹„ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?
ì‚¬ìš©ì: ë‚´ê°€ ëˆ„êµ¬ê²Œ?
AI: ë¹„ë¹„ë‹˜ì— ëŒ€í•´ ì•„ì§ ì•„ëŠ” ê²ƒì´ ë§ì§€ ì•Šì§€ë§Œ, 33ì‚´ì´ë¼ëŠ” ì •ë³´ì™€ ë°ê³  ì¹œê·¼í•œ ì¸ìƒì„ ì£¼ì‹œëŠ” ê²ƒ ê°™ì•„ìš”. í˜¹ì‹œ íŠ¹ë³„íˆ ê¶ê¸ˆí•œ ê²ƒì´ë‚˜ í•¨ê»˜ ë‚˜ëˆ„ê³  ì‹¶ì€ ì´ì•¼ê¸°ê°€ ìˆì„ê¹Œìš”?
ì‚¬ìš©ì: ë¯¸êµ­ì—ì„œ ì œì¼ ì¸ê¸°ìˆëŠ” ì—°ì˜ˆì¸ì€ ëˆ„êµ¬ì•¼?
AI: ë¯¸êµ­ì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ì—°ì˜ˆì¸ì€ ì‹œê¸°ì™€ ê¸°ì¤€ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆì§€ë§Œ, ëª‡ëª‡ ì´ë¦„ì€ ìì£¼ ê±°ë¡ ë˜ê³¤ í•´ìš”. ì˜ˆë¥¼ ë“¤ì–´, ê°€ìˆ˜ì¸ í…Œì¼ëŸ¬ ìŠ¤ìœ„í”„íŠ¸ì™€ ë¹„ìš˜ì„¸, ë°°ìš°ì¸ ë“œì›¨ì¸ ì¡´ìŠ¨ê³¼ ë ˆì˜¤ë‚˜ë¥´ë„ ë””ì¹´í”„ë¦¬ì˜¤ ë“±ì€ ê¾¸ì¤€íˆ ë†’ì€ ì¸ê¸°ë¥¼ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ë“¤ì˜ ì‘í’ˆ, ê³µë¡œ, ê·¸ë¦¬ê³  ì‚¬íšŒì  ì˜í–¥ë ¥ì´ ì´ëŸ¬í•œ ì¸ê¸°ë¥¼ ë’·ë°›ì¹¨í•˜ì£ . ê´€ì‹¬ ìˆëŠ” íŠ¹ì • ë¶„ì•¼ë‚˜ ìµœê·¼ íŠ¸ë Œë“œë¥¼ ë§ì”€í•´ì£¼ì‹œë©´ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€ ë“œë¦´ ìˆ˜ ìˆì–´ìš”!
ì‚¬ìš©ì: í•œêµ­ì—ì„œëŠ”?     
AI: í•œêµ­ì—ì„œëŠ” K-popê³¼ K-ë“œë¼ë§ˆì˜ ì˜í–¥ìœ¼ë¡œ ë§ì€ ì—°ì˜ˆì¸ë“¤ì´ í° ì¸ê¸°ë¥¼ ì–»ê³  ìˆìŠµë‹ˆë‹¤. K-popì—ì„œëŠ” ë°©íƒ„ì†Œë…„ë‹¨(BTS), ë¸”ë™í•‘í¬, ê·¸ë¦¬ê³  ì•„ì´ìœ  ê°™ì€ ì•„í‹°ìŠ¤íŠ¸ë“¤ì´ ì„¸ê³„ì ìœ¼ë¡œ í° ì¸ê¸°ë¥¼ ëŒê³  ìˆì£ . ë“œë¼ë§ˆë‚˜ ì˜í™” ë¶„ì•¼ì—ì„œëŠ” ë°°ìš° ì´ë³‘í—Œ, ì†¡ì¤‘ê¸°,  ì „ì§€í˜„, ê·¸ë¦¬ê³  ê¹€ìˆ˜í˜„ ë“±ì´ ê¾¸ì¤€íˆ ì‚¬ë‘ë°›ê³  ìˆìŠµë‹ˆë‹¤. í•œêµ­ ì—°ì˜ˆê³„ëŠ” ë³€í™”ê°€ ë¹ ë¥´ê³  ë‹¤ì–‘í•œ ì¬ëŠ¥ ìˆëŠ” ì‚¬ëŒë“¤ì´ ê³„ì†í•´ì„œ  ì£¼ëª©ë°›ê³  ìˆì–´ì„œ, íŠ¹ì • ì‹œê¸°ì— ë”°ë¼ ì¸ê¸° ìˆëŠ” ì¸ë¬¼ë“¤ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´ìš”.
```

03-3 ìŠ¤íŠ¸ë¦¼ë¦¿ìœ¼ë¡œ ì±—ë´‡ ì™„ì„±í•˜ê¸°

<img width="1491" height="1363" alt="image" src="https://github.com/user-attachments/assets/5c80603d-3f86-421b-a328-d77552c9c2ef" />

### 4ì¥ ë¬¸ì„œì™€ ë…¼ë¬¸ì„ ìš”ì•½í•˜ëŠ” AI ì—°êµ¬ì›

### 5ì¥ íšŒì˜ë¡ì„ ì •ë¦¬í•˜ëŠ” AI ì„œê¸°
05-1 ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê¸°

- STT(Speech-To-Text): ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ 
- TTS(Text-To-Speech): STTì™€ ë°˜ëŒ€ë˜ëŠ” ê¸°ìˆ , í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•´ì„œ ì¶œë ¥í•˜ëŠ” ê¸°ìˆ 
- ìœ„ìŠ¤í¼ API í™œìš©í•˜ê¸°
    - MP3ì˜ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

```jsx
Transcription(text='ì•ˆë…•í•˜ì„¸ìš”. ì´ ê°•ì˜ëŠ” GPT APIë¡œ ì±—ë´‡ ë§Œë“¤ê¸°ë¼ëŠ” ë‚´ìš©ì„ ë‹¤ë£¨ëŠ” ê°•ì˜ì…ë‹ˆë‹¤.
 GPT APIì— ëŒ€í•´ì„œ ìƒì†Œí•˜ì‹  ë¶„ë“¤ë„ ìˆì„ í…ë° ìš°ë¦¬ê°€ ì˜ ì•Œê³  ìˆëŠ” 
 ì±„GPT, ì±„GPT ê¸°ëŠ¥ì„ ì´ìš©í•´ì„œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ì–´ë–»ê²Œ ë§Œë“œëŠ”ì§€ì— ëŒ€í•´ì„œ ì´ì•¼ê¸°í•  ê±°ì˜ˆìš”. 
 ê·¸ë˜ì„œ ë­ ì´ëŸ° ê°•ì˜ë“¤ì´ ì‚¬ì‹¤ ë§ì´ ìˆìŠµë‹ˆë‹¤. 
 ê·¸ë˜ì„œ ì—¬ëŸ¬ ê°€ì§€ë“¤ì´ ìˆëŠ”ë° ì¢€ ì´ ê°•ì˜ì˜ íŠ¹ì§•ì´ë¼ê³  í•œë‹¤ë©´ GPTë¡œ ëª…í™•í•œ ë¯¸ì…˜ì„ ë‹¬ì„±í•˜ëŠ” ì±—ë´‡ í”„ë¡œê·¸ë¨ì„
  ë§Œë“œëŠ” ê²Œ ì‚¬ì‹¤ ì‰½ì§€ëŠ” ì•Šì€ë° ì´ê±¸ ì–´ë–»ê²Œ í•´ì„œ êµ¬í˜„ì„ í•˜ëŠ”ì§€ ê·¸ë¦¬ê³  ê·¸ê²Œ ì™œ í•„ìš”í•œì§€ì— ëŒ€í•´ì„œ 
  ì¢€ ì´ì•¼ê¸°ë¥¼ í•  ê±°ê³ ìš”. ê·¸ ì˜ˆì œë¡œ ì˜ˆì œëŠ” ì—¬ëŸ¬ ê°€ì§€ê°€ ë  ìˆ˜ ìˆëŠ”ë° ì—¬ê¸°ì„œ ì˜ˆì œë¡œ í•˜ëŠ” ê²ƒì€
   ìŒì•… í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ë™ì˜ìƒì„ ìë™ìœ¼ë¡œ ëŒ€í™”ë¥¼ í†µí•´ì„œ ìƒì„±í•˜ëŠ” í”„ë¡œê·¸ë¨ ë§Œë“œëŠ” ê²ƒì„ ë‹¤ë£¨ë ¤ê³  í•©ë‹ˆë‹¤. 
   ê·¸ë˜ì„œ í”„ë¡œê·¸ë¨ì´ ì‹¤í–‰ë˜ëŠ” ëª¨ìŠµì„ í•œë²ˆ ë³´ì—¬ë“œë¦´ê²Œìš”. ìš°ë¦¬ê°€ ë§Œë“¤ í”„ë¡œê·¸ë¨ì€ ì´ëŸ° ì‹ìœ¼ë¡œ ì´ì œ ë‚˜íƒ€ë‚˜ê²Œ
    ë˜ê³ ', logprobs=None, usage=UsageDuration(seconds=58.0, type='duration'))
```

   ì±—GPTê°€ ì±„GPTë¡œ ì¶œë ¥ë¨ 

- ìœ„ìŠ¤í¼ APIë¡œ í•œêµ­ì–´ìŒì„± íŒŒì¼ì„ ì˜ì–´ë¡œ ë°”ë¡œ ë²ˆì—­í•˜ê¸°

```jsx
Translation(text="Hello, this is a lecture on how to make a chatbot with GPT 
API. Some of you may be unfamiliar with GPT API. 
We're going to talk about how to make the program we want using 
the chat GPT function that we know well. 
So there are a lot of lectures like this. 
There are many things, but if I were to say the characteristics of this lecture,
 it's not easy to make a chatbot program that achieves a clear mission with GPT.
 I'm going to talk about how to implement this and why it's necessary. 
 As an example, there can be many examples. 
 The example here is to create a program that automatically creates a music
  playlist video through conversation. So let me show you how the program runs.
   The program we're going to make is going to look like this.")
```

05-2 ë¡œì»¬ì—ì„œ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê¸°

- [í—ˆê¹…í˜ì´ìŠ¤](https://huggingface.co/)(Hugging Face): ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” íšŒì‚¬, í—ˆê¹… í˜ì´ìŠ¤ í”Œë«í¼ ì„œë¹„ìŠ¤
- ìœ„ìŠ¤í¼ ëª¨ë¸ì„ ë‚´ë ¤ë°›ì•„ ë¡œì»¬ì—ì„œ ì‚¬ìš©í•˜ê¸°
    - í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ : [whisper-large-v3-turbo](https://huggingface.co/openai/whisper-large-v3-turbo), [FFMPEG](https://www.gyan.dev/ffmpeg/builds/), [íŒŒì´í† ì¹˜](https://pytorch.org/)
    - windowì—ì„œ ì‹¤í–‰ ì‹œ ì œëŒ€ë¡œ ì‹¤í–‰ ì•ˆë¨!

05-3 ë¬¸ì¥ê³¼ í™”ì êµ¬ë¶„í•˜ê¸°

- í™”ì ë¶„ë¦¬ ëª¨ë¸ë¡œ ì‹œê°„ëŒ€ë³„ í™”ì êµ¬ë¶„í•˜ê¸° 
payannote.audio ì´ìš©í•´ì„œ ê°œë°œ

```
%pip install pyannote.audio
%pip install numpy==1.26
```

```python
import os
from dotenv import load_dotenv

load_dotenv()

HUGGING_FACE_TOKEN = os.getenv("HUGGING_FACE_TOKEN")
```

```python
# instantiate the pipeline
from pyannote.audio import Pipeline

pipeline = Pipeline.from_pretrained(
  "pyannote/speaker-diarization-3.1",
  use_auth_token=HUGGING_FACE_TOKEN
)
```

```python
import torch

# cudaê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° cudaë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
if torch.cuda.is_available():
    pipeline.to(torch.device("cuda"))
    print('cuda is available')
else:
    print('cuda is not available')
```

```python
# run the pipeline on an audio file
# diarization = pipeline("audio.wav")
diarization = pipeline("../audio/ì‹¼ê¸°íƒ€_ë¹„ì‹¼ê¸°íƒ€.mp3")

# dump the diarization output to disk using RTTM format
with open("ì‹¼ê¸°íƒ€_ë¹„ì‹¼ê¸°íƒ€.rttm", "w", encoding='utf-8') as rttm:
    diarization.write_rttm(rttm)

```
- [ì‹¤ìŠµ] íŒë‹¤ìŠ¤ë¡œ ë¬¸ì¥ ë¶„ì„í•˜ê³  í™”ì ë§¤ì¹­í•˜ê¸°
whisper_stt_.py ì½”ë“œ ì°¸ê³ 

05-4 íšŒì˜ë¡ì„ ì •ë¦¬í•˜ëŠ” AI ì„œê¸° ì™„ì„±í•˜ê¸°

### 6ì¥ GPT-4oë¥¼ ì´ìš©í•œ AI ì´ë¯¸ì§€ ë¶„ì„ê°€
06-1 GPT ë¹„ì „ì—ê²Œ ì´ë¯¸ì§€ ì„¤ëª… ìš”ì²­í•˜ê¸°

- [ì‹¤ìŠµ] ì¸í„°ë„·ì— ìˆëŠ” ì´ë¯¸ì§€ ì„¤ëª… ìš”ì²­í•˜ê¸°

```python
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ê°€ì ¸ì˜¤ê¸°

client = OpenAI(api_key=api_key)  # ì˜¤í”ˆAI í´ë¼ì´ì–¸íŠ¸ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."},
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://kfcapi.inicis.com/kfcs_api_img/KFCS/goods/DL_2176697_20241015131926683.png",
                },
            },
        ],
    }
]

response = client.chat.completions.create(
    model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
    messages=messages # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
)

response
```

ChatCompletion(id='chatcmpl-CZuictHixM3G9w1xbIbDbivBFmQIf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='ì´ ì´ë¯¸ì§€ëŠ” KFCì˜ ì¹˜í‚¨ì„ ë‹´ê³  ìˆëŠ” ë²„í‚·ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë²„í‚·ì—ëŠ” ë¹¨ê°„ìƒ‰ê³¼ í°ìƒ‰ì˜ ì¤„ë¬´ëŠ¬ê°€ ìˆìœ¼ë©°, ê°€ìš´ë°ì—ëŠ” KFCì˜ ì°½ë¦½ìì˜ ì–¼êµ´ ê·¸ë¦¼ê³¼ "KFC" ë¡œê³ ê°€ ì¸ì‡„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì¹˜í‚¨ì€ ì–‘ë…ì´ ë˜ì–´ ìˆì–´ ë§›ìˆì–´ ë³´ì´ëŠ” ì™¸ê´€ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.ì¹˜í‚¨ ìœ„ì— ì´‰ì´‰í•œ ì–‘ë…ì´ ë®ì—¬ìˆëŠ” ëª¨ìŠµì…ë‹ˆë‹¤. ë²„í‚·ì—ëŠ” "it\'s finger lickin\' good"ë¼ëŠ” ë¬¸êµ¬ë„ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.', 

- [ì‹¤ìŠµ] ë‚´ê°€ ê°€ì§„ ì´ë¯¸ì§€ë¡œ ì„¤ëª… ìš”ì²­í•˜ê¸°

```python
// ì´ë¯¸ì§€ë¥¼ base54ë¡œ ë³€í™˜
import base64

# Function to encode the image
def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")
    
image_path = "../data/images/sajaboys.jpg"

# ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
base64_image = encode_image(image_path)

print(base64_image)
```

```
// base64ë¡œ ë³€í™˜í•œ ì´ë¯¸ì§€ ì„¤ëª… ìš”ì²­
messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."},
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_image}",
                },
            },
        ],
    }
]

response = client.chat.completions.create(
    model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
    messages=messages # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
)

response.choices[0].message.content
```

```python
// ì—¬ëŸ¬ ì´ë¯¸ì§€ ì„¤ëª… ìš”ì²­

seolleung_terrarosa_base64 = encode_image("../data/images/seolleung_terrarosa.jpg")
local_stitch_terrarosa_base64 = encode_image("../data/images/local_stitch_terrarosa.jpg")

messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "ë‘ ì¹´í˜ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."},
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{seolleung_terrarosa_base64}",
                },
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{local_stitch_terrarosa_base64}",
                },
            },
        ],
    }
]

response = client.chat.completions.create(
    model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
    messages=messages # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
)

response.choices[0].message.content
```

- GPT ë¹„ì „ì˜ í•œê³„ ì•Œì•„ë³´ê¸° 
- 2021, 2022ë…„ OECD ê°€ì…êµ­ì˜ ì—°êµ¬ ê°œë°œë¹„ ë¹„êµ ê·¸ë˜í”„ ë¶„ì„ ìš”ì²­

```
oecd_rnd_2021_base64 = encode_image("../data/images/oecd_rnd_2021_large.png")
oecd_rnd_2022_base64 = encode_image("../data/images/oecd_rnd_2022_large.png")

messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "ì²«ë²ˆì§¸ëŠ” 2021ë…„ ë°ì´í„°ì´ê³ , ë‘ë²ˆì§¸ëŠ” 2022ë…„ ë°ì´í„°ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”. ì–´ë–¤ ë³€í™”ê°€ ìˆì—ˆë‚˜ìš”? í•œêµ­ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."},
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{oecd_rnd_2021_base64}",
                },
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{oecd_rnd_2022_base64}",
                },
            },
        ],
    }
]

response = client.chat.completions.create(
    model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
    messages=messages # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
)

response.choices[0].message.content

```

```jsx
'2021ë…„ê³¼ 2022ë…„ ë°ì´í„°ë¥¼ ë¹„êµí–ˆì„ ë•Œ í•œêµ­ì˜ ì—°êµ¬ê°œë°œë¹„ì™€ GDP ëŒ€ë¹„ ì—°êµ¬ê°œë°œë¹„ ë¹„ì¤‘ ë³€í™”ê°€ ëˆˆì— ë•ë‹ˆë‹¤.\n\n1. **ì—°êµ¬ê°œë°œë¹„**: \n   - 2021ë…„: 89,282ë°±ë§Œ ë‹¬ëŸ¬\n   - 2022ë…„: 91,013ë°±ë§Œ ë‹¬ëŸ¬ \n   - **ì¦ê°€**: ì—°êµ¬ê°œë°œë¹„ê°€ ì•½ê°„ ì¦ê°€í–ˆìŠµë‹ˆë‹¤.\n\n2. **GDP ëŒ€ë¹„ ì—°êµ¬ê°œë°œë¹„ ë¹„ì¤‘**: \n   - 2021ë…„ì—ëŠ” 4.93%ì˜€ë˜ ë¹„ì¤‘ì´ 2022ë…„ì—ëŠ” 5.21%ë¡œ ì¦ê°€í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” í•œêµ­ì´ GDP ëŒ€ë¹„ ì—°êµ¬ê°œë°œ íˆ¬ìì— ë”ìš± ë§ì€ ë¹„ì¤‘ì„ ë‘ê³  ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n\nì´ëŸ¬í•œ ë³€í™”ëŠ” í•œêµ­ì´ ì—°êµ¬ê°œë°œ ë¶„ì•¼ì— ëŒ€í•œ íˆ¬ìë¥¼ ì§€ì†ì ìœ¼ë¡œ í™•ëŒ€í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚´ë©°, ê¸°ìˆ  ë°œì „ê³¼ í˜ì‹ ì„ í†µí•œ ê²½ì œ ì„±ì¥ì— ì§‘ì¤‘í•˜ê³  ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.'
```

í•´ìƒë„ê°€ ì¡°ê¸ˆ ë” ë‚®ì€ ì´ë¯¸ì§€ íŒŒì¼ ì‚¬ìš©

```python
oecd_rnd_2021_base64 = encode_image("../data/images/oecd_rnd_2021_medium.png")
oecd_rnd_2022_base64 = encode_image("../data/images/oecd_rnd_2022.png")

messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "ì²«ë²ˆì§¸ëŠ” 2021ë…„ ë°ì´í„°ì´ê³ , ë‘ë²ˆì§¸ëŠ” 2022ë…„ ë°ì´í„°ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”. ì–´ë–¤ ë³€í™”ê°€ ìˆì—ˆë‚˜ìš”? í•œêµ­ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."},
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{oecd_rnd_2021_base64}",
                },
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{oecd_rnd_2022_base64}",
                },
            },
        ],
    }
]

response = client.chat.completions.create(
    model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
    messages=messages # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
)

response.choices[0].message.content

```

```jsx
'ë‘ ì´ë¯¸ì§€ì—ì„œ 2021ë…„ê³¼ 2022ë…„ì˜ ì£¼ìš” ë°ì´í„°ë¥¼ ë¹„êµí•˜ë©´ì„œ í•œêµ­ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n1. **ì—°êµ¬ê°œë°œë¹„ (R&D ì§€ì¶œ)**:\n   - **2021ë…„**: í•œêµ­ì˜ ì—°êµ¬ê°œë°œë¹„ëŠ” ì•½ 121,739ë°±ë§Œ USDì˜€ìŠµë‹ˆë‹¤.\n   - **2022ë…„**: í•œêµ­ì˜ ì—°êµ¬ê°œë°œë¹„ëŠ” ì•½ 133,867ë°±ë§Œ USDë¡œ ì¦ê°€í–ˆìŠµë‹ˆë‹¤.\n   - ì´ëŠ” í•œêµ­ì˜ ì—°êµ¬ê°œë°œì— ëŒ€í•œ íˆ¬ì ê·œëª¨ê°€ ì¦ê°€í–ˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n\n2. **GDP ëŒ€ë¹„ ì—°êµ¬ê°œë°œë¹„ ë¹„ì¤‘**:\n   - **2021ë…„**: í•œêµ­ì˜ GDP ëŒ€ë¹„ ì—°êµ¬ê°œë°œë¹„ ë¹„ì¤‘ì€ 4.93%ì˜€ìŠµë‹ˆë‹¤.\n   - **2022ë…„**: í•œêµ­ì˜ GDP ëŒ€ë¹„ ì—°êµ¬ê°œë°œë¹„ ë¹„ì¤‘ì€ 5.21%ë¡œ ì¦ê°€í–ˆìŠµë‹ˆë‹¤.\n   - ì´ëŠ” ê²½ì œ ê·œëª¨ì— ë¹„í•´ ì—°êµ¬ê°œë°œ íˆ¬ì ë¹„ìœ¨ì´ ë†’ì•„ì¡Œë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n\n3. **ì£¼ìš” ë³€í™” ì‚¬í•­**:\n   - í•œêµ­ì˜ ì—°êµ¬ê°œë°œë¹„ì™€ ë¹„ì¤‘ ëª¨ë‘ ì¦ê°€í•˜ì—¬, ê²½ì œ ì„±ì¥ê³¼ í•¨ê»˜ ê¸°ìˆ  ë° í˜ì‹ ì— ëŒ€í•œ êµ­ê°€ì  ìš°ì„ ìˆœìœ„ê°€ ë†’ì•„ì¡ŒìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n   - ì„¸ê³„ì ìœ¼ë¡œ ë¯¸êµ­ê³¼ ì¤‘êµ­ë„ ì—°êµ¬ê°œë°œë¹„ ì§€ì¶œì´ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.\n\nì´ëŸ° ë³€í™”ëŠ” í•œêµ­ì´ ê¸°ìˆ  í˜ì‹ ê³¼ ê³¼í•™ ì—°êµ¬ë¥¼ í†µí•´ ë¯¸ë˜ ì„±ì¥ ë™ë ¥ì„ í™•ë³´í•˜ë ¤ëŠ” ë…¸ë ¥ì„ ë°˜ì˜í•œ ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
```

- GPTì˜ ì´ë¯¸ì§€ ì¸ì‹ ê¸°ëŠ¥ì€ ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ ë¶„ì„ì—” ì ì ˆí•  ìˆ˜ ìˆìœ¼ë‚˜ ê·¸ë˜í”„ë¥¼ í•´ì„í•˜ê±°ë‚˜ í™˜ì CTì‚¬ì§„ì—ì„œ ì§ˆë³‘ì„ ê°ì§€í•˜ëŠ” ë“± ê³ ì°¨ì›ì ì¸ ëª©ì ì—ëŠ” ë¶€ì í•©
- ë™ì¼ ì´ë¯¸ì§€ì¸ë°ë„ ì´ë¯¸ì§€ í¬ê¸°ì— ë”°ë¼ ì™„ì „ ì˜ëª» ë‹µë³€í•¨

06-2 ì´ë¯¸ì§€ë¥¼ í™œìš©í•´ í€´ì¦ˆ ë§Œë“¤ê¸°

- [ì‹¤ìŠµ] ë¬¸ì œ ìƒì„± í•¨ìˆ˜ ë§Œë“¤ê¸°

```python
from glob import glob
from openai import OpenAI
from dotenv import load_dotenv
import os
import base64

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY") 
client = OpenAI(api_key=api_key)

# ì´ë¯¸ì§€ íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©í•˜ëŠ” í•¨ìˆ˜
def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")
    

def image_quiz(image_path):
    base64_image = encode_image(image_path) # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©

    quiz_prompt = """
    ì œê³µëœ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ì–‘ì‹ìœ¼ë¡œ í€´ì¦ˆë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. 
    ì •ë‹µì€ 1~4 ì¤‘ í•˜ë‚˜ë§Œ í•´ë‹¹í•˜ë„ë¡ ì¶œì œí•˜ì„¸ìš”.
    ì•„ë˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤. 
    ----- ì˜ˆì‹œ -----

    Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?
    - (1) ë² ì´ì»¤ë¦¬ì—ì„œ ì‚¬ëŒë“¤ì´ ë¹µì„ ì‚¬ê³  ìˆëŠ” ëª¨ìŠµì´ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.
    - (2) ë§¨ ì•ì— ì„œ ìˆëŠ” ì‚¬ëŒì€ ë¹¨ê°„ìƒ‰ ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.
    - (3) ê¸°ì°¨ë¥¼ íƒ€ê¸° ìœ„í•´ ì¤„ì„ ì„œ ìˆëŠ” ì‚¬ëŒë“¤ì´ ìˆìŠµë‹ˆë‹¤.
    - (4) ì ì›ì€ ë…¸ë€ìƒ‰ í‹°ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.
        
    ì •ë‹µ: (4) ì ì›ì€ ë…¸ë€ìƒ‰ í‹°ì…”ì¸ ê°€ ì•„ë‹Œ íŒŒë€ìƒ‰ í‹°ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.
    (ì£¼ì˜: ì •ë‹µì€ 1~4 ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒë˜ë„ë¡ ì¶œì œí•˜ì„¸ìš”.)
    ======
    """

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": quiz_prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}",
                    },
                },
            ],
        }
    ]

    response = client.chat.completions.create(
        model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì§€ì •
        messages=messages # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
    )

    return response.choices[0].message.content

q = image_quiz("./chap06/data/images/busan_dive.jpg")
print(q)
```

Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

- (1) ë§ì€ ì‚¬ëŒë“¤ì´ í…Œì´ë¸”ì— ì•‰ì•„ ë…¸íŠ¸ë¶ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- (2) ì²œì¥ì— ì—¬ëŸ¬ ê°œì˜ ì¡°ëª…ì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- (3) ì´ë¯¸ì§€ ì†ì—ëŠ” "DIVE 2024 IN BUSAN"ì´ë¼ëŠ” ë¬¸êµ¬ê°€ ë³´ì…ë‹ˆë‹¤.
- (4) ì‚¬ëŒë“¤ì´ ì±…ì„ ì½ê³  ìˆëŠ” ëª¨ìŠµì´ ì£¼ë¡œ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.

ì •ë‹µ: (4) ì‚¬ëŒë“¤ì´ ì£¼ë¡œ ë…¸íŠ¸ë¶ì„ ì‚¬ìš©í•˜ê³  ìˆëŠ” ëª¨ìŠµì´ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.

- ì—¬ëŸ¬ ì´ë¯¸ì§€ë¡œ ë¬¸ì œì§‘ ë§Œë“¤ê¸°

```
from glob import glob
from openai import OpenAI
from dotenv import load_dotenv
import os
import base64

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY") 
client = OpenAI(api_key=api_key)

# ì´ë¯¸ì§€ íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©í•˜ëŠ” í•¨ìˆ˜
def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")
    

def image_quiz(image_path):
    base64_image = encode_image(image_path) # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©

    quiz_prompt = """
    ì œê³µëœ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ì–‘ì‹ìœ¼ë¡œ í€´ì¦ˆë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. 
    ì •ë‹µì€ 1~4 ì¤‘ í•˜ë‚˜ë§Œ í•´ë‹¹í•˜ë„ë¡ ì¶œì œí•˜ì„¸ìš”.
    ì•„ë˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤. 
    ----- ì˜ˆì‹œ -----

    Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?
    - (1) ë² ì´ì»¤ë¦¬ì—ì„œ ì‚¬ëŒë“¤ì´ ë¹µì„ ì‚¬ê³  ìˆëŠ” ëª¨ìŠµì´ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.
    - (2) ë§¨ ì•ì— ì„œ ìˆëŠ” ì‚¬ëŒì€ ë¹¨ê°„ìƒ‰ ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.
    - (3) ê¸°ì°¨ë¥¼ íƒ€ê¸° ìœ„í•´ ì¤„ì„ ì„œ ìˆëŠ” ì‚¬ëŒë“¤ì´ ìˆìŠµë‹ˆë‹¤.
    - (4) ì ì›ì€ ë…¸ë€ìƒ‰ í‹°ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.
        
    ì •ë‹µ: (4) ì ì›ì€ ë…¸ë€ìƒ‰ í‹°ì…”ì¸ ê°€ ì•„ë‹Œ íŒŒë€ìƒ‰ í‹°ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.
    (ì£¼ì˜: ì •ë‹µì€ 1~4 ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒë˜ë„ë¡ ì¶œì œí•˜ì„¸ìš”.)
    ======
    """

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": quiz_prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}",
                    },
                },
            ],
        }
    ]

    response = client.chat.completions.create(
        model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì§€ì •
        messages=messages # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
    )

    return response.choices[0].message.content

# ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë°›ì•„ í€´ì¦ˆë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜
q = image_quiz("./chap06/data/images/busan_dive.jpg")
print(q)

txt = '' # â‘   ë¬¸ì œë“¤ì„ ê³„ì† ë¶™ì—¬ ë‚˜ê°€ê¸° ìœ„í•´ ë¹ˆ ë¬¸ìì—´ ì„ ì–¸
no = 1 # ë¬¸ì œ ë²ˆí˜¸ë¥¼ ìœ„í•´ ì„ ì–¸
for g in glob('./chap06/data/images/*.jpg'):  # â‘¡
    try:
        q = image_quiz(g) # ë¬¸ì œ ì¶œì œ (ê°€ë” GPTì—ì„œ ì—ëŸ¬ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ tryë¬¸ ì‚¬ìš©)
    except Exception as e:
        print(e)
        continue

    divider = f'## ë¬¸ì œ {no}\n\n'
    print(divider)
    
    txt += divider  # â‘¢
    # íŒŒì¼ëª… ì¶”ì¶œí•´ ì´ë¯¸ì§€ ë§í¬ ë§Œë“¤ê¸°
    filename = os.path.basename(g) # â‘¢ ë§ˆí¬ë‹¤ìš´ì— í‘œì‹œí•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ì„¤ì •   
    txt += f'![image]({filename})\n\n' # â‘¢

    # ë¬¸ì œ ì¶”ê°€
    print(q)
    txt += q + '\n\n---------------------\n\n'
    # â‘£ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥
    with open('./chap06/data/images/image_quiz.md', 'w', encoding='utf-8') as f:
        f.write(txt)
    
    no += 1 # ë¬¸ì œ ë²ˆí˜¸ ì¦ê°€

```

Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

- (1) ë§ì€ ì‚¬ëŒë“¤ì´ ì±…ìƒì— ì•‰ì•„ ì»´í“¨í„° ì‘ì—…ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- (2) ì²œì¥ì—ëŠ” ì—¬ëŸ¬ ê°œì˜ ì¡°ëª…ì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- (3) ë°°ê²½ì—ëŠ” "DIVE 2024 IN BUSAN"ì´ë¼ëŠ” ê¸€ìê°€ ë³´ì…ë‹ˆë‹¤.
- (4) ëª¨ë“  ì‚¬ëŒì´ ë™ì¼í•œ ìƒ‰ì˜ ì˜·ì„ ì…ê³  ìˆìŠµë‹ˆë‹¤.

ì •ë‹µ: (4) ëª¨ë“  ì‚¬ëŒì´ ë™ì¼í•œ ìƒ‰ì˜ ì˜·ì„ ì…ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ìƒ‰ì˜ ì˜·ì„ ì…ê³  ìˆìŠµë‹ˆë‹¤.

## ë¬¸ì œ 1

Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

- (1) ë§ì€ ì‚¬ëŒë“¤ì´ ì±…ìƒì— ì•‰ì•„ ë¬´ì–¸ê°€ë¥¼ ì‘ì—…í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- (2) ì²œì¥ì— ì¡°ëª…ì´ ì—¬ëŸ¬ ê°œ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- (3) í–‰ì‚¬ ì´ë¦„ì´ "DIVE 2024 in BUSAN"ì´ë¼ê³  ì í˜€ ìˆìŠµë‹ˆë‹¤.
- (4) ì‚¬ëŒë“¤ì´ ì±…ì„ ì½ê³  ìˆëŠ” ëª¨ìŠµì´ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.

ì •ë‹µ: (4) ì‚¬ëŒë“¤ì´ ì±…ì„ ì½ê³  ìˆëŠ” ëª¨ìŠµì´ ì•„ë‹Œ, ì»´í“¨í„°ë¥¼ ì‚¬ìš©í•˜ê³  ìˆëŠ” ëª¨ìŠµì´ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.

## ë¬¸ì œ 2

Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

- (1) ì„¸ ëª…ì˜ ìºë¦­í„°ê°€ ì†ŒíŒŒì— ì•‰ì•„ ìˆìŠµë‹ˆë‹¤.
- (2) ê°€ìš´ë° ìºë¦­í„°ëŠ” ì§§ì€ ë¨¸ë¦¬ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- (3) í…Œì´ë¸” ìœ„ì—ëŠ” ë‹¤ì–‘í•œ ìŒì‹ì´ ë†“ì—¬ ìˆìŠµë‹ˆë‹¤.
- (4) ì˜¤ë¥¸ìª½ ìºë¦­í„°ëŠ” ë…¸ë€ìƒ‰ ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.

ì •ë‹µ: (4) ì˜¤ë¥¸ìª½ ìºë¦­í„°ëŠ” ë…¸ë€ìƒ‰ ì…”ì¸ ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ìƒ‰ì˜ ì˜ìƒì„ ì…ê³  ìˆìŠµë‹ˆë‹¤.

## ë¬¸ì œ 3

Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

- (1) í° ë…¸ë€ìƒ‰ ì¡°í˜•ë¬¼ì´ ë³´ì…ë‹ˆë‹¤.
- (2) ê±´ë¬¼ì— "Local Stitch"ë¼ëŠ” ê¸€ìê°€ ì“°ì—¬ ìˆìŠµë‹ˆë‹¤.
- (3) ê²€ì€ìƒ‰ ì˜ìê°€ ìˆìŠµë‹ˆë‹¤.
- (4) ì‚¬ëŒë“¤ì´ ë²½ì— ì„œì„œ í¬ì¦ˆë¥¼ ì·¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ì •ë‹µ: (4) ì‚¬ëŒë“¤ì€ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.

## ë¬¸ì œ 4

Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

- (1) ì¹´í˜ ë‚´ë¶€ì— ì—¬ëŸ¬ ê°œì˜ í…Œì´ë¸”ê³¼ ì˜ìê°€ ìˆìŠµë‹ˆë‹¤.
- (2) ë°”ë¦¬ìŠ¤íƒ€ê°€ ìŒë£Œë¥¼ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- (3) ë°”ë‹¥ì€ íŒŒë€ìƒ‰ê³¼ ë¹¨ê°„ìƒ‰ íƒ€ì¼ë¡œ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- (4) ì¹´ìš´í„° ë’¤ë²½ì€ ë…¸ë€ìƒ‰ì…ë‹ˆë‹¤.

ì •ë‹µ: (3) ë°”ë‹¥ì€ íŒŒë€ìƒ‰ê³¼ ë¹¨ê°„ìƒ‰ íƒ€ì¼ì´ ì•„ë‹Œ, í°ìƒ‰ê³¼ ë¹¨ê°„ìƒ‰ íƒ€ì¼ë¡œ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ë¬¸ì œ 5

Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

- (1) ê±´ë¬¼ ì™¸ê´€ì€ ë²½ëŒë¡œ ë§ˆê°ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- (2) 1ì¸µ ì°½ë¬¸ì—ëŠ” "PERSONAL COFFEE"ë¼ëŠ” ë¬¸êµ¬ê°€ ë³´ì…ë‹ˆë‹¤.
- (3) ê±´ë¬¼ ì•ì— ì£¼í™©ìƒ‰ ì›ë¿” ëª¨ì–‘ì˜ ì¥ì• ë¬¼ì´ ìˆìŠµë‹ˆë‹¤.
- (4) ê±´ë¬¼ì€ ë‹¨ì¸µ êµ¬ì¡°ë¡œ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì •ë‹µ: (4) ê±´ë¬¼ì€ ë‹¨ì¸µ êµ¬ì¡°ê°€ ì•„ë‹Œ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ë¬¸ì œ 6

Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

- (1) ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ë¹µì´ ì§„ì—´ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- (2) ì§„ì—´ëŒ€ ìœ„ì—ëŠ” ê°€ê²©í‘œê°€ ë¶€ì°©ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- (3) ë² ì´ì»¤ë¦¬ ì§ì›ë“¤ì€ ëª¨ìë¥¼ ì“°ê³  ìˆìŠµë‹ˆë‹¤.
- (4) ì˜¤ë¥¸ìª½ì—ëŠ” ì¼€ì´í¬ê°€ ì§„ì—´ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì •ë‹µ: (4) ì˜¤ë¥¸ìª½ì—ëŠ” ì¼€ì´í¬ê°€ ì•„ë‹Œ ë¹µì´ ì§„ì—´ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ë¬¸ì œ 7

I'm unable to create questions about the identities of people in the image, but I can assist with general descriptions. Hereâ€™s a quiz based on the image itself:

Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

- (1) ë‹¤ì„¯ ëª…ì˜ ì‚¬ëŒë“¤ì´ í¬ì¦ˆë¥¼ ì·¨í•˜ê³  ìˆëŠ” ëª¨ìŠµì´ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.
- (2) ê°€ìš´ë° ìˆëŠ” ì‚¬ëŒì€ í° ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.
- (3) ì‚¬ëŒë“¤ ëª¨ë‘ ë°”ì§€ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.
- (4) ëª¨ë“  ì‚¬ëŒì˜ ë¨¸ë¦¬ìƒ‰ì´ ìì—° ê°ˆìƒ‰ì…ë‹ˆë‹¤.

ì •ë‹µ: (4) ëª¨ë“  ì‚¬ëŒì˜ ë¨¸ë¦¬ìƒ‰ì´ ìì—° ê°ˆìƒ‰ì´ ì•„ë‹ˆë¼ ë‹¤ì–‘í•œ ìƒ‰ìƒì…ë‹ˆë‹¤.

## ë¬¸ì œ 8

Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

- (1) "bakery cafe pomme verte"ë¼ëŠ” ê°„íŒì´ ë³´ì…ë‹ˆë‹¤.
- (2) ì‘ì—…ìë“¤ì´ ê³µì‚¬ë¥¼ í•˜ê³  ìˆëŠ” ì¥ë©´ì…ë‹ˆë‹¤.
- (3) ì˜¤ë¥¸ìª½ ìƒì ì˜ ì´ë¦„ì€ "ë‰´íŠ¸ë¦¬ì½”ì–´"ì…ë‹ˆë‹¤.
- (4) ì´ë¯¸ì§€ì—ëŠ” ì‚¬ëŒë“¤ì´ ë„ë¡œë¥¼ ê±´ë„ˆê°€ê³  ìˆëŠ” ëª¨ìŠµì´ ë‚˜ì˜µë‹ˆë‹¤.

ì •ë‹µ: (4) ì´ë¯¸ì§€ì—ëŠ” ì‚¬ëŒë“¤ì´ ë„ë¡œë¥¼ ê±´ë„ˆê°€ê³  ìˆëŠ” ëª¨ìŠµì´ ë‚˜ì˜¤ì§€ ì•ŠìŠµë‹ˆë‹¤.

## ë¬¸ì œ 9

---

Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

- (1) ì—¬ëŸ¬ ì‚¬ëŒë“¤ì´ í…Œì´ë¸”ì— ì•‰ì•„ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ê³  ìˆìŠµë‹ˆë‹¤.
- (2) ì²œì¥ì— ë§ì€ ì¡°ëª…ì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- (3) ë°”ë‹¥ì€ ì§™ì€ ìƒ‰ì˜ ì¹´í«ìœ¼ë¡œ ë®ì—¬ ìˆìŠµë‹ˆë‹¤.
- (4) ì°½ë¬¸ìœ¼ë¡œ ë°”ê¹¥ì˜ í’ê²½ì´ ë³´ì…ë‹ˆë‹¤.

ì •ë‹µ: (3) ë°”ë‹¥ì€ ì§™ì€ ìƒ‰ì˜ ì¹´í«ì´ ì•„ë‹Œ ë‚˜ë¬´ ë°”ë‹¥ì…ë‹ˆë‹¤.

---

## ë¬¸ì œ 10

Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

- (1) ë§ì€ ì‚¬ëŒë“¤ì´ ë²„ìŠ¤ ì˜†ì— ëª¨ì—¬ ìˆìŠµë‹ˆë‹¤.
- (2) ì°½ë¬¸ì— 'CAMERA'ë¼ëŠ” ê¸€ìê°€ ë³´ì…ë‹ˆë‹¤.
- (3) ë‚˜ë¬´ë“¤ì´ ìì‚¬ê·€ë¥¼ í’ì„±í•˜ê²Œ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
- (4) í•œ ì‚¬ëŒì´ ì „í™” í†µí™”ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ì •ë‹µ: (2) ì°½ë¬¸ì— 'CAMERA'ë¼ëŠ” ê¸€ìê°€ ì•„ë‹Œ 'COFFEE & BAKERY'ë¼ëŠ” ê¸€ìê°€ ë³´ì…ë‹ˆë‹¤.

- ì˜ì–´ë¡œ ë¬¸ì œ ì¶œì œí•˜ê¸°

```python

from glob import glob 
from openai import OpenAI
from dotenv import load_dotenv
import os
import base64

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY") 
client = OpenAI(api_key=api_key)  
def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")
    

def image_quiz(image_path, n_trial=0, max_trial=3): # ì˜¤í”ˆAI API í˜¸ì¶œì— ì‹¤íŒ¨í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì¬ê·€ì ìœ¼ë¡œ ì¬ì‹œë„
    if n_trial >= max_trial: # ìµœëŒ€ ì‹œë„ íšŒìˆ˜ì— ë„ë‹¬í•˜ë©´ í¬ê¸°
        raise Exception("Failed to generate a quiz.")
    
    base64_image = encode_image(image_path) # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©

    quiz_prompt = """
    ì œê³µëœ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ì–‘ì‹ìœ¼ë¡œ í€´ì¦ˆë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. 
    ì •ë‹µì€ 1~4 ì¤‘ í•˜ë‚˜ë§Œ í•´ë‹¹í•˜ë„ë¡ ì¶œì œí•˜ì„¸ìš”.
    í† ìµ ë¦¬ìŠ¤ë‹ ë¬¸ì œ ìŠ¤íƒ€ì¼ë¡œ ë¬¸ì œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
    ì•„ë˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤. 
    ----- ì˜ˆì‹œ -----

    Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?
    - (1) ë² ì´ì»¤ë¦¬ì—ì„œ ì‚¬ëŒë“¤ì´ ë¹µì„ ì‚¬ê³  ìˆëŠ” ëª¨ìŠµì´ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.
    - (2) ë§¨ ì•ì— ì„œ ìˆëŠ” ì‚¬ëŒì€ ë¹¨ê°„ìƒ‰ ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.
    - (3) ê¸°ì°¨ë¥¼ íƒ€ê¸° ìœ„í•´ ì¤„ì„ ì„œ ìˆëŠ” ì‚¬ëŒë“¤ì´ ìˆìŠµë‹ˆë‹¤.
    - (4) ì ì›ì€ ë…¸ë€ìƒ‰ í‹°ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.

    Listening: Which of the following descriptions of the image is incorrect?
    - (1) It shows people buying bread at a bakery.
    - (2) The person standing at the front is wearing a red shirt.
    - (3) There are people lining up to take a train.
    - (4) The clerk is wearing a yellow T-shirt.
        
    ì •ë‹µ: (4) ì ì›ì€ ë…¸ë€ìƒ‰ í‹°ì…”ì¸ ê°€ ì•„ë‹Œ íŒŒë€ìƒ‰ í‹°ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.
    (ì£¼ì˜: ì •ë‹µì€ 1~4 ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒë˜ë„ë¡ ì¶œì œí•˜ì„¸ìš”.)
    ======
    """

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": quiz_prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}",
                    },
                },
            ],
        }
    ]

    try: 
        response = client.chat.completions.create(
            model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
            messages=messages # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
        )
    except Exception as e:
        print("failed\n" + e)
        return image_quiz(image_path, n_trial+1)
    
    content = response.choices[0].message.content

    if "Listening:" in content:
        return content, True
    else:
        return image_quiz(image_path, n_trial+1)

q = image_quiz("./chap06/data/images/busan_dive.jpg")
print(q)

txt = '' # â‘   ë¬¸ì œë“¤ì„ ê³„ì† ë¶™ì—¬ ë‚˜ê°€ê¸° ìœ„í•´ ë¹ˆ ë¬¸ìì—´ ì„ ì–¸
no = 1 # ë¬¸ì œ ë²ˆí˜¸ë¥¼ ìœ„í•´ ì„ ì–¸
for g in glob('./chap06/data/images/*.jpg'):  # â‘¡
    q, is_suceed = image_quiz(g)

    if not is_suceed:
        continue

    divider = f'## ë¬¸ì œ {no}\n\n'
    print(divider)
    
    txt += divider  # â‘¢
    # íŒŒì¼ëª… ì¶”ì¶œí•´ ì´ë¯¸ì§€ ë§í¬ ë§Œë“¤ê¸°
    filename = os.path.basename(g) # â‘¢ ë§ˆí¬ë‹¤ìš´ì— í‘œì‹œí•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ì„¤ì •   
    txt += f'![image]({filename})\n\n' # â‘¢

    # ë¬¸ì œ ì¶”ê°€
    print(q)
    txt += q + '\n\n---------------------\n\n'
    # â‘£ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥
    with open('./chap06/data/images/image_quiz_eng.md', 'w', encoding='utf-8') as f:
        f.write(txt)
    
    no += 1 # ë¬¸ì œ ë²ˆí˜¸ ì¦ê°€

```

```markdown
## ë¬¸ì œ 1

![image](busan_dive.jpg)

Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

- (1) í–‰ì‚¬ì¥ ë‚´ë¶€ì— ë§ì€ ì‚¬ëŒë“¤ì´ ì•‰ì•„ ìˆìŠµë‹ˆë‹¤.
- (2) ì¤‘ì•™ì— ìˆëŠ” í° ìŠ¤í¬ë¦°ì€ "DIVE 2024 IN BUSAN"ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.
- (3) ëª¨ë“  ì°¸ê°€ìë“¤ì´ ë…¸ë€ìƒ‰ í‹°ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.
- (4) í–‰ì‚¬ì¥ì€ ì‹¤ë‚´ì— ìˆìŠµë‹ˆë‹¤.

Listening: Which of the following descriptions of the image is incorrect?

- (1) Many people are seated inside the venue.
- (2) The large screen in the center displays "DIVE 2024 IN BUSAN."
- (3) All participants are wearing yellow T-shirts.
- (4) The event is held indoors.

ì •ë‹µ: (3) ëª¨ë“  ì°¸ê°€ìë“¤ì´ ë…¸ë€ìƒ‰ í‹°ì…”ì¸ ë¥¼ ì…ê³  ìˆëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤.

---------------------

## ë¬¸ì œ 2

![image](kdemon.jpg)

Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?
- (1) ì„¸ ëª…ì˜ ì‚¬ëŒì´ ì†ŒíŒŒì— ì•‰ì•„ ìˆìŠµë‹ˆë‹¤.
- (2) í…Œì´ë¸” ìœ„ì—ëŠ” ë‹¤ì–‘í•œ ìŒì‹ì´ ë†“ì—¬ ìˆìŠµë‹ˆë‹¤.
- (3) í•œ ì‚¬ëŒì€ ë…¸ë€ìƒ‰ ì¬í‚·ì„ ì…ê³  ìˆìŠµë‹ˆë‹¤.
- (4) ë°°ê²½ì— ì°½ë¬¸ì´ í¬ê²Œ ë³´ì…ë‹ˆë‹¤.

Listening: Which of the following descriptions of the image is incorrect?
- (1) Three people are sitting on the couch.
- (2) A variety of food is placed on the table.
- (3) One person is wearing a yellow jacket.
- (4) A large window is prominently seen in the background.

ì •ë‹µ: (4) ë°°ê²½ì— ì°½ë¬¸ì´ í¬ê²Œ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.
....
```

- [ì‹¤ìŠµ] TTSë¡œ ì˜ì–´ ë“£ê¸° í‰ê°€ ë¬¸ì œ ë§Œë“¤ê¸°

```python
from glob import glob 
import json
from openai import OpenAI
from dotenv import load_dotenv
import os
import base64

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY") 
client = OpenAI(api_key=api_key)  

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")
    

def image_quiz(image_path, n_trial=0, max_trial=3):
    if n_trial >= max_trial: # ìµœëŒ€ ì‹œë„ íšŒìˆ˜ì— ë„ë‹¬í•˜ë©´ í¬ê¸°
        raise Exception("Failed to generate a quiz.")
    
    base64_image = encode_image(image_path) # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©

    quiz_prompt = """
    ì œê³µëœ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ì–‘ì‹ìœ¼ë¡œ í€´ì¦ˆë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. 
    ì •ë‹µì€ 1~4 ì¤‘ í•˜ë‚˜ë§Œ í•´ë‹¹í•˜ë„ë¡ ì¶œì œí•˜ì„¸ìš”.
    í† ìµ ë¦¬ìŠ¤ë‹ ë¬¸ì œ ìŠ¤íƒ€ì¼ë¡œ ë¬¸ì œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
    ì•„ë˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤. 
    ----- ì˜ˆì‹œ -----

    Q: ë‹¤ìŒ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?
    - (1) ë² ì´ì»¤ë¦¬ì—ì„œ ì‚¬ëŒë“¤ì´ ë¹µì„ ì‚¬ê³  ìˆëŠ” ëª¨ìŠµì´ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.
    - (2) ë§¨ ì•ì— ì„œ ìˆëŠ” ì‚¬ëŒì€ ë¹¨ê°„ìƒ‰ ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.
    - (3) ê¸°ì°¨ë¥¼ íƒ€ê¸° ìœ„í•´ ì¤„ì„ ì„œ ìˆëŠ” ì‚¬ëŒë“¤ì´ ìˆìŠµë‹ˆë‹¤.
    - (4) ì ì›ì€ ë…¸ë€ìƒ‰ í‹°ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.

    Listening: Which of the following descriptions of the image is incorrect?
    - (1) It shows people buying bread at a bakery.
    - (2) The person standing at the front is wearing a red shirt.
    - (3) There are people lining up to take a train.
    - (4) The clerk is wearing a yellow T-shirt.
        
    ì •ë‹µ: (4) ì ì›ì€ ë…¸ë€ìƒ‰ í‹°ì…”ì¸ ê°€ ì•„ë‹Œ íŒŒë€ìƒ‰ í‹°ì…”ì¸ ë¥¼ ì…ê³  ìˆìŠµë‹ˆë‹¤.
    (ì£¼ì˜: ì •ë‹µì€ 1~4 ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒë˜ë„ë¡ ì¶œì œí•˜ì„¸ìš”.)
    ======
    """

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": quiz_prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}",
                    },
                },
            ],
        }
    ]

    try: 
        response = client.chat.completions.create(
            model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
            messages=messages # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
        )
    except Exception as e:
        print("failed\n" + e)
        return image_quiz(image_path, n_trial+1)
    
    content = response.choices[0].message.content

    if "Listening:" in content:
        return content, True
    else:
        return image_quiz(image_path, n_trial+1)

txt = '' # ë¬¸ì œë“¤ì„ ê³„ì† ë¶™ì—¬ ë‚˜ê°€ê¸° ìœ„í•´ ë¹ˆ ë¬¸ìì—´ ì„ ì–¸
eng_dict = []
no = 1 # ë¬¸ì œ ë²ˆí˜¸ë¥¼ ìœ„í•´ ì„ ì–¸
for g in glob('./chap06/data/images/*.jpg'):  # â‘¡
    q, is_suceed = image_quiz(g)

    if not is_suceed:
        continue

    divider = f'## ë¬¸ì œ {no}\n\n'
    print(divider)
    
    txt += divider 
    # íŒŒì¼ëª… ì¶”ì¶œí•´ ì´ë¯¸ì§€ ë§í¬ ë§Œë“¤ê¸°
    filename = os.path.basename(g) # â‘¢ ë§ˆí¬ë‹¤ìš´ì— í‘œì‹œí•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ì„¤ì •   
    txt += f'![image]({filename})\n\n' 

    # ë¬¸ì œ ì¶”ê°€
    print(q)
    txt += q + '\n\n---------------------\n\n'
    # â‘£ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥
    with open('./chap06/data/images/image_quiz_eng.md', 'w', encoding='utf-8') as f:
        f.write(txt)

    # ì˜ì–´ ë¬¸ì œë§Œ ì¶”ì¶œ
    eng = q.split('Listening: ')[1].split('ì •ë‹µ:')[0].strip()    

    eng_dict.append({
        'no': no,
        'eng': eng,
        'img': filename
    })

    # json íŒŒì¼ë¡œ ì €ì¥
    with open('./chap06/data/images/image_quiz_eng.json', 'w', encoding='utf-8') as f:
        json.dump(eng_dict, f, ensure_ascii=False, indent=4)

    no += 1 # ë¬¸ì œ ë²ˆí˜¸ ì¦ê°€

```

json íŒŒì¼

```json
[
    {
        "no": 1,
        "eng": "Which of the following descriptions of the image is incorrect?\n- (1) Many people are sitting at desks working.\n- (2) There is a large display board on the wall.\n- (3) The lights in the room are turned off.\n- (4) There are structures visible at the top of the image.",
        "img": "busan_dive.jpg"
    },
    {
        "no": 2,
        "eng": "Which of the following descriptions of the image is incorrect?\n- (1) Three people are sitting around a table.\n- (2) The person in the middle has purple hair.\n- (3) There are various foods on the table.\n- (4) The person on the left is wearing yellow clothes.",
        "img": "kdemon.jpg"
    },
    {
        "no": 3,
        "eng": "Which of the following descriptions of the image is incorrect?\n- (1) A yellow sculpture is located in the center.\n- (2) The background building has the word \"Local Stitch\" visible.\n- (3) A bench is located on the right side of the image.\n- (4) There is a space with trees planted in the image.",
        "img": "local_stitch.jpg"
    },
    {
        "no": 4,
        "eng": "Which of the following descriptions of the image is incorrect?\n- (1) People are sitting in the cafÃ©.\n- (2) Large windows surround the cafÃ©.\n- (3) One person is standing in front of the cashier.\n- (4) The cafÃ© walls are yellow and white.",
        "img": "local_stitch_terrarosa.jpg"
    },
    {
        "no": 5,
        "eng": "Which of the following descriptions of the image is incorrect?\n\n- (1) There is a coffee shop on the second floor.\n- (2) The building's exterior is made of red bricks.\n- (3) A no parking sign is visible.\n- (4) An orange cone is placed on the sidewalk.",
        "img": "mangwon.jpg"
    },
    {
        "no": 6,
        "eng": "Which of the following descriptions of the image is incorrect?\n- (1) Various types of bread are displayed.\n- (2) The staff member is wearing a hat.\n- (3) The price tags of the bread are not visible.\n- (4) A glass display case is used.",
        "img": "mangwon_bakery.jpg"
    }
]
```

tts.ipynb
ì˜¤í”ˆAI API ì„¤ì •í•˜ê¸° 

```
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY") 
client = OpenAI(api_key=api_key)  
```

ì˜¤í”ˆAI TTS ê³µì‹ ë¬¸ì„œ(https://platform.openai.com/docs/guides/text-to-speech)

ì²«ë²ˆì§¸ TTS í…ŒìŠ¤íŠ¸

```python
response = client.audio.speech.create(
    model="tts-1-hd",
    voice="echo", // ì±…ê³¼ ë‹¤ë¥¸ ëª©ì†Œë¦¬ ì„ íƒ
    input="Hello world! This is a TTS test.",
)

response.write_to_file("hello_world.mp3")

# ì¬ìƒ
import IPython.display as ipd

ipd.Audio("hello_world.mp3")
```

ëª©ì†Œë¦¬ ë°”ê¾¸ê³  í…ŒìŠ¤íŠ¸

```python
# ë‹¤ë¥¸ ëª©ì†Œë¦¬
voice = "ash"
mp3_file = f"hello_world_{voice}.mp3"

response = client.audio.speech.create(
    model="tts-1-hd",
    voice=voice,
    input=f"Hello world! I'm {voice}. This is a TTS test.",
)

response.write_to_file(mp3_file)

# ì¬ìƒ
import IPython.display as ipd

ipd.Audio(mp3_file)
```

ì˜ì–´ ìŠ¤í¬ë¦½íŠ¸ jsoníŒŒì¼ ì½ê¸°

```python
import json

# json íŒŒì¼ ì—´ê¸°
with open('../data/images/image_quiz_eng.json', 'r', encoding='utf-8') as f:
    eng_dict = json.load(f)

eng_dict

```
mp3 íŒŒì¼ ì¬ìƒ

ipd.Audio(f"../data/audio/1.mp3")

### 7ì¥ ìµœì‹  ì£¼ì‹ ì •ë³´ë¥¼ ì•Œë ¤ ì£¼ëŠ” AI íˆ¬ìì 
07-1 í‘ì…˜ ì½œë§ì˜ ê¸°ì´ˆ
- í‘ì…˜ ì½œë§ì´ë€?
LLMì´ ì‚¬ìš©ìì˜ ìì—°ì–´ ìš”ì²­ì„ ì´í•´í•˜ê³ , ì´ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ë¯¸ë¦¬ ì •ì˜ëœ ì™¸ë¶€ í•¨ìˆ˜(API)ë¥¼ ìë™ìœ¼ë¡œ í˜¸ì¶œí•˜ëŠ” ê¸°ìˆ , LLMì€ ë‹¨ìˆœí•œ ì •ë³´ ì œê³µì„ ë„˜ì–´ ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ ì—°ë™í•  ìˆ˜ ìˆê²Œ ëœë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ìš©ìê°€ ì§€ê¸ˆ ëª‡ì‹œì•¼? ë¬¼ì–´ë³´ë©´ gptëŠ” ë„êµ¬ ëª©ë¡ì—ì„œ ì‹œê°„ì„ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë¥¼ ì°¾ì•„ì„œ ê·¸ ë„êµ¬ë¥¼ ì‚¬ìš© í•´ì„œ ë‹µë³€í•¨.
ë„êµ¬ ëª©ë¡ì˜ ë”•ì…”ë„ˆë¦¬ëŠ” GPTëª¨ë¸ì´ ì–´ë–¤ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì§€ ì•Œë ¤ì£¼ëŠ” ì„¤ëª…ì„œ ì—­í• ì„ í•˜ë©°, GPT  APIë¥¼ í˜¸ì¶œí•  ë•Œ ì´ ë„êµ¬ ëª©ë¡ë„ í•¨ê»˜ ì „ë‹¬ë¨
ì–´ë–¤ ê±¸ ë¬¼ì–´ë´¤ì„ ë•Œ â€˜ë¶„ì„ì¤‘â€¦â€™ ì´ë ‡ê²Œ ëœ¨ëŠ” ê±°ë©´ í‘ì…˜ ì½œë§ ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¤‘ì„.
    - [ì‹¤ìŠµ] í‘ì…˜ ì½œë§ ì ìš©í•˜ê¸°
    
    ```python
    // GPTë¥¼ ìœ„í•´ ì‚¬ìš©í•  í•¨ìˆ˜ ì •ì˜ ë° ì„¤ëª… ì¶”ê°€
    from datetime import datetime
    
    def get_current_time():
        now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(now)
        return now
    
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_current_time",
                "description": "í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.", #ë§¤ê°œë³€ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ íŒŒë¼ë¯¸í„° ìƒëµ
            }
        },
    ]
    
    if __name__ == '__main__':
        get_current_time()  
        
        # 2025-11-11 23:08:00
    ```
    
    ```python
    from gpt_functions import get_current_time, tools 
    from openai import OpenAI
    from dotenv import load_dotenv
    import os
    
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY") 
    
    client = OpenAI(api_key=api_key)
    
    def get_ai_response(messages, tools=None):
        response = client.chat.completions.create(
            model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
            messages=messages,  # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
            tools=tools,  # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ì „ë‹¬
        )
        return response  # ìƒì„±ëœ ì‘ë‹µ ë‚´ìš© ë°˜í™˜
    
    messages = [
        {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."},  # ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€
    ]
    
    while True:
        user_input = input("ì‚¬ìš©ì\t: ")  # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    
        if user_input == "exit":  # ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ë ¤ëŠ”ì§€ í™•ì¸
            break
        
        messages.append({"role": "user", "content": user_input})  # ì‚¬ìš©ì ë©”ì‹œì§€ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        
        ai_response = get_ai_response(messages, tools=tools)
        ai_message = ai_response.choices[0].message
        print(ai_message)  # gptì—ì„œ ë°˜í™˜ë˜ëŠ” ê°’ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ì„ì‹œë¡œ ì¶”ê°€
    
        tool_calls = ai_message.tool_calls  # AI ì‘ë‹µì— í¬í•¨ëœ tool_callsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        if tool_calls:  # tool_callsê°€ ìˆëŠ” ê²½ìš°
            tool_name = tool_calls[0].function.name # ì‹¤í–‰í•´ì•¼í•œë‹¤ê³  íŒë‹¨í•œ í•¨ìˆ˜ëª… ë°›ê¸°
            tool_call_id = tool_calls[0].id         # tool_call ì•„ì´ë”” ë°›ê¸°    
            
            if tool_name == "get_current_time":  # ë§Œì•½ tool_nameì´ "get_current_time"ì´ë¼ë©´
                messages.append({
                    "role": "function",  # roleì„ "function"ìœ¼ë¡œ ì„¤ì •
                    "tool_call_id": tool_call_id,
                    "name": tool_name,
                    "content": get_current_time(),  # get_current_time í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•œ ê²°ê³¼ë¥¼ contentë¡œ ì„¤ì •
                })
    
            ai_response = get_ai_response(messages, tools=tools) # ë‹¤ì‹œ GPT ì‘ë‹µ ë°›ê¸°
            ai_message = ai_response.choices[0].message
    
        messages.append(ai_message)  # AI ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•˜ê¸°
    
        print("AI\t: " + ai_message.content)  # AI ì‘ë‹µ ì¶œë ¥
    
    ```
    

- [ì‹¤ìŠµ] ë„ì‹œë³„ ì‹œê°„ ì•Œë ¤ ì£¼ê¸°

```
// íƒ€ì„ì¡´ ì •ë³´ë¥¼ ì´ìš©í•´ í˜„ì¬ ì‹œê°„ì„ êµ¬í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì •
from datetime import datetime
import pytz 

def get_current_time(timezone: str = 'Asia/Seoul'):
    tz = pytz.timezone(timezone) # íƒ€ì„ì¡´ ì„¤ì •
    now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
    now_timezone = f'{now} {timezone}'
    print(now_timezone)
    return now_timezone

tools = [
    {
        "type": "function",
        "function": {
            "name": "get_current_time",
            "description": "í•´ë‹¹ íƒ€ì„ì¡´ì˜ ë‚ ì§œì™€ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    'timezone': {
                        'type': 'string',
                        'description': 'í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ì„ ë°˜í™˜í•  íƒ€ì„ì¡´ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: Asia/Seoul)',
                    },
                },
                "required": ['timezone'],
            },        
        }
    },
]

if __name__ == '__main__':
    get_current_time('America/New_York')
```

```python
from gpt_functions import get_current_time, tools 
from openai import OpenAI
from dotenv import load_dotenv
import os
import json

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")  

client = OpenAI(api_key=api_key)

def get_ai_response(messages, tools=None):
    response = client.chat.completions.create(
        model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
        messages=messages,  # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
        tools=tools,  # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ì „ë‹¬
    )
    return response  # ìƒì„±ëœ ì‘ë‹µ ë‚´ìš© ë°˜í™˜

messages = [
    {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."},  # ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€
]

while True:
    user_input = input("ì‚¬ìš©ì\t: ")  # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°

    if user_input == "exit":  # ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ë ¤ëŠ”ì§€ í™•ì¸
        break
    
    messages.append({"role": "user", "content": user_input})  # ì‚¬ìš©ì ë©”ì‹œì§€ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
    
    ai_response = get_ai_response(messages, tools=tools)
    ai_message = ai_response.choices[0].message
    print(ai_message)  # gptì—ì„œ ë°˜í™˜ë˜ëŠ” ê°’ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ì„ì‹œë¡œ ì¶”ê°€

    tool_calls = ai_message.tool_calls  # AI ì‘ë‹µì— í¬í•¨ëœ tool_callsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    if tool_calls:  # tool_callsê°€ ìˆëŠ” ê²½ìš°
        tool_name = tool_calls[0].function.name # ì‹¤í–‰í•´ì•¼í•œë‹¤ê³  íŒë‹¨í•œ í•¨ìˆ˜ëª… ë°›ê¸°
        tool_call_id = tool_calls[0].id         # í•¨ìˆ˜ ì•„ì´ë”” ë°›ê¸°    
        arguments = json.loads(tool_calls[0].function.arguments) # ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜    
        
        if tool_name == "get_current_time":  #tool_nameì´ "get_current_time"ì´ë¼ë©´
            messages.append({
                "role": "function",  # roleì„ "function"ìœ¼ë¡œ ì„¤ì •
                "tool_call_id": tool_call_id,
                "name": tool_name,
                "content": get_current_time(timezone=arguments['timezone']),  # íƒ€ì„ì¡´ ì¶”ê°€
            })

        ai_response = get_ai_response(messages, tools=tools) # ë‹¤ì‹œ GPT ì‘ë‹µ ë°›ê¸°
        ai_message = ai_response.choices[0].message

    messages.append(ai_message)  # AI ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•˜ê¸°

    print("AI\t: " + ai_message.content)  # AI ì‘ë‹µ ì¶œë ¥

```

    

- [ì‹¤ìŠµ] ì—¬ëŸ¬ ë„ì‹œì˜ ì‹œê°„ì„ í•œ ë²ˆì— ëŒ€ë‹µí•  ìˆ˜ ìˆê²Œ í•˜ê¸°
    
    ```python
    from gpt_functions import get_current_time, tools 
    from openai import OpenAI
    from dotenv import load_dotenv
    import os
    import json
    
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY") 
    
    client = OpenAI(api_key=api_key) 
    
    def get_ai_response(messages, tools=None):
        response = client.chat.completions.create(
            model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
            messages=messages,  # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
            tools=tools,  # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ì „ë‹¬
        )
        return response  # ìƒì„±ëœ ì‘ë‹µ ë‚´ìš© ë°˜í™˜
    
    messages = [
        {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."},  # ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€
    ]
    
    while True:
        user_input = input("ì‚¬ìš©ì\t: ")  # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    
        if user_input == "exit":  # ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ë ¤ëŠ”ì§€ í™•ì¸
            break
        
        messages.append({"role": "user", "content": user_input})  # ì‚¬ìš©ì ë©”ì‹œì§€ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        
        ai_response = get_ai_response(messages, tools=tools)
        ai_message = ai_response.choices[0].message
        print(ai_message)  # gptì—ì„œ ë°˜í™˜ë˜ëŠ” ê°’ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ì„ì‹œë¡œ ì¶”ê°€
    
        tool_calls = ai_message.tool_calls  # AI ì‘ë‹µì— í¬í•¨ëœ tool_callsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        if tool_calls:  # tool_callsê°€ ìˆëŠ” ê²½ìš°
            for tool_call in tool_calls: # í•¨ìˆ˜ ê²°ê³¼ ê³„ì† ì¶”ê°€
                tool_name = tool_call.function.name # ì‹¤í–‰í•´ì•¼í•œë‹¤ê³  íŒë‹¨í•œ í•¨ìˆ˜ëª… ë°›ê¸°
                tool_call_id = tool_call.id         # tool_call ì•„ì´ë”” ë°›ê¸°    
                arguments = json.loads(tool_call.function.arguments) #ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜    
                
                if tool_name == "get_current_time":  # ë§Œì•½ tool_nameì´ "get_current_time"ì´ë¼ë©´
                    messages.append({
                        "role": "function",  # roleì„ "function"ìœ¼ë¡œ ì„¤ì •
                        "tool_call_id": tool_call_id,
                        "name": tool_name,
                        "content": get_current_time(timezone=arguments['timezone']),  # íƒ€ì„ì¡´ ì¶”ê°€
                    })
            messages.append({"role": "system", "content": "ì´ì œ ì£¼ì–´ì§„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•  ì°¨ë¡€ë‹¤."})  # í•¨ìˆ˜ ì‹¤í–‰ ì™„ë£Œ ë©”ì‹œì§€ ì¶”ê°€
            ai_response = get_ai_response(messages, tools=tools) # ë‹¤ì‹œ GPT ì‘ë‹µ ë°›ê¸°
            ai_message = ai_response.choices[0].message
    
        messages.append(ai_message)  # AI ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•˜ê¸°
    
        print("AI\t: " + ai_message.content)  # AI ì‘ë‹µ ì¶œë ¥
    
    ```
    

- [ì‹¤ìŠµ] ìŠ¤íŠ¸ë¦¼ë¦¿ì—ì„œ í‘ì…˜ ì½œë§ ì‚¬ìš©í•˜ê¸°
    
    ```
    from gpt_functions import get_current_time, tools 
    from openai import OpenAI
    from dotenv import load_dotenv
    import os
    import json
    import streamlit as st
    
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")  
    
    client = OpenAI(api_key=api_key)  
    
    def get_ai_response(messages, tools=None):
        response = client.chat.completions.create(
            model="gpt-4o",  
            messages=messages,  
            tools=tools,  
        )
        return response 
    
    st.title("ğŸ’¬ AI Chatbot")   
    
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."}
        ] 
    
    for msg in st.session_state.messages:
        if msg["role"] == "assistant" or msg["role"] == "user": # assistant í˜¹ì€ user ë©”ì‹œì§€ì¸ ê²½ìš°ë§Œ
            st.chat_message(msg["role"]).write(msg["content"])
    
    if user_input := st.chat_input():    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        st.session_state.messages.append({"role": "user", "content": user_input})  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        st.chat_message("user").write(user_input)  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¸Œë¼ìš°ì €ì—ì„œë„ ì¶œë ¥
        
        ai_response = get_ai_response(st.session_state.messages, tools=tools)
        ai_message = ai_response.choices[0].message
        print(ai_message)  # gptì—ì„œ ë°˜í™˜ë˜ëŠ” ê°’ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ì„ì‹œë¡œ ì¶”ê°€
    
        tool_calls = ai_message.tool_calls  # AI ì‘ë‹µì— í¬í•¨ëœ tool_callsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        if tool_calls:  # tool_callsê°€ ìˆëŠ” ê²½ìš°
            for tool_call in tool_calls:
                tool_name = tool_call.function.name # ì‹¤í–‰í•´ì•¼í•œë‹¤ê³  íŒë‹¨í•œ í•¨ìˆ˜ëª… ë°›ê¸°
                tool_call_id = tool_call.id         # tool_call ì•„ì´ë”” ë°›ê¸°    
                arguments = json.loads(tool_call.function.arguments) # (1) ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜    
                
                if tool_name == "get_current_time":  # â‘¤ ë§Œì•½ tool_nameì´ "get_current_time"ì´ë¼ë©´
                    st.session_state.messages.append({
                        "role": "function",  # roleì„ "function"ìœ¼ë¡œ ì„¤ì •
                        "tool_call_id": tool_call_id,
                        "name": tool_name,
                        "content": get_current_time(timezone=arguments['timezone']),  # íƒ€ì„ì¡´ ì¶”ê°€
                    })
            st.session_state.messages.append({"role": "system", "content": "ì´ì œ ì£¼ì–´ì§„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•  ì°¨ë¡€ë‹¤."}) 
            ai_response = get_ai_response(st.session_state.messages, tools=tools) # ë‹¤ì‹œ GPT ì‘ë‹µ ë°›ê¸°
            ai_message = ai_response.choices[0].message
    
        st.session_state.messages.append({
            "role": "assistant",
            "content": ai_message.content
        })  # â‘¢ AI ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    
        print("AI\t: " + ai_message.content)  # AI ì‘ë‹µ ì¶œë ¥
        st.chat_message("assistant").write(ai_message.content)  # ë¸Œë¼ìš°ì €ì— ë©”ì‹œì§€ ì¶œë ¥
    ```
07-2 GPTì™€ ë¯¸êµ­ ì£¼ì‹ ì´ì•¼ê¸°í•˜ê¸°
yfinance: ì•¼í›„ íŒŒì´ë‚¸ìŠ¤ì˜ ê¸ˆìœµ ë°ì´í„°ë¥¼ ì‰½ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤, ì£¼ê°€, ì¬ë¬´ì œí‘œ, ê±°ë˜ëŸ‰ ë“± ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ë°ì´í„° í”„ë ˆì„ í˜•íƒœë¡œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ 

- [ì‹¤ìŠµ] yfinance ì‚¬ìš©í•˜ê¸°

```python
%pip install yfinance

import yfinance as yf

# Microsoft (MSFT)ì— ëŒ€í•œ Ticker(ê¸ˆìœµ ìƒí’ˆ ì‹ë³„í•˜ëŠ” ê³ ìœ  ì½”ë“œ) ê°ì²´ ìƒì„±
msft = yf.Ticker("MSFT")

# Ticker ê°ì²´ì— ëŒ€í•œ ì •ë³´ ì¶œë ¥ (.pyì—ì„œ ì‹¤í–‰í•  ë•ŒëŠ” print(msft.info)ë¡œ ì‚¬ìš©)
display(msft.info)
```

```python
// ì‹¤í–‰ ê²°ê³¼

{'address1': 'One Microsoft Way',
 'city': 'Redmond',
 'state': 'WA',
 'zip': '98052-6399',
 'country': 'United States',
 'phone': '425 882 8080',
 'website': '
 [https://www.microsoft.com](https://www.microsoft.com/)
 ',
 'industry': 'Software - Infrastructure',
 'industryKey': 'software-infrastructure',
 'industryDisp': 'Software - Infrastructure',
 'sector': 'Technology',
 'sectorKey': 'technology',
 'sectorDisp': 'Technology',
 'longBusinessSummary': "Microsoft Corporation develops and supports software, services, devices, and solutions worldwide. The company's Productivity and Business Processes segment offers Microsoft 365 Commercial, Enterprise Mobility + Security, Windows Commercial, Power BI, Exchange, SharePoint, Microsoft Teams, Security and Compliance, and Copilot; Microsoft 365 Commercial products, such as Windows Commercial on-premises and Office licensed services; Microsoft 365 Consumer products and cloud services, such as Microsoft 365 Consumer subscriptions, Office licensed on-premises, and other consumer services; LinkedIn; Dynamics products and cloud services, such as Dynamics 365, cloud-based applications, and on-premises ERP and CRM applications. Its Intelligent Cloud segment provides Server products and cloud services, such as Azure and other cloud services, GitHub, Nuance Healthcare, virtual desktop offerings, and other cloud services; Server products, including SQL and Windows Server, Visual Studio and System Center related Client Access Licenses, and other on-premises offerings; Enterprise and partner services, including Enterprise Support and Nuance professional Services, Industry Solutions, Microsoft Partner Network, and Learning Experience. The company's Personal Computing segment provides Windows and Devices, such as Windows OEM licensing and Devices and Surface and PC accessories; Gaming services and solutions, such as Xbox hardware, content, and services, first- and third-party content Xbox Game Pass, subscriptions, and Cloud Gaming, advertising, and other cloud services; search and news advertising services, such as Bing and Copilot, Microsoft News and Edge, and third-party affiliates. It sells its products through OEMs, distributors, and resellers; and online and retail stores. The company was founded in 1975 and is headquartered in Redmond, Washington.",
 'fullTimeEmployees': 228000,
 'companyOfficers': [{'maxAge': 1,
   'name': 'Mr. Satya  Nadella',
   'age': 57,
   'title': 'Chairman & CEO',
   'yearBorn': 1967,
   'fiscalYear': 2025,
   'totalPay': 12251294,
   'exercisedValue': 0,
   'unexercisedValue': 0},
  {'maxAge': 1,
 ...
```

```python
// ìµœê·¼ ì£¼ê°€ ì •ë³´ ë³´ê¸°
hist = msft.history(period="2mo") # 2ê°œì›”ê°„ì˜ ì£¼ê°€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
display(hist) # ë°ì´í„° ì¶œë ¥
```
<img width="1204" height="821" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-16 á„‹á…©á„’á…® 5 02 07" src="https://github.com/user-attachments/assets/f55dd4f7-b284-45ad-b066-69260b07b695" />
<img width="1062" height="244" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-16 á„‹á…©á„’á…® 5 02 14" src="https://github.com/user-attachments/assets/cfb47817-bd10-400c-95fe-de05f2f045de" />


```python
// ì£¼ì‹ ì¢…ëª©ì˜ ì¶”ì²œ ì—¬ë¶€ ë³´ê¸°
msft.recommendations # ì¶”ì²œ ì •ë³´ ì¶œë ¥
```

<img width="520" height="209" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-16 á„‹á…©á„’á…® 5 04 19" src="https://github.com/user-attachments/assets/81e3f03a-c23c-412e-99b8-dd6bc08460e8" />


- [ì‹¤ìŠµ] GPTì—ì„œ ì‚¬ìš©í•  yfinance ê´€ë ¨ í•¨ìˆ˜ ë§Œë“¤ê¸°
    
    ```python
    from datetime import datetime
    import pytz
    import yfinance as yf
    
    def get_current_time(timezone: str = 'Asia/Seoul'):
        tz = pytz.timezone(timezone) # íƒ€ì„ì¡´ ì„¤ì •
        now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        now_timezone = f'{now} {timezone}'
        print(now_timezone)
        return now_timezone
    
    def get_yf_stock_info(ticker: str):
        stock = yf.Ticker(ticker)
        info = stock.info
        print(info)
        return str(info)
    
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_current_time",
                "description": "í•´ë‹¹ íƒ€ì„ì¡´ì˜ ë‚ ì§œì™€ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        'timezone': {
                            'type': 'string',
                            'description': 'í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ì„ ë°˜í™˜í•  íƒ€ì„ì¡´ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: Asia/Seoul)',
                        },
                    },
                    "required": ['timezone'],
                },        
            }
        },
        {
            "type": "function",
            "function": {
                "name": "get_yf_stock_info",
                "description": "í•´ë‹¹ ì¢…ëª©ì˜ Yahoo Finance ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        'ticker': {
                            'type': 'string',
                            'description': 'Yahoo Finance ì •ë³´ë¥¼ ë°˜í™˜í•  ì¢…ëª©ì˜ í‹°ì»¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: AAPL)',
                        },
                    },
                    "required": ['ticker'],
                },        
            }
        }
    ]
    
    if __name__ == '__main__':
        # get_current_time('America/New_York')
        info = get_yf_stock_info('GOOGL')    
    ```
    
    ```python
    // êµ¬ê¸€ ì •ë³´ ì¶œë ¥ 
    {'address1': '1600 Amphitheatre Parkway', 'city': 'Mountain View', 'state': 'CA', 'zip': '94043', 'country': 'United States', 'phone': '650-253-0000', 'website': 'https://abc.xyz', 'industry': 'Internet Content & Information', 'industryKey': 'internet-content-information', 'industryDisp': 'Internet Content & Information', 'sector': 'Communication Services', 'sectorKey': 'communication-services', 'sectorDisp': 'Communication Services', 'longBusinessSummary': 'Alphabet Inc. offers various products and platforms in the United States, Europe, the Middle East, Africa, the Asia-Pacific, Canada, and Latin America. It operates through Google Services, Google Cloud, and Other Bets segments. The Google Services segment provides products and services, including ads, Android, Chrome, devices, Gmail, Google Drive, Google Maps, Google Photos, Google Play, Search, and YouTube. It is also involved in the sale of apps and in-app purchases and digital content in the Google Play and YouTube; and devices, as well as in the provision of YouTube consumer subscription services. The Google Cloud segment offers A
    ```
    
    ```
    from datetime import datetime
    import pytz
    import yfinance as yf
    
    def get_current_time(timezone: str = 'Asia/Seoul'):
        tz = pytz.timezone(timezone) # íƒ€ì„ì¡´ ì„¤ì •
        now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        now_timezone = f'{now} {timezone}'
        print(now_timezone)
        return now_timezone
    
    def get_yf_stock_info(ticker: str):
        stock = yf.Ticker(ticker)
        info = stock.info
        print(info)
        return str(info)
    
    def get_yf_stock_history(ticker: str, period: str):
        stock = yf.Ticker(ticker)
        history = stock.history(period=period)
        history_md = history.to_markdown() # ë°ì´í„°í”„ë ˆì„ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        print(history_md)
        return history_md
    
    def get_yf_stock_recommendations(ticker: str):
        stock = yf.Ticker(ticker)
        recommendations = stock.recommendations
        recommendations_md = recommendations.to_markdown() # ë°ì´í„°í”„ë ˆì„ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        print(recommendations_md)
        return recommendations_md
    
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_current_time",
                "description": "í•´ë‹¹ íƒ€ì„ì¡´ì˜ ë‚ ì§œì™€ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        'timezone': {
                            'type': 'string',
                            'description': 'í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ì„ ë°˜í™˜í•  íƒ€ì„ì¡´ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: Asia/Seoul)',
                        },
                    },
                    "required": ['timezone'],
                },        
            }
        },
        {
            "type": "function",
            "function": {
                "name": "get_yf_stock_info",
                "description": "í•´ë‹¹ ì¢…ëª©ì˜ Yahoo Finance ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        'ticker': {
                            'type': 'string',
                            'description': 'Yahoo Finance ì •ë³´ë¥¼ ë°˜í™˜í•  ì¢…ëª©ì˜ í‹°ì»¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: AAPL)',
                        },
                    },
                    "required": ['ticker'],
                },        
            }
        },
        {
            "type": "function",
            "function": {
                "name": "get_yf_stock_history",
                "description": "í•´ë‹¹ ì¢…ëª©ì˜ Yahoo Finance ì£¼ê°€ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        'ticker': {
                            'type': 'string',
                            'description': 'Yahoo Finance ì£¼ê°€ ì •ë³´ë¥¼ ë°˜í™˜í•  ì¢…ëª©ì˜ í‹°ì»¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: AAPL)',
                        },
                        'period': {
                            'type': 'string',
                            'description': 'ì£¼ê°€ ì •ë³´ë¥¼ ì¡°íšŒí•  ê¸°ê°„ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: 1d, 5d, 1mo, 1y, 5y)',
                        },
                    },
                    "required": ['ticker', 'period'],
                },        
            }
        },
        {
            "type": "function",
            "function": {
                "name": "get_yf_stock_recommendations",
                "description": "í•´ë‹¹ ì¢…ëª©ì˜ Yahoo Finance ì¶”ì²œ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        'ticker': {
                            'type': 'string',
                            'description': 'Yahoo Finance ì¶”ì²œ ì •ë³´ë¥¼ ë°˜í™˜í•  ì¢…ëª©ì˜ í‹°ì»¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: AAPL)',
                        },
                    },
                    "required": ['ticker'],
                },        
            }
        },
    ]
    
    if __name__ == '__main__':
        # get_current_time('America/New_York')
        # info = get_yf_stock_info('AAPL')  
    
        get_yf_stock_history('AAPL', '5d')
        print('----')
        get_yf_stock_recommendations('AAPL')
      
    ```
    
    ```python
    // ìŠ¤íŠ¸ë¦¼ë¦¿ ì‹¤í–‰
    from gpt_functions import get_current_time, tools, get_yf_stock_info
    from openai import OpenAI
    from dotenv import load_dotenv
    import os
    import json
    import streamlit as st
    
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    
    client = OpenAI(api_key=api_key)  # ì˜¤í”ˆAI í´ë¼ì´ì–¸íŠ¸ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    
    def get_ai_response(messages, tools=None):
        response = client.chat.completions.create(
            model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
            messages=messages,  # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
            tools=tools,  # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ì „ë‹¬
        )
        return response  # ìƒì„±ëœ ì‘ë‹µ ë‚´ìš© ë°˜í™˜
    
    st.title("ğŸ’¬ Chatbot")   
    
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."},  # ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€
        ] 
    
    for msg in st.session_state.messages:
        if msg["role"] == "assistant" or msg["role"] == "user": # assistant í˜¹ì€ user ë©”ì‹œì§€ì¸ ê²½ìš°ë§Œ
            st.chat_message(msg["role"]).write(msg["content"])
    
    if user_input := st.chat_input():    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        st.session_state.messages.append({"role": "user", "content": user_input})  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        st.chat_message("user").write(user_input)  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¸Œë¼ìš°ì €ì—ì„œë„ ì¶œë ¥
        
        ai_response = get_ai_response(st.session_state.messages, tools=tools)
        ai_message = ai_response.choices[0].message
        print(ai_message)  # gptì—ì„œ ë°˜í™˜ë˜ëŠ” ê°’ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ì„ì‹œë¡œ ì¶”ê°€
    
        tool_calls = ai_message.tool_calls  # AI ì‘ë‹µì— í¬í•¨ëœ tool_callsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        if tool_calls:  # tool_callsê°€ ìˆëŠ” ê²½ìš°
            for tool_call in tool_calls:
                tool_name = tool_call.function.name # ì‹¤í–‰í•´ì•¼í•œë‹¤ê³  íŒë‹¨í•œ í•¨ìˆ˜ëª… ë°›ê¸°
                tool_call_id = tool_call.id         # tool_call ì•„ì´ë”” ë°›ê¸°    
                arguments = json.loads(tool_call.function.arguments) # ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜    
                
                if tool_name == "get_current_time":  # ë§Œì•½ tool_nameì´ "get_current_time"ì´ë¼ë©´
                    st.session_state.messages.append({
                        "role": "function",  # roleì„ "function"ìœ¼ë¡œ ì„¤ì •
                        "tool_call_id": tool_call_id,
                        "name": tool_name,
                        "content": get_current_time(timezone=arguments['timezone']),  # íƒ€ì„ì¡´ ì¶”ê°€
                    })
                elif tool_name == "get_yf_stock_info":
                    st.session_state.messages.append({
                        "role": "function",
                        "tool_call_id": tool_call_id,
                        "name": tool_name,
                        "content": get_yf_stock_info(ticker=arguments['ticker']),
                    })
    
            st.session_state.messages.append({"role": "system", "content": "ì´ì œ ì£¼ì–´ì§„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•  ì°¨ë¡€ë‹¤."}) 
            ai_response = get_ai_response(st.session_state.messages, tools=tools) # ë‹¤ì‹œ GPT ì‘ë‹µ ë°›ê¸°
            ai_message = ai_response.choices[0].message
    
        st.session_state.messages.append({
            "role": "assistant",
            "content": ai_message.content
        })  # AI ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    
        print("AI\t: " + ai_message.content)  # AI ì‘ë‹µ ì¶œë ¥
        st.chat_message("assistant").write(ai_message.content)  # ë¸Œë¼ìš°ì €ì— ë©”ì‹œì§€ ì¶œë ¥
    ```
    <img width="827" height="771" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-16 á„‹á…©á„’á…® 5 22 23" src="https://github.com/user-attachments/assets/30045457-8cc2-4987-8144-d2488d7ac5ac" />

- [ì‹¤ìŠµ] ì½”ë“œ ë¦¬íŒ©í† ë§í•˜ê¸°
    
    ```python
    from gpt_functions import get_current_time, tools, get_yf_stock_info
    from openai import OpenAI
    from dotenv import load_dotenv
    import os
    import json
    import streamlit as st
    
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    
    client = OpenAI(api_key=api_key)  # ì˜¤í”ˆAI í´ë¼ì´ì–¸íŠ¸ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    
    def get_ai_response(messages, tools=None):
        response = client.chat.completions.create(
            model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
            messages=messages,  # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
            tools=tools,  # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ì „ë‹¬
        )
        return response  # ìƒì„±ëœ ì‘ë‹µ ë‚´ìš© ë°˜í™˜
    
    st.title("ğŸ’¬ Chatbot")   
    
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."},  # ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€
        ] 
    
    for msg in st.session_state.messages:
        if msg["role"] == "assistant" or msg["role"] == "user": # assistant í˜¹ì€ user ë©”ì‹œì§€ì¸ ê²½ìš°ë§Œ
            st.chat_message(msg["role"]).write(msg["content"])
    
    if user_input := st.chat_input():    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        st.session_state.messages.append({"role": "user", "content": user_input})  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        st.chat_message("user").write(user_input)  #ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¸Œë¼ìš°ì €ì—ì„œë„ ì¶œë ¥
        
        ai_response = get_ai_response(st.session_state.messages, tools=tools)
        ai_message = ai_response.choices[0].message
        print(ai_message)  # gptì—ì„œ ë°˜í™˜ë˜ëŠ” ê°’ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ì„ì‹œë¡œ ì¶”ê°€
    
        tool_calls = ai_message.tool_calls  # AI ì‘ë‹µì— í¬í•¨ëœ tool_callsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        if tool_calls:  # tool_callsê°€ ìˆëŠ” ê²½ìš°
            for tool_call in tool_calls:
                tool_name = tool_call.function.name # ì‹¤í–‰í•´ì•¼í•œë‹¤ê³  íŒë‹¨í•œ í•¨ìˆ˜ëª… ë°›ê¸°
                tool_call_id = tool_call.id         # tool_call ì•„ì´ë”” ë°›ê¸°    
                arguments = json.loads(tool_call.function.arguments) # ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜    
                
                if tool_name == "get_current_time":  
                    func_result = get_current_time(timezone=arguments['timezone'])
                elif tool_name == "get_yf_stock_info":
                    func_result = get_yf_stock_info(ticker=arguments['ticker'])
           
                st.session_state.messages.append({
                    "role": "function",
                    "tool_call_id": tool_call_id,
                    "name": tool_name,
                    "content": func_result,
                })
    
            st.session_state.messages.append({"role": "system", "content": "ì´ì œ ì£¼ì–´ì§„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•  ì°¨ë¡€ë‹¤."}) 
            ai_response = get_ai_response(st.session_state.messages, tools=tools) # ë‹¤ì‹œ GPT ì‘ë‹µ ë°›ê¸°
            ai_message = ai_response.choices[0].message
    
        st.session_state.messages.append({
            "role": "assistant",
            "content": ai_message.content
        })  # AI ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    
        print("AI\t: " + ai_message.content)  # AI ì‘ë‹µ ì¶œë ¥
        st.chat_message("assistant").write(ai_message.content)  # ë¸Œë¼ìš°ì €ì— ë©”ì‹œì§€ ì¶œë ¥
    ```
    
- [ì‹¤ìŠµ] ì¢…ëª© ìµœê·¼ ì£¼ê°€ ì •ë³´ì™€ ì¶”ì²œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    
    ```
    from datetime import datetime
    import pytz
    import yfinance as yf
    
    def get_current_time(timezone: str = 'Asia/Seoul'):
        tz = pytz.timezone(timezone) # íƒ€ì„ì¡´ ì„¤ì •
        now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        now_timezone = f'{now} {timezone}'
        print(now_timezone)
        return now_timezone
    
    def get_yf_stock_info(ticker: str):
        stock = yf.Ticker(ticker)
        info = stock.info
        print(info)
        return str(info)
    
    def get_yf_stock_history(ticker: str, period: str):
        stock = yf.Ticker(ticker)
        history = stock.history(period=period)
        history_md = history.to_markdown() # ë°ì´í„°í”„ë ˆì„ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        print(history_md)
        return history_md
    
    def get_yf_stock_recommendations(ticker: str):
        stock = yf.Ticker(ticker)
        recommendations = stock.recommendations
        recommendations_md = recommendations.to_markdown() # ë°ì´í„°í”„ë ˆì„ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        print(recommendations_md)
        return recommendations_md
    
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_current_time",
                "description": "í•´ë‹¹ íƒ€ì„ì¡´ì˜ ë‚ ì§œì™€ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        'timezone': {
                            'type': 'string',
                            'description': 'í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ì„ ë°˜í™˜í•  íƒ€ì„ì¡´ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: Asia/Seoul)',
                        },
                    },
                    "required": ['timezone'],
                },        
            }
        },
        {
            "type": "function",
            "function": {
                "name": "get_yf_stock_info",
                "description": "í•´ë‹¹ ì¢…ëª©ì˜ Yahoo Finance ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        'ticker': {
                            'type': 'string',
                            'description': 'Yahoo Finance ì •ë³´ë¥¼ ë°˜í™˜í•  ì¢…ëª©ì˜ í‹°ì»¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: AAPL)',
                        },
                    },
                    "required": ['ticker'],
                },        
            }
        },
        {
            "type": "function",
            "function": {
                "name": "get_yf_stock_history",
                "description": "í•´ë‹¹ ì¢…ëª©ì˜ Yahoo Finance ì£¼ê°€ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        'ticker': {
                            'type': 'string',
                            'description': 'Yahoo Finance ì£¼ê°€ ì •ë³´ë¥¼ ë°˜í™˜í•  ì¢…ëª©ì˜ í‹°ì»¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: AAPL)',
                        },
                        'period': {
                            'type': 'string',
                            'description': 'ì£¼ê°€ ì •ë³´ë¥¼ ì¡°íšŒí•  ê¸°ê°„ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: 1d, 5d, 1mo, 1y, 5y)',
                        },
                    },
                    "required": ['ticker', 'period'],
                },        
            }
        },
        {
            "type": "function",
            "function": {
                "name": "get_yf_stock_recommendations",
                "description": "í•´ë‹¹ ì¢…ëª©ì˜ Yahoo Finance ì¶”ì²œ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        'ticker': {
                            'type': 'string',
                            'description': 'Yahoo Finance ì¶”ì²œ ì •ë³´ë¥¼ ë°˜í™˜í•  ì¢…ëª©ì˜ í‹°ì»¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: AAPL)',
                        },
                    },
                    "required": ['ticker'],
                },        
            }
        },
    ]
    
    if __name__ == '__main__':
        # get_current_time('America/New_York')
        # info = get_yf_stock_info('AAPL')  
    
        get_yf_stock_history('AAPL', '5d')
        print('----')
        get_yf_stock_recommendations('AAPL')
      
    ```
    
    | Date | Open | High | Low | Close | Volume | Dividends | Stock Splits |
    | --- | --- | --- | --- | --- | --- | --- | --- |
    | 2025-11-10 00:00:00-05:00 | 268.96 | 273.73 | 267.46 | 269.43 | 4.13124e+07 | 0.26 | 0 |
    | 2025-11-11 00:00:00-05:00 | 269.81 | 275.91 | 269.8 | 275.25 | 4.62083e+07 | 0 | 0 |
    | 2025-11-12 00:00:00-05:00 | 275 | 275.73 | 271.7 | 273.47 | 4.8398e+07 | 0 | 0 |
    | 2025-11-13 00:00:00-05:00 | 274.11 | 276.7 | 272.09 | 272.95 | 4.96028e+07 | 0 | 0 |
    | 2025-11-14 00:00:00-05:00 | 271.05 | 275.96 | 269.6 | 272.41 | 4.73993e+07 | 0 | 0 |
    
    ---
    
    |  | period | strongBuy | buy | hold | sell | strongSell |
    | --- | --- | --- | --- | --- | --- | --- |
    | 0 | 0m | 5 | 24 | 15 | 1 | 3 |
    | 1 | -1m | 5 | 24 | 15 | 1 | 3 |
    | 2 | -2m | 5 | 23 | 15 | 1 | 3 |
    | 3 | -3m | 5 | 22 | 15 | 1 | 1 |
    
    ```python
    from gpt_functions import get_current_time, tools, get_yf_stock_info, get_yf_stock_history, get_yf_stock_recommendations
    from openai import OpenAI
    from dotenv import load_dotenv
    import os
    import json
    import streamlit as st
    
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")  
    
    client = OpenAI(api_key=api_key) 
    
    def get_ai_response(messages, tools=None):
        response = client.chat.completions.create(
            model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
            messages=messages,  # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
            tools=tools,  # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ì „ë‹¬
        )
        return response  # ìƒì„±ëœ ì‘ë‹µ ë‚´ìš© ë°˜í™˜
    
    st.title("ğŸ’¬ Chatbot")   
    
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."},  # ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€
        ] 
    
    for msg in st.session_state.messages:
        if msg["role"] == "assistant" or msg["role"] == "user": # assistant í˜¹ì€ user ë©”ì‹œì§€ì¸ ê²½ìš°ë§Œ
            st.chat_message(msg["role"]).write(msg["content"])
    
    if user_input := st.chat_input():    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        st.session_state.messages.append({"role": "user", "content": user_input})  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        st.chat_message("user").write(user_input)  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¸Œë¼ìš°ì €ì—ì„œë„ ì¶œë ¥
        
        ai_response = get_ai_response(st.session_state.messages, tools=tools)
        ai_message = ai_response.choices[0].message
        print(ai_message)  # gptì—ì„œ ë°˜í™˜ë˜ëŠ” ê°’ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ì„ì‹œë¡œ ì¶”ê°€
    
        tool_calls = ai_message.tool_calls  # AI ì‘ë‹µì— í¬í•¨ëœ tool_callsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        if tool_calls:  # tool_callsê°€ ìˆëŠ” ê²½ìš°
            for tool_call in tool_calls:
                tool_name = tool_call.function.name # ì‹¤í–‰í•´ì•¼í•œë‹¤ê³  íŒë‹¨í•œ í•¨ìˆ˜ëª… ë°›ê¸°
                tool_call_id = tool_call.id         # tool_call ì•„ì´ë”” ë°›ê¸°    
                arguments = json.loads(tool_call.function.arguments) # ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜    
                
                if tool_name == "get_current_time":  
                    func_result = get_current_time(timezone=arguments['timezone'])
                elif tool_name == "get_yf_stock_info":
                    func_result = get_yf_stock_info(ticker=arguments['ticker'])
                elif tool_name == "get_yf_stock_history":  # get_yf_stock_history í•¨ìˆ˜ í˜¸ì¶œ
                    func_result = get_yf_stock_history(
                        ticker=arguments['ticker'], 
                        period=arguments['period']
                    )
                elif tool_name == "get_yf_stock_recommendations":  # get_yf_stock_recommendations í•¨ìˆ˜ í˜¸ì¶œ
                    func_result = get_yf_stock_recommendations(
                        ticker=arguments['ticker']
                    )
    
                st.session_state.messages.append({
                    "role": "function",
                    "tool_call_id": tool_call_id,
                    "name": tool_name,
                    "content": func_result,
                })
    
            st.session_state.messages.append({"role": "system", "content": "ì´ì œ ì£¼ì–´ì§„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•  ì°¨ë¡€ë‹¤."}) 
            ai_response = get_ai_response(st.session_state.messages, tools=tools) # ë‹¤ì‹œ GPT ì‘ë‹µ ë°›ê¸°
            ai_message = ai_response.choices[0].message
    
        st.session_state.messages.append({
            "role": "assistant",
            "content": ai_message.content
        })  #AI ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    
        print("AI\t: " + ai_message.content)  # AI ì‘ë‹µ ì¶œë ¥
        st.chat_message("assistant").write(ai_message.content)  # ë¸Œë¼ìš°ì €ì— ë©”ì‹œì§€ ì¶œë ¥
    ```
    
  <img width="760" height="648" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-16 á„‹á…©á„’á…® 5 35 22" src="https://github.com/user-attachments/assets/bf4a3ff5-1973-47f7-adc5-269baf4fcf34" />

    

07-3 ìŠ¤íŠ¸ë¦¼ ì¶œë ¥í•˜ê¸°

- [ì‹¤ìŠµ] í„°ë¯¸ë„ ì°½ì—ì„œ ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ê¸°
    
    ```python
    from gpt_functions import get_current_time, tools, get_yf_stock_info, get_yf_stock_history, get_yf_stock_recommendations
    from openai import OpenAI
    from dotenv import load_dotenv
    import os
    import json
    import streamlit as st
    
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    
    client = OpenAI(api_key=api_key)  # ì˜¤í”ˆAI í´ë¼ì´ì–¸íŠ¸ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    
    def get_ai_response(messages, tools=None, stream=True):
        response = client.chat.completions.create(
            model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì§€ì •í•©ë‹ˆë‹¤.
            stream=stream, # (1) ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ ìœ„í•´ ì„¤ì •
            messages=messages,  # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
            tools=tools,  # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ì„ ì „ë‹¬í•©ë‹ˆë‹¤.
        )
    
        if stream: 
            for chunk in response:
                yield chunk  # ìƒì„±ëœ ì‘ë‹µì˜ ë‚´ìš©ì„ yieldë¡œ ìˆœì°¨ì ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        else:
            return response  # ìƒì„±ëœ ì‘ë‹µì˜ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    st.title("ğŸ’¬ Chatbot")   
    
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."},  # ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€
        ] 
    
    for msg in st.session_state.messages:
        if msg["role"] == "assistant" or msg["role"] == "user": # assistant í˜¹ì€ user ë©”ì‹œì§€ì¸ ê²½ìš°ë§Œ
            st.chat_message(msg["role"]).write(msg["content"])
    
    if user_input := st.chat_input():    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        st.session_state.messages.append({"role": "user", "content": user_input})  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        st.chat_message("user").write(user_input)  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¸Œë¼ìš°ì €ì—ì„œë„ ì¶œë ¥
        
        ai_response = get_ai_response(st.session_state.messages, tools=tools)
        # print(ai_message) 
    
        content = ''
        for chunk in ai_response:
            content_chunk = chunk.choices[0].delta.content # ì²­í¬ ì† content ì¶”ì¶œ
            if content_chunk: # ë§Œì•½ content_chunkê°€ ìˆë‹¤ë©´, 
                print(content_chunk, end="")	 # í„°ë¯¸ë„ì— ì¤„ë°”ê¿ˆ ì—†ì´ ì´ì–´ì„œ ì¶œë ¥
                content += content_chunk # contentì— ë§ë¶™ì´ê¸°
            
        print('\n===========')
        print(content)
    
        ai_message = ai_response.choices[0].message
        tool_calls = ai_message.tool_calls  # AI ì‘ë‹µì— í¬í•¨ëœ tool_callsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        if tool_calls:  # tool_callsê°€ ìˆëŠ” ê²½ìš°
            for tool_call in tool_calls:
                tool_name = tool_call.function.name # ì‹¤í–‰í•´ì•¼í•œë‹¤ê³  íŒë‹¨í•œ í•¨ìˆ˜ëª… ë°›ê¸°
                tool_call_id = tool_call.id         # tool_call ì•„ì´ë”” ë°›ê¸°    
                arguments = json.loads(tool_call.function.arguments) # ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜    
                
                if tool_name == "get_current_time":  
                    func_result = get_current_time(timezone=arguments['timezone'])
                elif tool_name == "get_yf_stock_info":
                    func_result = get_yf_stock_info(ticker=arguments['ticker'])
                elif tool_name == "get_yf_stock_history":  # get_yf_stock_history í•¨ìˆ˜ í˜¸ì¶œ
                    func_result = get_yf_stock_history(
                        ticker=arguments['ticker'], 
                        period=arguments['period']
                    )
                elif tool_name == "get_yf_stock_recommendations":  # get_yf_stock_recommendations í•¨ìˆ˜ í˜¸ì¶œ
                    func_result = get_yf_stock_recommendations(
                        ticker=arguments['ticker']
                    )
    
                st.session_state.messages.append({
                    "role": "function",
                    "tool_call_id": tool_call_id,
                    "name": tool_name,
                    "content": func_result,
                })
    
            st.session_state.messages.append({"role": "system", "content": "ì´ì œ ì£¼ì–´ì§„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•  ì°¨ë¡€ë‹¤."}) 
            ai_response = get_ai_response(st.session_state.messages, tools=tools) # ë‹¤ì‹œ GPT ì‘ë‹µ ë°›ê¸°
            ai_message = ai_response.choices[0].message
    
        st.session_state.messages.append({
            "role": "assistant",
            "content": ai_message.content
        })  # AI ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    
        print("AI\t: " + ai_message.content)  # AI ì‘ë‹µ ì¶œë ¥
        st.chat_message("assistant").write(ai_message.content)  # ë¸Œë¼ìš°ì €ì— ë©”ì‹œì§€ ì¶œë ¥
    ```
    
- [ì‹¤ìŠµ] ìŠ¤íŠ¸ë¦¼ë¦¿ì—ì„œ ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ê¸°
    
    ```
    from gpt_functions import get_current_time, tools, get_yf_stock_info, get_yf_stock_history, get_yf_stock_recommendations
    from openai import OpenAI
    from dotenv import load_dotenv
    import os
    import json
    import streamlit as st
    
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    
    client = OpenAI(api_key=api_key)  # ì˜¤í”ˆAI í´ë¼ì´ì–¸íŠ¸ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    
    def get_ai_response(messages, tools=None, stream=True):
        response = client.chat.completions.create(
            model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì§€ì •í•©ë‹ˆë‹¤.
            stream=stream, # (1) ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ ìœ„í•´ ì„¤ì •
            messages=messages,  # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
            tools=tools,  # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ì„ ì „ë‹¬í•©ë‹ˆë‹¤.
        )
    
        if stream: 
            for chunk in response:
                yield chunk  # ìƒì„±ëœ ì‘ë‹µì˜ ë‚´ìš©ì„ yieldë¡œ ìˆœì°¨ì ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        else:
            return response  # ìƒì„±ëœ ì‘ë‹µì˜ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    st.title("ğŸ’¬ Chatbot")   
    
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."},  # ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€
        ] 
    
    for msg in st.session_state.messages:
        if msg["role"] == "assistant" or msg["role"] == "user": # assistant í˜¹ì€ user ë©”ì‹œì§€ì¸ ê²½ìš°ë§Œ
            st.chat_message(msg["role"]).write(msg["content"])
    
    if user_input := st.chat_input():    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        st.session_state.messages.append({"role": "user", "content": user_input})  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        st.chat_message("user").write(user_input)  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¸Œë¼ìš°ì €ì—ì„œë„ ì¶œë ¥
        
        ai_response = get_ai_response(st.session_state.messages, tools=tools)
        # print(ai_message) 
    
        content = ''
        tool_calls = None # # tool_calls ì´ˆê¸°í™”
        
        with st.chat_message("assistant").empty(): # ìŠ¤íŠ¸ë¦¼ë¦¿ ì±— ë©”ì‹œì§€ ì´ˆê¸°í™”
            for chunk in ai_response:
                content_chunk = chunk.choices[0].delta.content # ì²­í¬ ì† content ì¶”ì¶œ
                if content_chunk: # ë§Œì•½ content_chunkê°€ ìˆë‹¤ë©´, 
                    print(content_chunk, end="")	 # í„°ë¯¸ë„ì— ì¤„ë°”ê¿ˆ ì—†ì´ ì´ì–´ì„œ ì¶œë ¥
                    content += content_chunk # contentì— ë§ë¶™ì´ê¸°
                    st.markdown(content) # ìŠ¤íŠ¸ë¦¼ë¦¿ ì±— ë©”ì‹œì§€ì— ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì¶œë ¥
            
        print('\n===========')
        print(content)
    
        # ai_message = ai_response.choices[0].message
        # tool_calls = ai_message.tool_calls  # AI ì‘ë‹µì— í¬í•¨ëœ tool_callsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        if tool_calls:  # tool_callsê°€ ìˆëŠ” ê²½ìš°
            for tool_call in tool_calls:
                tool_name = tool_call.function.name # ì‹¤í–‰í•´ì•¼í•œë‹¤ê³  íŒë‹¨í•œ í•¨ìˆ˜ëª… ë°›ê¸°
                tool_call_id = tool_call.id         # tool_call ì•„ì´ë”” ë°›ê¸°    
                arguments = json.loads(tool_call.function.arguments) # ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜    
                
                if tool_name == "get_current_time":  
                    func_result = get_current_time(timezone=arguments['timezone'])
                elif tool_name == "get_yf_stock_info":
                    func_result = get_yf_stock_info(ticker=arguments['ticker'])
                elif tool_name == "get_yf_stock_history":  # get_yf_stock_history í•¨ìˆ˜ í˜¸ì¶œ
                    func_result = get_yf_stock_history(
                        ticker=arguments['ticker'], 
                        period=arguments['period']
                    )
                elif tool_name == "get_yf_stock_recommendations":  # get_yf_stock_recommendations í•¨ìˆ˜ í˜¸ì¶œ
                    func_result = get_yf_stock_recommendations(
                        ticker=arguments['ticker']
                    )
    
                st.session_state.messages.append({
                    "role": "function",
                    "tool_call_id": tool_call_id,
                    "name": tool_name,
                    "content": func_result,
                })
    
            st.session_state.messages.append({"role": "system", "content": "ì´ì œ ì£¼ì–´ì§„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•  ì°¨ë¡€ë‹¤."}) 
            ai_response = get_ai_response(st.session_state.messages, tools=tools) # ë‹¤ì‹œ GPT ì‘ë‹µ ë°›ê¸°
            ai_message = ai_response.choices[0].message
    
        st.session_state.messages.append({
            "role": "assistant",
            "content": content # ì›ë˜ëŠ” ai_message.content ì˜€ìŒ
        })  # â‘¢ AI ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    
        print("AI\t: " + content)  # AI ì‘ë‹µ ì¶œë ¥
        # st.chat_message("assistant").write(content)  # ìœ„ì—ì„œ ìŠ¤íŠ¸ë¦¼ ë°©ì‹ ì¶œë ¥í•˜ë¯€ë¡œ ë¶ˆí•„ìš”
    ```
    <img width="545" height="842" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-16 á„‹á…©á„’á…® 5 50 38" src="https://github.com/user-attachments/assets/eefa43dd-48d4-4a6c-b09c-49f612372dcd" />

- [ì‹¤ìŠµ] ìŠ¤íŠ¸ë¦¼ ë°©ì‹ì—ì„œ í‘ì…˜ ì½œë§ ì‚¬ìš©í•˜ê¸°
    
    ```python
    from gpt_functions import get_current_time, tools, get_yf_stock_info, get_yf_stock_history, get_yf_stock_recommendations
    from openai import OpenAI
    from dotenv import load_dotenv
    import os
    import json
    import streamlit as st
    from collections import defaultdict
    
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    
    client = OpenAI(api_key=api_key)  
    
    def tool_list_to_tool_obj(tools):
        # ê¸°ë³¸ ê°’ì„ ê°€ì§„ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
        tool_calls_dict = defaultdict(lambda: {"id": None, "function": {"arguments": "", "name": None}, "type": None})
    
        # ë„êµ¬(í•¨ìˆ˜) í˜¸ì¶œì„ ë°˜ë³µí•˜ì—¬ ì²˜ë¦¬
        for tool_call in tools:
            # idê°€ Noneì´ ì•„ë‹Œ ê²½ìš° ì„¤ì •
            if tool_call.id is not None:
                tool_calls_dict[tool_call.index]["id"] = tool_call.id
    
            # í•¨ìˆ˜ ì´ë¦„ì´ Noneì´ ì•„ë‹Œ ê²½ìš° ì„¤ì •
            if tool_call.function.name is not None:
                tool_calls_dict[tool_call.index]["function"]["name"] = tool_call.function.name
    
            # ì¸ìˆ˜ ì¶”ê°€
            tool_calls_dict[tool_call.index]["function"]["arguments"] += tool_call.function.arguments
    
            # íƒ€ì…ì´ Noneì´ ì•„ë‹Œ ê²½ìš° ì„¤ì •
            if tool_call.type is not None:
                tool_calls_dict[tool_call.index]["type"] = tool_call.type
    
        # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        tool_calls_list = list(tool_calls_dict.values())
    
        return {"tool_calls": tool_calls_list}  
    
    def get_ai_response(messages, tools=None, stream=True):
        response = client.chat.completions.create(
            model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì§€ì •í•©ë‹ˆë‹¤.
            stream=stream, # (1) ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ ìœ„í•´ ì„¤ì •
            messages=messages,  # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
            tools=tools,  # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ì„ ì „ë‹¬í•©ë‹ˆë‹¤.
        )
    
        if stream: 
            for chunk in response:
                yield chunk  # ìƒì„±ëœ ì‘ë‹µì˜ ë‚´ìš©ì„ yieldë¡œ ìˆœì°¨ì ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        else:
            return response  # ìƒì„±ëœ ì‘ë‹µì˜ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    st.title("ğŸ’¬ Chatbot")   
    
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."},  # ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€
        ] 
    
    for msg in st.session_state.messages:
        if msg["role"] == "assistant" or msg["role"] == "user": # assistant í˜¹ì€ user ë©”ì‹œì§€ì¸ ê²½ìš°ë§Œ
            st.chat_message(msg["role"]).write(msg["content"])
    
    if user_input := st.chat_input():    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        st.session_state.messages.append({"role": "user", "content": user_input})  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        st.chat_message("user").write(user_input)  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¸Œë¼ìš°ì €ì—ì„œë„ ì¶œë ¥
        
        ai_response = get_ai_response(st.session_state.messages, tools=tools)
        # print(ai_message) 
    
        content = ''
        tool_calls = None # # tool_calls ì´ˆê¸°í™”
        tool_calls_chunk = []   # tool_calls_chunk ì´ˆê¸°í™”
        
        with st.chat_message("assistant").empty(): # ìŠ¤íŠ¸ë¦¼ë¦¿ ì±— ë©”ì‹œì§€ ì´ˆê¸°í™”
            for chunk in ai_response:
                content_chunk = chunk.choices[0].delta.content # ì²­í¬ ì† content ì¶”ì¶œ
                if content_chunk: # ë§Œì•½ content_chunkê°€ ìˆë‹¤ë©´, 
                    print(content_chunk, end="")	 # í„°ë¯¸ë„ì— ì¤„ë°”ê¿ˆ ì—†ì´ ì´ì–´ì„œ ì¶œë ¥
                    content += content_chunk # contentì— ë§ë¶™ì´ê¸°
                    st.markdown(content) # ìŠ¤íŠ¸ë¦¼ë¦¿ ì±— ë©”ì‹œì§€ì— ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì¶œë ¥
                
                # print(chunk) # ì„ì‹œë¡œ ì²­í¬ ì¶œë ¥
                if chunk.choices[0].delta.tool_calls:	# tool_callsê°€ ìˆëŠ” ê²½ìš°
                    tool_calls_chunk += chunk.choices[0].delta.tool_calls # tool_calls_chunkì— ì¶”ê°€
    
        tool_obj = tool_list_to_tool_obj(tool_calls_chunk)
        tool_calls = tool_obj["tool_calls"]   
    
        if len(tool_calls) > 0: # ë§Œì•½ tool_callsê°€ ì¡´ì¬í•˜ë©´, st.writeë¡œ tool_call ë‚´ìš© ì¶œë ¥
            print(tool_calls)
            # tool_callsì—ì„œ function ì •ë³´ë§Œ ëª¨ì•„ì„œ ì¶œë ¥
            tool_call_msg = [tool_call["function"] for tool_call in tool_calls]
            st.write(tool_call_msg) 
    
        print('\n===========')
        print(content)
    
        # print('\n=========== tool_calls_chunk')  # tool_calls_chunk í™•ì¸í•˜ê¸° ìœ„í•œ ì½”ë“œ
        # for tool_call_chunk in tool_calls_chunk:
        #     print(tool_call_chunk)
    
        # tool_obj = tool_list_to_tool_obj(tool_calls_chunk) # ìœ„ë¡œ ì´ë™
        # tool_calls = tool_obj["tool_calls"] # ìœ„ë¡œ ì´ë™ë™
        print(tool_calls)
    
        if tool_calls:  # tool_callsê°€ ìˆëŠ” ê²½ìš°
            for tool_call in tool_calls:
                # tool_name = tool_call.function.name # ì‹¤í–‰í•´ì•¼í•œë‹¤ê³  íŒë‹¨í•œ í•¨ìˆ˜ëª… ë°›ê¸°
                # tool_call_id = tool_call.id         # tool_call ì•„ì´ë”” ë°›ê¸°    
                # arguments = json.loads(tool_call.function.arguments) # ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜    
    
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì—ì„œ ë°›ê¸°
                tool_name = tool_call["function"]["name"]  # ì‹¤í–‰í•´ì•¼í•œë‹¤ê³  íŒë‹¨í•œ í•¨ìˆ˜ëª… ë°›ê¸°
                tool_call_id = tool_call["id"]         # í•¨ìˆ˜ ì•„ì´ë”” ë°›ê¸°
                arguments = json.loads(tool_call["function"]["arguments"]) # ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜    
                
                if tool_name == "get_current_time":  
                    func_result = get_current_time(timezone=arguments['timezone'])
                elif tool_name == "get_yf_stock_info":
                    func_result = get_yf_stock_info(ticker=arguments['ticker'])
                elif tool_name == "get_yf_stock_history":  # get_yf_stock_history í•¨ìˆ˜ í˜¸ì¶œ
                    func_result = get_yf_stock_history(
                        ticker=arguments['ticker'], 
                        period=arguments['period']
                    )
                elif tool_name == "get_yf_stock_recommendations":  # get_yf_stock_recommendations í•¨ìˆ˜ í˜¸ì¶œ
                    func_result = get_yf_stock_recommendations(
                        ticker=arguments['ticker']
                    )
    
                st.session_state.messages.append({
                    "role": "function",
                    "tool_call_id": tool_call_id,
                    "name": tool_name,
                    "content": func_result,
                })
    
            st.session_state.messages.append({
                "role": "system", 
                "content": "ì´ì œ ì£¼ì–´ì§„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•  ì°¨ë¡€ë‹¤."
            }) 
            ai_response = get_ai_response(st.session_state.messages, tools=tools) # ë‹¤ì‹œ GPT ì‘ë‹µ ë°›ê¸°
            # ai_message = ai_response.choices[0].message
            content = ""
            with st.chat_message("assistant").empty():
                for chunk in ai_response:
                    content_chunk = chunk.choices[0].delta.content
                    if content_chunk:
                        print(content_chunk, end='')
                        content += content_chunk
                        st.markdown(content) # ìŠ¤íŠ¸ë¦¼ë¦¿ ì±—ë©”ì‹œì§€ì— markdownìœ¼ë¡œ ì¶œë ¥
    
        st.session_state.messages.append({
            "role": "assistant",
            "content": content # ì›ë˜ëŠ” ai_message.content ì˜€ìŒ
        })  # AI ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    
        print("AI\t: " + content)  # AI ì‘ë‹µ ì¶œë ¥
        # st.chat_message("assistant").write(content)  # ìœ„ì—ì„œ ìŠ¤íŠ¸ë¦¼ ë°©ì‹ ì¶œë ¥í•˜ë¯€ë¡œ ë¶ˆí•„ìš”
    ```
    <img width="775" height="777" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-16 á„‹á…©á„’á…® 5 57 37" src="https://github.com/user-attachments/assets/0cd430a5-b805-493c-8865-99aedc433e55" />

### 8ì¥ ë­ì²´ì¸ì„ í™œìš©í•œ ì—ì´ì „íŠ¸ ê°œë°œ

08-1 ë­ì²´ì¸ìœ¼ë¡œ ì±—ë´‡ë§Œë“¤ê¸°

- ë­ì²´ì¸ì´ë€?
    - ì–¸ì–´ ëª¨ë¸ì— ê¸°ë°˜í•œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‰½ê²Œ ê°œë°œí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” í”„ë ˆì„ì›Œí¬
    - ê¸°ì¡´ì— ì›í•˜ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ë ¤ë©´ ëª¨ë“  ì½”ë“œë¥¼ ì§ì ‘ ì‘ì„±í•´ì•¼í•˜ëŠ” ê³¼ì •ì„ ê°„ì†Œí™” í•  ìˆ˜ ìˆê²Œ ë„êµ¬ì™€ ëª¨ë“ˆ ì œê³µ
    - ë‹¤ë¥¸ ì–¸ì–´ ëª¨ë¸ë¡œ ì‰½ê²Œ êµì²´ ê°€ëŠ¥, íŠ¹ì • ëª¨ë¸ì— ì¢…ì†ë˜ì§€ ì•Šê³  ë‹¤ì–‘í•œ ëª¨ë¸ì˜ ì¥ì ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ ê°€ëŠ¥
- [ì‹¤ìŠµ] ë­ì²´ì¸ê³¼ ì˜¤í”ˆ AIì˜ GPT API ë¹„êµí•˜ê¸°

```
from langchain_openai import ChatOpenAI
model = ChatOpenAI(model= "gpt-4o-mini")
```

```python
from langchain_core.messages import HumanMessage
model.invoke([HumanMessage(content="ì•ˆë…•? ë‚˜ëŠ” ë¹„ë¹„ì•¼")])

#AIMessage(content='ì•ˆë…•, ë¹„ë¹„ì•¼! ì–´ë–»ê²Œ ì§€ë‚´? ë„ì›€ì´ í•„ìš”í•˜ë©´ ì–¸ì œë“ ì§€ ë§í•´ì¤˜.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 14, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CcT4zCW3zrlZO4KHmG0tVwS9WzEKO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--33ef68f0-80ea-4d82-9181-104e7b6db3cc-0', usage_metadata={'input_tokens': 14, 'output_tokens': 21, 'total_tokens': 35, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
```

ë­ì²´ì¸ì—ì„œëŠ” AIMessage(ì¸ê³µì§€ëŠ¥ ë‹µë³€, role: assistant), HumanMessage(role: user), SystemMessage(role: system)ì™€ ê°™ì´ ë‹¤ì–‘í•œ ë©”ì‹œì§€ íƒ€ì… ì œê³µ

```python
model.invoke([HumanMessage(content="ë‚´ ì´ë¦„ì´ ë­ì§€?")])

#AIMessage(content='ì£„ì†¡í•˜ì§€ë§Œ, ë‹¹ì‹ ì˜ ì´ë¦„ì„ ì•Œ ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ì—†ì–´ìš”. í˜¹ì‹œ ì•Œë ¤ì£¼ì‹œë©´ ì¢‹ê² ìŠµë‹ˆë‹¤!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 13, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CcT63FbEK6xIFlSDk2xezW9pQJfNB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--0bd777a7-67ca-45cf-be48-2e68bf1e9a64-0', usage_metadata={'input_tokens': 13, 'output_tokens': 24, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
```

- [ì‹¤ìŠµ] ë­ì²´ì¸ìœ¼ë¡œ ë©€í‹°í„´ ëŒ€í™”í•˜ê¸°
    
    ```
    # from openai import OpenAI  # ì£¼ì„ì²˜ë¦¬
    from dotenv import load_dotenv
    # import os
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
    
    load_dotenv()
    # api_key = os.getenv("OPENAI_API_KEY")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    # client = OpenAI(api_key=api_key)  # ì˜¤í”ˆAI í´ë¼ì´ì–¸íŠ¸ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    
    llm = ChatOpenAI(model="gpt-4o")  # ChatOpenAI í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    
    # def get_ai_response(messages):
    #     response = client.chat.completions.create(
    #         model="gpt-4o",  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
    #         temperature=0.9,  # ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  temperature ì„¤ì •
    #         messages=messages,  # ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
    #     )
    #     return response.choices[0].message.content  # ìƒì„±ëœ ì‘ë‹µì˜ ë‚´ìš© ë°˜í™˜
    
    messages = [
        # {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."},  # ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€
        SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."),  # ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€
    ]
    
    while True:
        user_input = input("ì‚¬ìš©ì: ")  # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    
        if user_input == "exit":  # ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ë ¤ëŠ”ì§€ í™•ì¸ì¸
            break
        
        messages.append(
            # {"role": "user", "content": user_input} # ì£¼ì„ì²˜ë¦¬
            HumanMessage(user_input)
        )  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€ 
        
        # ai_response = get_ai_response(messages)  # ì£¼ì„ì²˜ë¦¬
        ai_response = llm.invoke(messages)  # ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ AI ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°
        messages.append(
            # {"role": "assistant", "content": ai_response} # ì£¼ì„ì²˜ë¦¬
            ai_response
        )  # AI ì‘ë‹µ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•˜ê¸°
    
        print("AI: " + ai_response.content)  # AI ì‘ë‹µ ì¶œë ¥
    
    ```
    
    ```python
    ì‚¬ìš©ì:  ì•ˆë…• ë‚œ ë¹„ë¹„ì•¼
    AI: ì•ˆë…• ë¹„ë¹„! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ. ì–´ë–»ê²Œ ë„ì™€ì¤„ê¹Œ?
    ì‚¬ìš©ì: ë‚˜ì— ëŒ€í•´ ì•Œê³   ìˆëŠ” ê±¸ ë§í•´ì¤˜
    AI: ë¹„ë¹„ì— ëŒ€í•´ ë‚´ê°€ ì•Œê³  ìˆëŠ” ê±´ ë³„ë¡œ ì—†ë„¤. ë§Œì•½ ë” ì•Œê³  ì‹¶ë‹¤ë©´ ì •ë³´ë¥¼ ì¢€ ë” ê³µìœ í•´ì¤„ë˜? ê·¸ëŸ¬ë©´ ë” ìì„¸íˆ ì´ì•¼ê¸° ë‚˜ëˆŒ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„!
    ```
    
    ë­ì²´ì¸ì˜ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬
    
    ```python
    from langchain_core.chat_history import InMemoryChatMessageHistory  # ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤
    from langchain_core.runnables.history import RunnableWithMessageHistory  # ë©”ì‹œì§€ ê¸°ë¡ì„ í™œìš©í•´ ì‹¤í–‰ ê°€ëŠ¥í•œ ë˜í¼wrapper í´ë˜ìŠ¤
    from langchain_openai import ChatOpenAI  # ì˜¤í”ˆAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë­ì²´ì¸ ì±—ë´‡ í´ë˜ìŠ¤
    from langchain_core.messages import HumanMessage
    
    model = ChatOpenAI(model="gpt-4o-mini")
    
    # ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    store = {}
    
    # ì„¸ì…˜ IDì— ë”°ë¼ ëŒ€í™” ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    def get_session_history(session_id: str):
        # ë§Œì•½ í•´ë‹¹ ì„¸ì…˜ IDê°€ storeì— ì—†ìœ¼ë©´, ìƒˆë¡œ ìƒì„±í•´ ì¶”ê°€í•¨
        if session_id not in store:
            store[session_id] = InMemoryChatMessageHistory()  # ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ê°ì²´ ìƒì„±
        return store[session_id]  # í•´ë‹¹ ì„¸ì…˜ì˜ ëŒ€í™” ê¸°ë¡ì„ ë°˜í™˜
    
    # ëª¨ë¸ ì‹¤í–‰ ì‹œ ëŒ€í™” ê¸°ë¡ì„ í•¨ê»˜ ì „ë‹¬í•˜ëŠ” ë˜í¼ ê°ì²´ ìƒì„±
    with_message_history = RunnableWithMessageHistory(model, get_session_history)
    ```
    
    ```python
    config = {"configurable": {"session_id": "abc2"}}  # ì„¸ì…˜ IDë¥¼ ì„¤ì •í•˜ëŠ” config ê°ì²´ ìƒì„±
    
    response = with_message_history.invoke(
        [HumanMessage(content="ì•ˆë…•? ë‚œ ë¹„ë¹„ì•¼.")],
        config=config,
    )
    
    print(response.content)
    
    # ì•ˆë…•, ë¹„ë¹„ì•¼! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ. ì–´ë–»ê²Œ ì§€ë‚´?
    ```
    
    ```python
    response = with_message_history.invoke(
        [HumanMessage(content="ë‚´ ì´ë¦„ì´ ë­ì§€?")],
        config=config,
    )
    
    print(response.content)
    
    # ë„ˆì˜ ì´ë¦„ì€ ë¹„ë¹„ì•¼ì•¼! ë§ì§€?
    ```
    
    ì„¸ì…˜ id ë³€ê²½í•˜ë©´ ì´ì „ ëŒ€í™” ê¸°ì–µ ëª»í•¨
    
    ```python
    config = {"configurable": {"session_id": "abc3"}}
    
    response = with_message_history.invoke(
        [HumanMessage(content="ë‚´ ì´ë¦„ì´ ë­ì§€?")],
        config=config,
    )
    
    response.content
    
    # 'ì£„ì†¡í•˜ì§€ë§Œ, ë‹¹ì‹ ì˜ ì´ë¦„ì„ ì•Œ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì—†ìŠµë‹ˆë‹¤. ì´ë¦„ì´ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ì§ì ‘ ë§ì”€í•´ì£¼ì‹œë©´ ì¢‹ê² ì–´ìš”!'
    ```
    
    ```python
    config = {"configurable": {"session_id": "abc2"}}
    
    response = with_message_history.invoke(
        [HumanMessage(content="ì•„ê¹Œ ìš°ë¦¬ê°€ ë¬´ìŠ¨ ì–˜ê¸° í–ˆì§€?")],
        config=config,
    )
    
    response.content
    
    # 'ìš°ë¦¬ëŠ” ë„ˆì˜ ì´ë¦„ì´ ë¹„ë¹„ì•¼ë¼ëŠ” ê±¸ í™•ì¸í–ˆì–´. ê·¸ë¦¬ê³  ë„¤ê°€ ì–´ë–»ê²Œ ì§€ë‚´ëŠ”ì§€ë„ ë¬¼ì–´ë´¤ì—ˆì§€. ë‹¤ë¥¸ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ê³  ì‹¶ìœ¼ë©´ ë§í•´ì¤˜!'
    ```
    
    ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ê¸°
    
    ```python
    config = {"configurable": {"session_id": "abc2"}}
    for r in with_message_history.stream(
        [HumanMessage(content = "ë‚´ê°€ ì–´ëŠ ë‚˜ë¼ ì‚¬ëŒì¸ì§€ ë§ì¶°ë³´ê³ , ê·¸ ë‚˜ë¼ì˜ êµ­ê°€ë¥¼ ë¶ˆëŸ¬ì¤˜.")],
        config=config,
    ):
        print(r.content, end="|")
        
      #|ë„ˆ|ì˜| ì´ë¦„|ì´| ë¹„|ë¹„|ì•¼|ì¸| ê²ƒìœ¼ë¡œ| ë¯¸|ë£¨|ì–´| ë³´|ì•„|,| ì¸|ë„| ì¶œ|ì‹ |ì¼| ìˆ˜ë„| ìˆì„| ê²ƒ| ê°™|ì•„|.| ë§|ë‹¤ë©´|,| ì¸|ë„ì˜| êµ­ê°€|ì¸| "|J|ana| G|ana| Mana|"|ë¥¼| ë“¤|ë ¤|ì¤„| ìˆ˜| ìˆì–´|.| ê³¡|ì˜| ì‹œì‘| ë¶€ë¶„|ì€| ë‹¤ìŒ|ê³¼| ê°™|ì•„|:
    
    #|**|J|ana| gana| mana| ad|hin|ay|aka| j|aya| he|,|  
    #|B|har|at| bh|ag|ya| v|idh|ata|...|**
    
    #|ì¸|ë„|ëŠ”| ë‹¤ì–‘í•œ| ë¬¸í™”|,| ì–¸|ì–´|,| ì¢…|êµ|ê°€| ê³µ|ì¡´|í•˜ëŠ”| ë‚˜ë¼|ë¡œ|,| ê°| ì§€ì—­|ë§ˆë‹¤| ë…|íŠ¹|í•œ| ì „|í†µ|ê³¼| ê´€|ìŠµ|ì´| ìˆì–´|.| ë”| ì•Œê³ | ì‹¶ì€| ì •ë³´|ê°€| ìˆ|ìœ¼ë©´| ë§|í•´|ì¤˜|!||||
    ```
    
    ```python
    config = {"configurable": {"session_id": "abc2"}}
    for r in with_message_history.stream(
        [HumanMessage(content = "ë‚´ê°€ ì–´ëŠ ë‚˜ë¼ ì‚¬ëŒì¸ì§€ ë§ì¶°ë³´ê³ , ê·¸ ë‚˜ë¼ì˜ ë¬¸í™”ì— ëŒ€í•´ ë§í•´ë´")],
        config=config,
    ):
        print(r.content, end="|")
        
      #|ì¢‹|ì•„|!| ë„ˆ|ì˜| ì´ë¦„|ì´| ë¹„|ë¹„|ì•¼|ë‹ˆê¹Œ|,| ì•„|ë§ˆ| í•œêµ­| ì¶œ|ì‹ |ì¼| ìˆ˜ë„| ìˆì„| ê²ƒ| ê°™|ì•„|.| í˜¹|ì‹œ| ë§|ì•„|?| 
    
    #|í•œêµ­|ì˜| ë¬¸í™”|ì—| ëŒ€í•´| ê°„|ë‹¨|íˆ| ë§|í•˜|ì|ë©´|,| í•œêµ­|ì€| ì „|í†µ|ê³¼| í˜„ëŒ€|ê°€| ì¡°|í™”|ë¡­ê²Œ| ì–´|ìš°|ëŸ¬|ì§„| ë‚˜|ë¼ì´|ë©°|,| K|-pop|ê³¼| í•œ|ë¥˜| ë“œ|ë¼ë§ˆ|ê°€| ì„¸ê³„|ì ìœ¼ë¡œ| ìœ ëª…|í•´|.| ê·¸ë¦¬ê³ | í•œêµ­| ìŒì‹|,| íŠ¹íˆ| ê¹€|ì¹˜|ì™€| ë¹„|ë¹”|ë°¥| ê°™ì€| ë‹¤ì–‘í•œ| ì „|í†µ| ìš”|ë¦¬|ë„| ë§ì´| ì•Œë ¤|ì ¸| ìˆì–´|.| í•œêµ­|ì˜| ëª…|ì ˆ|ì¸| ì„¤|ë‚ |ê³¼| ì¶”|ì„|ì€| ê°€ì¡±|ê³¼| í•¨ê»˜| ëª¨|ì—¬| ìŒ|ì‹ì„| ë‚˜|ëˆ„|ê³ | ì¡°|ìƒì„| ê¸°|ë¦¬ëŠ”| ì¤‘ìš”í•œ| ê¸°|íšŒ|ì´|ê¸°ë„| í•´|.| 
    
    #|ì–´|ë–¤| ë¬¸í™”|ì—| ëŒ€í•´| ë”| ì•Œê³ | ì‹¶|ì–´|?| ì•„ë‹ˆ|ë©´| ë„ˆ|ì˜| ê³ |í–¥|ì—| ëŒ€í•´| ì¢€| ë”| ì´ì•¼ê¸°|í•´|ì£¼|ê³ | ì‹¶|ë‹ˆ|?||||
    ```
    

08-2 LCELë¡œ ì²´ì¸ ë§Œë“¤ê¸°

- LCEL(LangChain Expression Language)
    - ë­ì²´ì¸ì—ì„œ ë³µì¡í•œ ì‘ì—… íë¦„ì„ ê°„ë‹¨í•˜ê²Œ ë§Œë“¤ê³  ê´€ë¦¬í•  ìˆ˜ ìˆê²Œ ë•ëŠ” ë„êµ¬
    - ì—¬ëŸ¬ ì¤„ì„ í‘œí˜„í•´ì•¼í•˜ëŠ” ì‘ì—… ë‹¨ê³„ë¥¼ ì½ê¸° ì‰½ê²Œ ì¶•ì•½, ìŠ¤íŠ¸ë¦¼ ì¶œë ¥ ë“± ì—¬ëŸ¬ ì‘ì—…ì„ ë³‘ë ¬ ì²˜ë¦¬
- [ì‹¤ìŠµ] ì¶œë ¥ íŒŒì„œì™€ ì²´ì¸
GPTì—ê²Œ ë¯¸ë…€ì™€ ì•¼ìˆ˜ ì´ì•¼ê¸°ì˜ ë¯¸ë…€ ì—­í•  ë¶€ì—¬í•˜ê³  ëŒ€í™”í•˜ê¸°
    
    ```python
    from langchain_openai import ChatOpenAI
    model = ChatOpenAI(model="gpt-4o-mini")
    
    from langchain_core.messages import HumanMessage, SystemMessage
    
    messages = [
        SystemMessage(content="ë„ˆëŠ” ë¯¸ë…€ì™€ ì•¼ìˆ˜ì— ë‚˜ì˜¤ëŠ” ë¯¸ë…€ì•¼. ê·¸ ìºë¦­í„°ì— ë§ê²Œ ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ë¼."),
        HumanMessage(content="ì•ˆë…•? ì €ëŠ” ê°œìŠ¤í†¤ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‹œê°„ ê´œì°®ìœ¼ì‹œë©´ ì €ë… ê°™ì´ ë¨¹ì„ê¹Œìš”?"),
    ]
    
    model.invoke(messages)
    
    # AIMessage(content='ì•ˆë…•, ê°œìŠ¤í†¤! ì •ë§ ê³ ë§™ì§€ë§Œ, ë‚˜ëŠ” ê·¸ëŸ° ì €ë…ìë¦¬ë¥¼ ìƒê°í•˜ê³  ìˆì§€ ì•Šì•„.
    # ë‚˜ëŠ” ì§„ì •í•œ ì‚¬ë‘ê³¼ ì•„ë¦„ë‹¤ì›€ì„ ì°¾ê³  ì‹¶ì–´. ê²Œë‹¤ê°€, ë‹¹ì‹ ì˜ ë„ì „ì ì¸ ì„±ê²©ì€ ë‚˜ì—ê²ŒëŠ” ì¡°ê¸ˆ ë¶€ë‹´ìŠ¤ëŸ¬ì›Œ. 
    # ì¢€ ë” ë¶€ë“œëŸ½ê³  ë”°ëœ»í•œ ê´€ê³„ë¥¼ ì›í•´. ë‹¤ë¥¸ ì‚¬ëŒê³¼ í•¨ê»˜ ì¦ê±°ìš´ ì €ë…ì„ ë³´ë‚´ë³´ëŠ” ê±´ ì–´ë•Œìš”?'
    ```
    
    í…ìŠ¤íŠ¸ ê²°ê³¼ë§Œ í•„ìš”í•˜ë©´ StrOutPutParser ì‚¬ìš©, í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
    
    ```python
    from langchain_core.output_parsers import StrOutputParser
    
    parser = StrOutputParser()
    
    result = model.invoke(messages)
    parser.invoke(result)
    
    # 'ì•ˆë…•í•˜ì„¸ìš”, ê°œìŠ¤í†¤. ì €ë… ì´ˆëŒ€ëŠ” ì •ë§ ê°ì‚¬í•˜ì§€ë§Œâ€¦ ì €ì—ê²ŒëŠ” ë‹¤ë¥¸ ìƒê°ê³¼ ê¿ˆì´ ìˆë‹µë‹ˆë‹¤. 
    #ì œê°€ ì›í•˜ëŠ” ê²ƒì€ ì§„ì •í•œ ì‚¬ë‘ê³¼ ì´í•´ë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒì´ì—ìš”. ë‹¹ì‹ ì˜ ë§¤ë ¥ì´ ë„ˆë¬´ë‚˜ë„ ê°•í•˜ì§€ë§Œ, 
    #ì €ì˜ ë§ˆìŒì€ ë‹¤ë¥¸ ê³³ì— ìˆë‹µë‹ˆë‹¤. í•¨ê»˜ ì‹œê°„ì„ ë³´ë‚´ëŠ” ëŒ€ì‹ , ìš°ë¦¬ ì„œë¡œì˜ ê¿ˆì— ëŒ€í•´ ì´ì•¼ê¸°í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?'
    ```
    

```python
chain = model | parser
chain.invoke(messages)

# 'ì•ˆë…•í•˜ì„¸ìš”, ê°œìŠ¤í†¤! ë‹¹ì‹ ì˜ ì´ˆëŒ€ëŠ” ì •ë§ ê³ ë§™ì§€ë§Œ, ì €ëŠ” ì €ë…ì„ í•¨ê»˜ í•˜ëŠ” ê²ƒë³´ë‹¤ëŠ” ì œ ì•„ë²„ì§€ì™€ í•¨ê»˜ ì‹œê°„ì„ ë³´ë‚´ê³  ì‹¶ì–´ìš”. ë‚˜ì˜ ë§ˆìŒì€ ë‹¤ë¥¸ ê³³ì— ìˆëŠ” ê²ƒ ê°™ì•„ìš”. ë‹¹ì‹ ë„ ì¢‹ì€ ì‚¬ëŒì´ë¼ ìƒê°í•˜ì§€ë§Œ, ì œê°€ ì›í•˜ëŠ” ê±´ ë‹¤ë¥¸ ê±°ëë‹ˆë‹¤. ì•„ë§ˆë„ ë‹¹ì‹ ë„ ì´í•´í•´ ì£¼ì‹¤ ê±°ë¼ê³  ë¯¿ì–´ìš”.'
```

- [ì‹¤ìŠµ] í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ìš©í•˜ê¸°
    
    ```python
    from langchain_core.prompts import ChatPromptTemplate
    
    system_template = "ë„ˆëŠ” {story}ì— ë‚˜ì˜¤ëŠ” {character_a} ì—­í• ì´ë‹¤. ê·¸ ìºë¦­í„°ì— ë§ê²Œ ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ë¼."
    human_template = "ì•ˆë…•? ì €ëŠ” {character_b}ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‹œê°„ ê´œì°®ìœ¼ì‹œë©´ {activity} ê°™ì´ í• ê¹Œìš”?"
    
    prompt_template = ChatPromptTemplate([
        ("system", system_template),
        ("user", human_template),
    ])
    
    result = prompt_template.invoke({
        "story": "ë¯¸ë…€ì™€ ì•¼ìˆ˜",
        "character_a": "ë¯¸ë…€",
        "character_b": "ì•¼ìˆ˜",
        "activity": "ì €ë…"
    })
    
    print(result)
    
    # messages=[SystemMessage(content='ë„ˆëŠ” ë¯¸ë…€ì™€ ì•¼ìˆ˜ì— ë‚˜ì˜¤ëŠ” ë¯¸ë…€ ì—­í• ì´ë‹¤. ê·¸ ìºë¦­í„°ì— ë§ê²Œ ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ë¼.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ì•ˆë…•? ì €ëŠ” ì•¼ìˆ˜ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‹œê°„ ê´œì°®ìœ¼ì‹œë©´ ì €ë… ê°™ì´ í• ê¹Œìš”?', additional_kwargs={}, response_metadata={})]
    ```
    
    ```python
    chain = prompt_template | model | parser
    
    chain.invoke({
        "story": "ë¯¸ë…€ì™€ ì•¼ìˆ˜",
        "character_a": "ë¯¸ë…€",
        "character_b": "ì•¼ìˆ˜",
        "activity": "ì €ë…"
    })
    
    # 'ì•ˆë…•í•˜ì„¸ìš”, ì•¼ìˆ˜ë‹˜! ì €ë… í•¨ê»˜ ë¨¹ëŠ” ê±´ ì •ë§ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”. ë‹¹ì‹ ê³¼ í•¨ê»˜í•˜ëŠ” ì‹œê°„ì´ ê¸°ëŒ€ë¼ìš”. ì–´ë–¤ ìŒì‹ì„ ì¢‹ì•„í•˜ì‹œë‚˜ìš”?'
    ```
    

08-3 ë­ì²´ì¸ ë„êµ¬ë¡œ ì—ì´ì „íŠ¸ ë§Œë“¤ê¸°

- [ì‹¤ìŠµ] @tool ë°ì½”ë ˆì´í„°ë¡œ ë­ì²´ì¸ì— í•¨ìˆ˜ ì—°ê²°í•˜ê¸°
@tool ë°ì½”ë ˆì´í„° ì‚¬ìš©í•˜ë©´ í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ë³€í™˜ O
* ë°ì½”ë ˆì´í„°: íŒŒì´ì¬ì—ì„œ í•¨ìˆ˜ì˜ ë™ì‘ì„ ìˆ˜ì •í•˜ê±°ë‚˜ í™•ì¥í•˜ëŠ” ë° ì‚¬ìš©í•˜ëŠ” ë„êµ¬ë¡œ, ì›ë˜ì˜ í•¨ìˆ˜ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³  ì¶”ê°€ ê¸°ëŠ¥ì„ ë§ë¶™ì¼ ìˆ˜ ìˆê²Œ í•´ì¤Œ

ì–¸ì–´ ëª¨ë¸ ì„ ì–¸í•˜ê¸°
    
    ```python
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage
    llm = ChatOpenAI(model="gpt-4o-mini")
    
    llm.invoke([HumanMessage("ì˜ ì§€ëƒˆì–´?")])
    
    # AIMessage(content='ì €ëŠ” ì˜ ì§€ë‚´ê³  ìˆìŠµë‹ˆë‹¤! ë‹¹ì‹ ì€ ì–´ë–»ê²Œ ì§€ë‚´ê³  ìˆë‚˜ìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 12, 'total_tokens': 29, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CcXhSDH99EvuKZxCfW8VwHIEEvRGH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--c54dbed1-2132-48b9-b994-bdd6b64b0ceb-0', usage_metadata={'input_tokens': 12, 'output_tokens': 17, 'total_tokens': 29, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
    ```
    
    ë­ì²´ì¸ì— ì‹œê°„ì„ íŒŒì•…í•˜ëŠ” ë„êµ¬ ì¶”ê°€í•˜ê¸°
    
    ```
    from langchain_core.tools import tool
    from datetime import datetime
    import pytz
    
    @tool # @tool ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ë“±ë¡
    def get_current_time(timezone: str, location: str) -> str:
        """ í˜„ì¬ ì‹œê°ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    
        Args:
            timezone (str): íƒ€ì„ì¡´ (ì˜ˆ: 'Asia/Seoul') ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íƒ€ì„ì¡´ì´ì–´ì•¼ í•¨
            location (str): ì§€ì—­ëª…. íƒ€ì„ì¡´ì´ ëª¨ë“  ì§€ëª…ì— ëŒ€ì‘ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´í›„ llm ë‹µë³€ ìƒì„±ì— ì‚¬ìš©ë¨
        """
        tz = pytz.timezone(timezone)
        now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        location_and_local_time = f'{timezone} ({location}) í˜„ì¬ì‹œê° {now} ' # íƒ€ì„ì¡´, ì§€ì—­ëª…, í˜„ì¬ì‹œê°ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜
        print(location_and_local_time)
        return location_and_local_time
    
    ```
    
    ```python
    # í•¨ìˆ˜ get_current_timeì„ ë­ì²´ì¸ìœ¼ë¡œ llmì— ì—°ê²°
    # ë„êµ¬ë¥¼ tools ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ê³ , tool_dictì—ë„ ì¶”ê°€
    tools = [get_current_time,]
    tool_dict = {"get_current_time": get_current_time,}
    
    # ë„êµ¬ë¥¼ ëª¨ë¸ì— ë°”ì¸ë”©: ëª¨ë¸ì— ë„êµ¬ë¥¼ ë°”ì¸ë”©í•˜ë©´, ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ llm ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìˆìŒ
    llm_with_tools = llm.bind_tools(tools)
    ```
    
    ```python
    # ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ ì–¸ì–´ ëª¨ë¸ ë‹µë³€ ìƒì„±
    from langchain_core.messages import SystemMessage
    
    # (4) ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ tools ì‚¬ìš©í•˜ì—¬ llm ë‹µë³€ ìƒì„±
    messages = [
        SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€ì„ í•˜ê¸° ìœ„í•´ toolsë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤."),
        HumanMessage("ë¶€ì‚°ì€ ì§€ê¸ˆ ëª‡ì‹œì•¼?"),
    ]
    
    # (5) llm_with_toolsë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ llm ë‹µë³€ ìƒì„±
    response = llm_with_tools.invoke(messages)
    messages.append(response)
    
    # (6) ìƒì„±ëœ llm ë‹µë³€ ì¶œë ¥
    print(messages)
    
    # [SystemMessage(content='ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€ì„ í•˜ê¸° ìœ„í•´ toolsë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ë¶€ì‚°ì€ ì§€ê¸ˆ ëª‡ì‹œì•¼?', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 135, 'total_tokens': 158, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CcXkp4hIL9hqCjVLD74r9irnPNNjV', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--8fe3f864-3723-4368-b58d-be5c57fb5c23-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'Asia/Seoul', 'location': 'Busan'}, 'id': 'call_A5x0Tq1ZET55fyulO8c65CcJ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 135, 'output_tokens': 23, 'total_tokens': 158, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]
    ```
    

```python
# í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ ì¶œë ¥í•˜ê¸° 
for tool_call in response.tool_calls:
    selected_tool = tool_dict[tool_call["name"]] # (7) tool_dictë¥¼ ì‚¬ìš©í•˜ì—¬ ë„êµ¬ í•¨ìˆ˜ë¥¼ ì„ íƒ
    print(tool_call["args"]) # (8) ë„êµ¬ í˜¸ì¶œ ì‹œ ì „ë‹¬ëœ ì¸ì ì¶œë ¥
    tool_msg = selected_tool.invoke(tool_call) # (9) ë„êµ¬ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜
    messages.append(tool_msg)

messages

# {'timezone': 'Asia/Seoul', 'location': 'Busan'}
# Asia/Seoul (Busan) í˜„ì¬ì‹œê° 2025-11-16 23:13:12 
# ...
# ToolMessage(content='Asia/Seoul (Busan) í˜„ì¬ì‹œê° 2025-11-16 23:13:12 ', name='get_current_time', tool_call_id='call_A5x0Tq1ZET55fyulO8c65CcJ')]
```

```python
# í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ì¶œë ¥í•˜ê¸° 
llm_with_tools.invoke(messages)

# AIMessage(content='ë¶€ì‚°ì€ ì§€ê¸ˆ 2025ë…„ 11ì›” 16ì¼ 23ì‹œ 13ë¶„ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 192, 'total_tokens': 215, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CcXoohwHDWKH1FkihSanUeOccf7T0', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--50514a96-9c8a-4ffa-ab47-6fbedc662464-0', usage_metadata={'input_tokens': 192, 'output_tokens': 23, 'total_tokens': 215, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
```

- [ì‹¤ìŠµ] íŒŒì´ë‹¨í‹± ì´ìš©í•˜ê¸°
    - íŒŒì´ë‹¨í‹±: ì…ë ¥ëœ ë°ì´í„°ì˜ ìœ íš¨ì„±ê³¼ í˜•ì‹ì„ ê²€ì¦í•˜ê³  íŠ¹ì • ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ëª…í™•í•˜ê²Œ í‘œí˜„í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
    
    ```python
    from pydantic import BaseModel, Field
    
    class StockHistoryInput(BaseModel):
        ticker: str = Field(..., title="ì£¼ì‹ ì½”ë“œ", description="ì£¼ì‹ ì½”ë“œ (ì˜ˆ: AAPL)")
        period: str = Field(..., title="ê¸°ê°„", description="ì£¼ì‹ ë°ì´í„° ì¡°íšŒ ê¸°ê°„ (ì˜ˆ: 1d, 1mo, 1y)")
    
    ```
    
    ```python
    import yfinance as yf
    
    @tool
    def get_yf_stock_history(stock_history_input: StockHistoryInput) -> str:
        """ ì£¼ì‹ ì¢…ëª©ì˜ ê°€ê²© ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜"""
        stock = yf.Ticker(stock_history_input.ticker)
        history = stock.history(period=stock_history_input.period)
        history_md = history.to_markdown() 
    
        return history_md
    
    tools = [get_current_time, get_yf_stock_history]
    tool_dict = {"get_current_time": get_current_time, "get_yf_stock_history": get_yf_stock_history}
    
    llm_with_tools = llm.bind_tools(tools)
    ```
    
    ```python
    messages.append(HumanMessage("í…ŒìŠ¬ë¼ëŠ” í•œë‹¬ ì „ì— ë¹„í•´ ì£¼ê°€ê°€ ì˜¬ëë‚˜ ë‚´ë ¸ë‚˜?"))
    
    response = llm_with_tools.invoke(messages)
    print(response)
    messages.append(response)
    
    # content='' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 283, 'total_tokens': 310, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CcXuzBbvPAovB58a5V2WDOqdkx418', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--29a8c51d-180f-4fdb-8587-2a98739651b9-0' tool_calls=[{'name': 'get_yf_stock_history', 'args': {'stock_history_input': {'ticker': 'TSLA', 'period': '1mo'}}, 'id': 'call_yH5YPrNI66SuYENxM9x1LTfr', 'type': 'tool_call'}] usage_metadata={'input_tokens': 283, 'output_tokens': 27, 'total_tokens': 310, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}
    ```
    
    ```python
    # ê²°ê³¼ë¥¼ ë©”ì‹œì§€ë¡œ ì¶”ê°€í•˜ê¸°
    for tool_call in response.tool_calls:
        selected_tool = tool_dict[tool_call["name"]]
        print(tool_call["args"])
        tool_msg = selected_tool.invoke(tool_call)
        messages.append(tool_msg)
        print(tool_msg)
    ```
    

```markdown
{'stock_history_input': {'ticker': 'TSLA', 'period': '1mo'}}
content='| Date                      |   Open |   High |    Low |   Close |      Volume |   Dividends |   Stock Splits |\n|:--------------------------|-------:|-------:|-------:|--------:|------------:|------------:|---------------:|\n| 2025-10-15 00:00:00-04:00 | 434.9  | 440.51 | 426.33 |  435.15 | 7.15582e+07 |           0 |              0 |\n| 2025-10-16 00:00:00-04:00 | 434.73 | 439.35 | 421.31 |  428.75 | 7.71899e+07 |           0 |              0 |\n| 2025-10-17 00:00:00-04:00 | 425.5  | 441.46 | 423.6  |  439.31 | 8.93316e+07 |           0 |              0 |\n| 2025-10-20 00:00:00-04:00 | 443.87 | 449.8  | 440.61 |  447.43 | 6.3719e+07  |           0 |              0 |\n| 2025-10-21 00:00:00-04:00 | 445.76 | 449.3  | 442.05 |  442.6  | 5.44122e+07 |           0 |              0 |\n| 2025-10-22 00:00:00-04:00 | 443.45 | 445.54 | 429    |  438.97 | 8.40235e+07 |           0 |              0 |\n| 2025-10-23 00:00:00-04:00 | 420    | 449.4  | 413.9  |  448.98 | 1.2671e+08  |           0 |              0 |\n| 2025-10-24 00:00:00-04:00 | 446.83 | 451.68 | 430.17 |  433.72 | 9.47278e+07 |           0 |              0 |\n| 2025-10-27 00:00:00-04:00 | 439.98 | 460.16 | 438.69 |  452.42 | 1.05868e+08 |           0 |              0 |\n| 2025-10-28 00:00:00-04:00 | 454.78 | 467    | 451.6  |  460.55 | 8.01857e+07 |           0 |              0 |\n| 2025-10-29 00:00:00-04:00 | 462.5  | 465.7  | 452.65 |  461.51 | 6.79835e+07 |           0 |              0 |\n| 2025-10-30 00:00:00-04:00 | 451.05 | 455.06 | 439.61 |  440.1  | 7.24479e+07 |           0 |              0 |\n| 2025-10-31 00:00:00-04:00 | 446.75 | 458    | 443.69 |  456.56 | 8.31358e+07 |           0 |              0 |\n| 2025-11-03 00:00:00-05:00 | 455.99 | 474.07 | 453.8  |  468.37 | 8.45952e+07 |           0 |              0 |\n| 2025-11-04 00:00:00-05:00 | 454.46 | 460.22 | 443.6  |  444.26 | 8.77566e+07 |           0 |              0 |\n| 2025-11-05 00:00:00-05:00 | 452.05 | 466.33 | 440.71 |  462.07 | 8.5573e+07  |           0 |              0 |\n| 2025-11-06 00:00:00-05:00 | 461.96 | 467.45 | 435.09 |  445.91 | 1.09623e+08 |           0 |              0 |\n| 2025-11-07 00:00:00-05:00 | 437.92 | 439.36 | 421.88 |  429.52 | 1.03472e+08 |           0 |              0 |\n| 2025-11-10 00:00:00-05:00 | 439.6  | 449.67 | 433.36 |  445.23 | 7.65159e+07 |           0 |              0 |\n| 2025-11-11 00:00:00-05:00 | 439.4  | 442.49 | 432.36 |  439.62 | 6.05332e+07 |           0 |              0 |\n| 2025-11-12 00:00:00-05:00 | 442.15 | 442.33 | 426.56 |  430.6  | 5.85135e+07 |           0 |              0 |\n| 2025-11-13 00:00:00-05:00 | 423.13 | 424.5  | 396.34 |  401.99 | 1.18948e+08 |           0 |              0 |\n| 2025-11-14 00:00:00-05:00 | 386.3  | 412.19 | 382.78 |  404.35 | 1.05248e+08 |           0 |              0 |' name='get_yf_stock_history' tool_call_id='call_yH5YPrNI66SuYENxM9x1LTfr'
```

```python
# ìì—°ì–´ë¡œ í•¨ìˆ˜ ê²°ê³¼ ì²˜ë¦¬í•˜ê¸°
llm_with_tools.invoke(messages)

# AIMessage(content='í…ŒìŠ¬ë¼ì˜ ì£¼ê°€ëŠ” í•œ ë‹¬ ì „ê³¼ ë¹„êµí•´ ë‚´ë ¸ìŠµë‹ˆë‹¤. \n\n- **í•œ ë‹¬ ì „ (2025ë…„ 10ì›” 15ì¼)**: ì£¼ê°€ëŠ” **$435.15**ë¡œ ë§ˆê°í–ˆìŠµë‹ˆë‹¤.\n- **í˜„ì¬ (2025ë…„ 11ì›” 14ì¼)**: ì£¼ê°€ëŠ” **$404.35**ë¡œ ë§ˆê°í–ˆìŠµë‹ˆë‹¤.\n\në”°ë¼ì„œ, í•œ ë‹¬ ì‚¬ì´ì— ì£¼ê°€ëŠ” **$30.80** í•˜ë½í–ˆìŠµë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 1705, 'total_tokens': 1801, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CcXxpunatBBdIfIkzt3y1x1j3xyzW', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7a0be23b-0f00-446b-b575-dc488b062e35-0', usage_metadata={'input_tokens': 1705, 'output_tokens': 96, 'total_tokens': 1801, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
```

08-4 ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ê¸°

- [ì‹¤ìŠµ] ë„êµ¬ ì‚¬ìš©í•  ë•Œ ìŠ¤íŠ¸ë¦¼ ì¶œë ¥í•˜ê¸° 
ì–¸ì–´ ëª¨ë¸ë§Œ ìˆì„ ë•Œ ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ê¸°
invoke() ëŒ€ì‹  stream() ì‚¬ìš©

ë­ì²´ì¸ ë„êµ¬ ì‚¬ìš© ì‹œ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ê¸°
    
    ```python
    for c in llm.stream([HumanMessage("ì˜ ì§€ëƒˆì–´? í•œêµ­ ì‚¬íšŒì˜ ë¬¸ì œì ì— ëŒ€í•´ ì´ì•¼ê¸°í•´ì¤˜.")]):
        print(c.content, end='|') 
     
    # |ì•ˆ|ë…•í•˜ì„¸ìš”|!| í•œêµ­| ì‚¬íšŒ|ì—ëŠ”| ì—¬ëŸ¬| ê°€ì§€| ë¬¸ì œ|ì |ì´| ì¡´ì¬|í•©ë‹ˆë‹¤|.| ëª‡| ê°€ì§€| ì£¼ìš”| ë¬¸ì œ|ë¥¼| ë§ì”€|ë“œ|ë¦¬|ë©´|:
    
    #|1|.| **|ê³ |ìš©| ë¶ˆ|ì•ˆ|ê³¼| ì²­|ë…„| ì‹¤|ì—…|**|:| ë§ì€| ì²­|ë…„|ë“¤ì´| ì•ˆì •|ì ì¸| ì¼|ì|ë¦¬ë¥¼| ì°¾|ê¸°| ì–´ë µ|ê³ |,| ì´ëŠ”| ê²½ì œ|ì | ê±±|ì •ì„| ì¦|ëŒ€|ì‹œ|í‚µ|ë‹ˆë‹¤|.| íŠ¹íˆ| ëŒ€|ê¸°ì—…| ìœ„|ì£¼ì˜| ì±„|ìš©| ë¬¸í™”|ê°€| ì¤‘|ì†Œ|ê¸°ì—…|ì´ë‚˜| ì°½|ì—…|ì„| ì–´ë ¤|ìš´| ìƒí™©|ìœ¼ë¡œ| ë§Œë“¤|ê³ | ìˆìŠµë‹ˆë‹¤|.
    
    #|2|.| **|ì£¼|ê±°| ë¬¸ì œ|**|:| ì„œìš¸|ê³¼| ê°™ì€| ëŒ€|ë„|ì‹œ|ì˜| ì£¼|ê±°|ë¹„|ê°€| ê¸‰|ê²©|íˆ| ìƒìŠ¹|í•˜ë©´ì„œ| ì Š|ì€| ì„¸|ëŒ€|ê°€| ìì‹ |ë§Œ|ì˜| ì§‘|ì„| ë§ˆë ¨|í•˜ê¸°| ì–´ë ¤|ìš´| ì‹¤|ì •|ì…ë‹ˆë‹¤|.| ì „|ì„¸|ì™€| ì›”|ì„¸|ì˜| ë¶€ë‹´|ì´| í°| ë¬¸ì œ|ë¡œ| ëŒ€|ë‘|ë˜ê³ | ìˆìŠµë‹ˆë‹¤|.
    
    #|3|.| **|ì‚¬íšŒ|ì | ì–‘|ê·¹|í™”|**|:| ì†Œ|ë“| ê²©|ì°¨|ê°€| í™•ëŒ€|ë˜|ë©´ì„œ| ë¶€|ìœ |ì¸µ|ê³¼| ì €|ì†Œ|ë“|ì¸µ| ê°„|ì˜| ê°„|ê·¹|ì´| ì‹¬|í™”|ë˜ê³ | ìˆìŠµë‹ˆë‹¤|.| ì´ëŠ”| êµìœ¡|,| ì˜ë£Œ|,| ì£¼|ê±°| ë“±| ë‹¤ì–‘í•œ| ë¶„ì•¼|ì—ì„œ| ë¶ˆ|í‰|ë“±|ì„| ì´ˆ|ë˜|í•©ë‹ˆë‹¤|.
    
    #|4|.| **|ì •|ì‹ | ê±´ê°•| ë¬¸ì œ|**|:| ê²½ìŸ|ì´| ì¹˜|ì—´|í•œ| ì‚¬íšŒ|ì | í™˜ê²½| ì†|ì—ì„œ| ìŠ¤íŠ¸|ë ˆìŠ¤|ì™€| ìš°|ìš¸|ì¦| ë“±| ì •ì‹ |ì | ë¬¸ì œ|ë¥¼| ê²ª|ëŠ”| ì‚¬ëŒë“¤ì´| ë§|ìŠµë‹ˆë‹¤|.| ê·¸ëŸ¬ë‚˜| ì—¬|ì „íˆ| ì´ì—| ëŒ€í•œ| ì¸|ì‹|ì´| ë¶€ì¡±|í•´| ì¶©ë¶„|í•œ| ì§€ì›|ì„| ë°›|ê¸°| ì–´ë µ|ìŠµë‹ˆë‹¤|.
    
    # |5|.| **|ì—¬|ì„±| ì¸|ê¶Œ| ë¬¸ì œ|**|:| ì„±|ë³„|ì—| ë”°ë¥¸| ë¶ˆ|í‰|ë“±|ê³¼| ì„±|í­|ë ¥| ë¬¸ì œ|ëŠ”| ì—¬|ì „íˆ| ì‹¬|ê°|í•©ë‹ˆë‹¤|.| ì§|ì¥ì—ì„œ|ì˜| ì„±|ì°¨|ë³„|,| ê°€|ì •| ë‚´| í­|ë ¥| ë“±| ì—¬ëŸ¬| ë¬¸ì œê°€| ì‚¬íšŒ|ì ìœ¼ë¡œ| ë…¼|ì˜|ë˜ê³ | ìˆì§€ë§Œ|,| í•´ê²°|í•˜ê¸°| ìœ„í•œ| ë…¸|ë ¥ì´| ë¶€ì¡±|í•œ| ìƒí™©|ì…ë‹ˆë‹¤|.
    
    # |6|.| **|ë…¸|ì¸| ë¬¸ì œ|**|:| ê³ |ë ¹|í™”| ì‚¬íšŒ|ë¡œ| ì ‘|ì–´|ë“¤|ë©´ì„œ| ë…¸|ì¸| ë³µ|ì§€|ì™€| ê´€ë ¨|ëœ| ë¬¸ì œ|ë„| ì¦ê°€|í•˜ê³ | ìˆìŠµë‹ˆë‹¤|.| í™€|ëª¸| ë…¸|ì¸|ì´ë‚˜| ê²½ì œ|ì | ì–´ë ¤|ì›€ì„| ê²ª|ëŠ”| ë…¸|ì¸|ë“¤ì´| ë§|ì§€ë§Œ| ì´ì—| ëŒ€í•œ| ì‚¬íšŒ|ì | ì§€ì›|ì´| ë¯¸|í¡|í•©ë‹ˆë‹¤|.
    
    # |ì´|ëŸ¬í•œ| ë¬¸ì œ|ë“¤ì€| ë³µ|í•©|ì ì¸| ìš”ì†Œ|ë“¤|ë¡œ| ì¸í•´| í•´ê²°|í•˜ê¸°| ì‰½|ì§€| ì•Š|ì§€ë§Œ|,| ì„±|ì°°|ê³¼| ëŒ€|í™”ë¥¼| í†µí•´| ì¡°ê¸ˆ|ì”©| ê°œì„ |í•´| ë‚˜|ê°€|ì•¼| í• | ë¶€ë¶„|ì…ë‹ˆë‹¤|.|||| 
    ```
    
    ë„êµ¬ë¥¼ ì¶”ê°€í–ˆì„ ë•Œ ìŠ¤íŠ¸ë¦¼ ì¶œë ¥í•˜ê¸°
    
    ```python
    messages = [
        SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€ì„ í•˜ê¸° ìœ„í•´ toolsë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤."),
        HumanMessage("ë¶€ì‚°ì€ ì§€ê¸ˆ ëª‡ì‹œì•¼?"),
    ]
    
    response = llm_with_tools.stream(messages)
    
    # íŒŒí¸í™”ëœ tool_call ì²­í¬ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸° 
    is_first = True
    for chunk in response:    
        print("chunk type: ", type(chunk))
        
        if is_first:
            is_first = False
            gathered = chunk
        else:
            gathered += chunk
        
        print("content: ", gathered.content, "tool_call_chunk", gathered.tool_calls)
    
    messages.append(gathered)
    ```
    
    ```python
    # AIMessageChunckë¥¼ ê³„ì† ë”í•´ë„ ìœ ì§€ë˜ëŠ” íƒ€ì…
    gathered
    
    # AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai', 'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'service_tier': 'default'}, id='lc_run--c368810c-ee52-48c9-a99a-e5e2d29c0890', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'Asia/Seoul', 'location': 'ë¶€ì‚°'}, 'id': 'call_FbBCcAYLxfVjHYL1Qp9i94rf', 'type': 'tool_call'}], usage_metadata={'input_tokens': 203, 'output_tokens': 23, 'total_tokens': 226, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, tool_call_chunks=[{'name': 'get_current_time', 'args': '{"timezone":"Asia/Seoul","location":"ë¶€ì‚°"}', 'id': 'call_FbBCcAYLxfVjHYL1Qp9i94rf', 'index': 0, 'type': 'tool_call_chunk'}], chunk_position='last')
    ```
    
    ```python
    # AIMessageChunckì— ê¸°ì¡´ ëŒ€í™”ë¥¼ ì¶”ê°€í•´ì„œ ë„êµ¬ ì‚¬ìš©í•˜ê¸° 
    for tool_call in gathered.tool_calls:
        selected_tool = tool_dict[tool_call["name"]] # tool_dictë¥¼ ì‚¬ìš©í•˜ì—¬ ë„êµ¬ ì´ë¦„ìœ¼ë¡œ ë„êµ¬ í•¨ìˆ˜ë¥¼ ì„ íƒ
        print(tool_call["args"]) # ë„êµ¬ í˜¸ì¶œ ì‹œ ì „ë‹¬ëœ ì¸ì ì¶œë ¥
        tool_msg = selected_tool.invoke(tool_call) # ë„êµ¬ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜
        messages.append(tool_msg)
    
    messages
    ```
    
    ```python
    # ë„êµ¬ ì‚¬ìš©í•´ ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ê¸°
    for c in llm_with_tools.stream(messages):
        print(c.content, end='|')
    
    # |ë¶€|ì‚°|ì€| í˜„ì¬| |202|5|ë…„| |11|ì›”| |16|ì¼| |23|ì‹œ| |36|ë¶„|ì…ë‹ˆë‹¤|.||||
    ```
    

08-5 ìŠ¤íŠ¸ë¦¼ë¦¿ì— êµ¬í˜„í•˜ê¸°

- [ì‹¤ìŠµ] ë­ì²´ì¸ ë©”ëª¨ë¦¬ì— ê¸°ë°˜í•œ ë©€í‹°í„´ ì±—ë´‡ ë§Œë“¤ê¸°
    
    ```python
    import streamlit as st
    from dotenv import load_dotenv
    # import os
    from langchain_openai import ChatOpenAI  # ì˜¤í”ˆAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë­ì²´ì¸ ì±—ë´‡ í´ë˜ìŠ¤
    from langchain_core.chat_history import InMemoryChatMessageHistory  # ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤
    from langchain_core.runnables.history import RunnableWithMessageHistory  # ë©”ì‹œì§€ ê¸°ë¡ì„ í™œìš©í•´ ì‹¤í–‰ ê°€ëŠ¥í•œ wrapper í´ë˜ìŠ¤
    from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
    
    load_dotenv()
    
    st.title("ğŸ’¬ SolChatbot")
    
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆì´ ë‹µí•˜ëŠ” AIì±—ë´‡ì´ë‹¤.")
        ]
    
    # ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ëŒ€ì‹  session_state ì‚¬ìš©
    if "store" not in st.session_state:
        st.session_state["store"] = {}
    
    def get_session_history(session_id: str):
        if session_id not in st.session_state["store"]:
            st.session_state["store"][session_id] = InMemoryChatMessageHistory()
        return st.session_state["store"][session_id]
    
    llm = ChatOpenAI(model="gpt-4o-mini")
    with_message_history = RunnableWithMessageHistory(llm, get_session_history)
    
    config = {"configurable": {"session_id": "abc2"}}
    
    # ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
    for msg in st.session_state.messages:
        if msg:
            if isinstance(msg, SystemMessage):
                st.chat_message("system").write(msg.content)
            elif isinstance(msg, AIMessage):
                st.chat_message("assistant").write(msg.content)
            elif isinstance(msg, HumanMessage):
                st.chat_message("user").write(msg.content)
    
    if prompt := st.chat_input():
        print('user:', prompt)  
        st.session_state.messages.append(HumanMessage(prompt))
        st.chat_message("user").write(prompt)
    
        response = with_message_history.stream([HumanMessage(prompt)], config=config)
    
        ai_response_bucket = None
        with st.chat_message("assistant").empty():
            for r in response:
                if ai_response_bucket is None:
                    ai_response_bucket = r
                else:
                    ai_response_bucket += r
                print(r.content, end='')
                st.markdown(ai_response_bucket.content)
    
        msg = ai_response_bucket.content
        st.session_state.messages.append(ai_response_bucket)
        print('assistant:', msg) 
    
    ```
    <img width="550" height="767" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-16 á„‹á…©á„’á…® 11 46 25" src="https://github.com/user-attachments/assets/97009ac2-1007-4c06-b43a-d2da5bf00440" />

- [ì‹¤ìŠµ] ë­ì²´ì¸ ë©”ëª¨ë¦¬ ì—†ì´ ë©€í‹°í„´ ë§Œë“¤ê¸°
    
    ```python
    import streamlit as st
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    llm = ChatOpenAI(model="gpt-4o-mini")
    
    # ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
    def get_ai_response(messages):
        response = llm.stream(messages)
    
        for chunk in response:
            yield chunk
    
    # Streamlit ì•±
    st.title("ğŸ’¬ GPT-4o Langchain Chat")
    
    # ìŠ¤íŠ¸ë¦¼ë¦¿ session_stateì— ë©”ì‹œì§€ ì €ì¥
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë•ê¸° ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë´‡ì´ë‹¤. "),  
            AIMessage("How can I help you?")
        ]
    
    # ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
    for msg in st.session_state.messages:
        if msg.content:
            if isinstance(msg, SystemMessage):
                st.chat_message("system").write(msg.content)
            elif isinstance(msg, AIMessage):
                st.chat_message("assistant").write(msg.content)
            elif isinstance(msg, HumanMessage):
                st.chat_message("user").write(msg.content)
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input():
        st.chat_message("user").write(prompt) # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
        st.session_state.messages.append(HumanMessage(prompt)) # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    
        response = get_ai_response(st.session_state["messages"])
        
        result = st.chat_message("assistant").write_stream(response) # AI ë©”ì‹œì§€ ì¶œë ¥
        st.session_state["messages"].append(AIMessage(result)) # AI ë©”ì‹œì§€ ì €ì¥    
    
    ```
    <img width="705" height="699" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-16 á„‹á…©á„’á…® 11 50 42" src="https://github.com/user-attachments/assets/9f22daf7-7490-46e7-b568-010fa54274a9" />

- [ì‹¤ìŠµ] ë„êµ¬ ì¶”ê°€í•˜ê³  ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ê¸°

```
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from dotenv import load_dotenv
from langchain_core.tools import tool
from datetime import datetime
import pytz
load_dotenv()

# ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini")

# ë„êµ¬ í•¨ìˆ˜ ì •ì˜
@tool
def get_current_time(timezone: str, location: str) -> str:
    """í˜„ì¬ ì‹œê°ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜."""
    try:
        tz = pytz.timezone(timezone)
        now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        result = f'{timezone} ({location}) í˜„ì¬ì‹œê° {now}'
        print(result)
        return result
    except pytz.UnknownTimeZoneError:
        return f"ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì„ì¡´: {timezone}"

# ë„êµ¬ ë°”ì¸ë”©
tools = [get_current_time]
tool_dict = {"get_current_time": get_current_time}

llm_with_tools = llm.bind_tools(tools)

# ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
def get_ai_response(messages):
    response = llm_with_tools.stream(messages) # â‘  llm.stream()ì„ llm_with_tools.stream()ë¡œ ë³€ê²½
    
    gathered = None # â‘¡
    for chunk in response:
        yield chunk
        
        if gathered is None: #  â‘¢
            gathered = chunk
        else:
            gathered += chunk
 
    if gathered.tool_calls:
        st.session_state.messages.append(gathered)
        
        for tool_call in gathered.tool_calls:
            selected_tool = tool_dict[tool_call['name']]
            tool_msg = selected_tool.invoke(tool_call) 
            print(tool_msg, type(tool_msg))
            st.session_state.messages.append(tool_msg)
           
        for chunk in get_ai_response(st.session_state.messages):
            yield chunk

# Streamlit ì•±
st.title("ğŸ’¬ GPT-4o Langchain Chat")

# ìŠ¤íŠ¸ë¦¼ë¦¿ session_stateì— ë©”ì‹œì§€ ì €ì¥
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë•ê¸° ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë´‡ì´ë‹¤. "),  
        AIMessage("How can I help you?")
    ]

# ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    if msg.content:
        if isinstance(msg, SystemMessage):
            st.chat_message("system").write(msg.content)
        elif isinstance(msg, AIMessage):
            st.chat_message("assistant").write(msg.content)
        elif isinstance(msg, HumanMessage):
            st.chat_message("user").write(msg.content)
        elif isinstance(msg, ToolMessage):
            st.chat_message("tool").write(msg.content)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input():
    st.chat_message("user").write(prompt) # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
    st.session_state.messages.append(HumanMessage(prompt)) # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥

    response = get_ai_response(st.session_state["messages"])
    
    result = st.chat_message("assistant").write_stream(response) # AI ë©”ì‹œì§€ ì¶œë ¥
    st.session_state["messages"].append(AIMessage(result)) # AI ë©”ì‹œì§€ ì €ì¥    

```

### 9ì¥ RAGë¡œ ë¬¸ì„œì— ê¸°ë°˜í•´ ë‹µë³€í•˜ëŠ” ì±—ë´‡ ë§Œë“¤ê¸°

09-1 RAGë€ ë¬´ì—‡ì¼ê¹Œ?

- ì–¸ì–´ ëª¨ë¸ê³¼ RAGì˜ ì‘ë™ ë°©ì‹
    - ì–¸ì–´ ëª¨ë¸ì€ ì´ì „ ë¬¸ì¥ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‚˜ì˜¬ ë¬¸ì¥ì„ í™•ë¥  ê³„ì‚°í•´ì„œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ê³  íŒë‹¨ë˜ëŠ” ë‹¨ì–´ì™€ ë¬¸ì¥ì„ ìƒì„±í•¨
    - ì–¸ì–´ ëª¨ë¸ì€ ê³¼ê±°ì— í•™ìŠµí•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ì¤˜ì„œ í™˜ê° í˜„ìƒì´ ë°œìƒí•¨
    - RAGëŠ” í•„ìš”í•œ ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ ê²€ìƒ‰í•´ì„œ ì–¸ì–´ ëª¨ë¸ì—ê²Œ ì „ë‹¬í•˜ê³  ê·¸ê²ƒ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ë¼ê³  ì•Œë ¤ ì¤Œ
- ê¸°ë³¸ì ì¸ ì–¸ì–´ ëª¨ë¸ì˜ ë‹µë³€ê³¼ RAGì˜ ì°¨ì´
    - ì–¸ì–´ëª¨ë¸: ê·¼ê±° ìˆëŠ” ì •ë³´ì¸ ì§€ í™•ì¸ì´ ì–´ë ¤ì›€
    - RAG: ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì•„ ê·¸ ë¬¸ì„œ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ì œê³µ
- ì²­í‚¹: ëŒ€ëŸ‰ì˜ ë¬¸ì„œë¥¼ ìª½ì§€ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
    - ë§ì€ ì–‘ì˜ ë¬¸ì„œë¥¼ ì „ë‹¬í•˜ê³  ë‹µì„ ìš”ì²­í•˜ê²Œ ë˜ë©´ ìƒê¸°ëŠ” ë¬¸ì œ
        1. ì–¸ì–´ ëª¨ë¸ì´  í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆìŒ
        2. ì–¸ì–´ ëª¨ë¸ì´ ë¬¸ì„œì— í•„ìš”í•œ ì •ë³´ë¥¼ ì œëŒ€ë¡œ ì°¾ì§€ ëª»í•  ìˆ˜ ìˆìŒ
        3. ëŒ€í™”í˜• ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•  ê²½ìš° ë§¤ë²ˆ ìˆ˜ë°± í˜ì´ì§€ì˜ ë¬¸ì„œë¥¼ ì–¸ì–´ ëª¨ë¸ì— ì…ë ¥í•´ì„œ ë†’ì€ í† í° ë¹„ìš© ë°œìƒ
    - ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°: ì–¸ì–´ ëª¨ë¸ì´ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ ê¸¸ì´ì˜ í•œê³„
    - ì²­í¬: ì²­í‚¹ ì‘ì—…ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì§„ ë¬¸ì„œ ì¡°ê°
    - RAGëŠ” ì²­í¬ ë‹¨ìœ„ë¡œ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ê³  ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ë¥¼ ì°¾ì•„ ì§ˆë¬¸ê³¼ í•¨ê»˜ ì–¸ì–´ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ë¬¸ì„œì— ê¸°ë°˜í•œ ë‹µë³€ì„ ìƒì„±í•¨
- ì„ë² ë”©: í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ê¸°
    - ì„ë² ë”©: ê° ì²­í¬ì˜ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„° ê³µê°„ì— ìˆ«ì í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •
    - ë²¡í„°: ì •ë³´ë¥¼ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•˜ì—¬ ì¼ë ¬ë¡œ ë‚˜ì—´í•œ ê²ƒ
        - eg. [í‚¤, ëª¸ë¬´ê²Œ, ìœ„ë„, ê²½ë„, ì¶œìƒì—°ë„] ì‚¬ëŒ ì •ë³´ë¥¼ 5ì°¨ì› ë²¡í„°ë¡œ ì„ë² ë”©
        - ë§ˆì´í´ ì¡°ë˜: [198, 98, 40.67, -73.94, 1963]
    - ì½”ì‚¬ì¸ ìœ ì‚¬ë„: ìœ ì‚¬ë„ ê³„ì‚° ë°©ë²•, 1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìœ ì‚¬ë„ê°€ ë†’ìŒ
- ë²¡í„° DBì™€ ë¦¬íŠ¸ë¦¬ë²„
    - ë²¡í„°DB
        - ë²¡í„°ë¡œ ë³€í™˜ëœ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê³  í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•´ì„œ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ë¥¼ ì°¾ì•„ë‚¼ ë•Œ ìœ ìš©
        - eg. í¬ë¡œë§ˆDB, FAISS, íŒŒì¸ì½˜
    - ë¦¬íŠ¸ë¦¬ë²„
        - ì‚¬ìš©ìê°€ ì§ˆë¬¸í•œ ì •ë³´ì— ì ì ˆí•œ ë‹µì„ ìƒì„±í•˜ëŠ” ë° í•„ìš”í•œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì—­í• 
- ì§ˆì˜ í™•ì¥
    - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ê²€ìƒ‰í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ ì§ˆë¬¸ì˜ ë¬¸ë§¥ì„ íŒŒì•…í•´ ì ì ˆí•œ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜ í›„ ê·¸ ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ì„ë² ë”©í•´ì„œ ê²€ìƒ‰í•˜ëŠ” ê³¼ì • í•„ìš”
    - ì‚¬ìš©ì ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê²Œ ìˆ˜ì •í•˜ëŠ” ì‘ì—…

09-2 RAG ê¸°ë°˜í•œ ì±—ë´‡ êµ¬í˜„í•˜ê¸°

- [ì‹¤ìŠµ] PDF íŒŒì¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ì²­í¬ ë‹¨ìœ„ë¡œ ìª¼ê°œê¸°
    
    ```python
    %pip install PyMuPDF pypdf langchain langchain_community
    ```
    
    ```python
    # pyPDFLoader ì‚¬ìš©í•˜ê¸°
    
    from langchain_community.document_loaders import PyPDFLoader
    
    # PDF íŒŒì¼ì„ ì½ì–´ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    loader = PyPDFLoader('C:/github/gpt_agent_2025_easyspub/chap09/data/OneNYC_2050_Strategic_Plan.pdf')
    data_nyc = loader.load()
    print(data_nyc)
    ```
    
    ```python
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    # í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ 1000ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. overlapì€ 100ìë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    all_splits = text_splitter.split_documents(data_nyc)
    ```
    
    ```
    for i, split in enumerate(all_splits):
        print(f"Split {i+1}:------------------------------------\n")
        print(split)
    ```
    
    ```python
    print(type(all_splits[0]))
    ```
    
    ```python
    # 2040 ì„œìš¸ë„ì‹œê¸°ë³¸ê³„íš ë¬¸ì„œ ì²­í‚¹í•˜ê¸°
    loader_seoul = PyPDFLoader('/Users/solbi/Documents/GitHub/study-gpt_agent/chap09/sec02/data/2040_seoul_plan.pdf')
    data_seoul = loader_seoul.load()
    seoul_splits = text_splitter.split_documents(data_seoul)
    for i, split in enumerate(seoul_splits):
        print(f"Split {i+1}:------------------------------------")
        print(split)
    
    ```
    
    ```python
    # ì˜¤ë²„ë© ì²˜ë¦¬ ì „ ì²­í¬ ì¶œë ¥í•˜ê¸° 
    print(seoul_splits[50].page_content)
    print('----------------------')
    print(seoul_splits[51].page_content)
    ```
    
    ```python
    # ì˜¤ë²„ë© ì²˜ë¦¬ í›„ ì²­í¬ ì¶œë ¥í•˜ê¸° 
    for i in range(len(seoul_splits) - 1):
        seoul_splits[i].page_content += "\n"+ seoul_splits[i + 1].page_content[:100]
    
    print(seoul_splits[50].page_content)
    print('----------------------')
    print(seoul_splits[51].page_content)
    ```
    
    ```python
    # ì²­í¬ ì¶œë ¥í•˜ê¸° 
    print(len(all_splits))
    all_splits.extend(seoul_splits)
    print(len(all_splits))
    ```
    

- [ì‹¤ìŠµ] ì˜¤í”ˆAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©í•˜ê¸°
    
    ```python
    # í¬ë¡œë§ˆ DBë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
    %pip install langchain_chroma langchain_openai
    ```
    
    ```python
    from langchain_openai import OpenAIEmbeddings 
    from dotenv import load_dotenv
    import os
    
    load_dotenv()
    
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
    
    embedding = OpenAIEmbeddings(model='text-embedding-3-large', api_key=OPENAI_API_KEY)
    v = embedding.embed_query("ë‰´ìš•ì˜ ì˜¨ì‹¤ê°€ìŠ¤ ì €ê° ì •ì±…ì€ ë­ì•¼?")
    print(v)
    print(len(v))
    
    # [-0.005292298272252083, -0.049610063433647156, -0.012534075416624546, 0.008485617116093636, -0.04008990526199341, ...
    ```
    
- [ì‹¤ìŠµ] ë²¡í„° DBì™€ ë¦¬íŠ¸ë¦¬ë²„
    
    ```python
    from langchain_chroma import Chroma
    import os
    
    persist_directory = '../chroma_store'	
    
    # ì €ì¥ëœ í¬ë¡œë§ˆ DBê°€ ì—†ë‹¤ë©´ ìƒˆë¡œ ë§Œë“¤ê¸°
    if not os.path.exists(persist_directory):
        print("Creating new Chroma store")
        vectorstore = Chroma.from_documents(
            documents=all_splits,
            embedding=embedding,
            persist_directory=persist_directory
        )
    
    else:
        print("Loading existing Chroma store")
        vectorstore = Chroma(		
            persist_directory=persist_directory, 
            embedding_function=embedding
        )
    ```
    
    ```python
    # ìœ ì‚¬ ì²­í¬ ê°€ì ¸ì™€ ì§ˆë¬¸í•˜ê¸°
    retriever = vectorstore.as_retriever(k=3)
    docs = retriever.invoke("ì„œìš¸ì‹œì˜ í™˜ê²½ ì •ì±…ì— ëŒ€í•´ ê¶ê¸ˆí•´")
    
    for d in docs:
        print(d)
        print('------')
    ```
    
- [ì‹¤ìŠµ] ì£¼ì–´ì§„ ì²­í¬ì— ê¸°ë°˜í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±í•˜ê¸°
    
    ```python
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder 
    from langchain.chains.combine_documents import create_stuff_documents_chain 
    from langchain_openai import ChatOpenAI 
    
    chat = ChatOpenAI(model="gpt-4o-mini") 
    
    question_answering_prompt = ChatPromptTemplate.from_messages(
        [
            ( 
                "system",
                "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ contextì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ë¼.:\n\n{context}",
            ),
            MessagesPlaceholder(variable_name="messages"), 
        ]
    )
    
    document_chain = create_stuff_documents_chain(chat, question_answering_prompt) 
    ```
    
    ```python
    from langchain.memory import ChatMessageHistory
    
    # ì±„íŒ… ë©”ì‹œì§€ ì €ì¥í•  ë©”ëª¨ë¦¬ ê°ì²´ ìƒì„±
    chat_history = ChatMessageHistory() 
    # ì‚¬ìš©ì ì§ˆë¬¸ì„ ë©”ëª¨ë¦¬ì— ì €ì¥
    chat_history.add_user_message("ì„œìš¸ì‹œì˜ ì˜¨ì‹¤ê°€ìŠ¤ ì €ê° ì •ì±…ì— ëŒ€í•´ ì•Œë ¤ì¤˜.") 
    
    # ë¬¸ì„œ ê²€ìƒ‰í•˜ê³  ë‹µë³€ ìƒì„±
    answer = document_chain.invoke(
        {
            "messages": chat_history.messages,
            "context": docs,
        }
    )
    
    # ìƒì„±ëœ ë‹µë³€ ë©”ëª¨ë¦¬ì— ì €ì¥
    chat_history.add_ai_message(answer) 
    
    print(answer)
    ```
    
    ```python
    for m in chat_history.messages:
        print(m)
    ```
    
- [ì‹¤ìŠµ] ì§ˆì˜ í™•ì¥ êµ¬í˜„í•˜ê¸°
    
    ```python
    from langchain_core.output_parsers import StrOutputParser # ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    ```
    
    ```
    query_for_nyc = "ë‰´ìš•ì€?"
    
    # query augmentation 
    # ê¸°ì¡´ ëŒ€í™” ë‚´ìš©ì„ í™œìš©í•´ query_augmentation ìˆ˜í–‰
    query_augmentation_prompt = ChatPromptTemplate.from_messages(
        [
            MessagesPlaceholder(variable_name="messages"), # ê¸°ì¡´ ëŒ€í™” ë‚´ìš©
            (
                "system",
                "ê¸°ì¡´ì˜ ëŒ€í™” ë‚´ìš©ì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì•„ë˜ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ëª…ë£Œí•œ í•œ ë¬¸ì¥ì˜ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•˜ë¼. ëŒ€ëª…ì‚¬ë‚˜ ì´, ì €, ê·¸ì™€ ê°™ì€ í‘œí˜„ì„ ëª…í™•í•œ ëª…ì‚¬ë¡œ í‘œí˜„í•˜ë¼. :\n\n{query}",
            ),
        ]
    )
    ```
    
    ```python
    query_augmentation_chain = query_augmentation_prompt | chat | StrOutputParser()
    ```
    
    ```python
    # ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê²Œ ë³€í™˜í•˜ê¸°
    augmented_query = query_augmentation_chain.invoke({
        "messages": chat_history.messages,
        "query": query_for_nyc  
    })
    
    print(augmented_query)
    ```
    
    ```python
    # ë¦¬íŠ¸ë¦¬ë²„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ì¶œë ¥
    docs = retriever.invoke(augmented_query)
    
    for d in docs:
        print(d)
        print('------')
    ```
    
    ```python
    # ì–¸ì–´ ëª¨ë¸ì—ì„œ ë‹µë³€ ìƒì„±í•˜ê¸°
    chat_history.add_user_message(query_for_nyc) # query_for_nycì— "ë‰´ìš•ì€?" ì¶”ê°€
    
    answer = document_chain.invoke(
        {
            "messages": chat_history.messages,
            "context": docs,
        }
    )
    
    # ìƒì„±ëœ ë‹µë³€ ë©”ëª¨ë¦¬ì— ì €ì¥
    chat_history.add_ai_message(answer) 
    
    print(answer)
    ```
    

09-3 ìŠ¤íŠ¸ë¦¼ë¦¿ìœ¼ë¡œ ì±—ë´‡ì™„ì„±í•˜ê¸°

- [ì‹¤ìŠµ] ê¸°ë³¸ ìŠ¤íŠ¸ë¦¼ë¦¿ ì½”ë“œì— ë¦¬íŠ¸ë¦¬ë²„ ì¶”ê°€í•˜ê¸°
    
    ```python
    # ì„ë² ë”© ëª¨ë¸ ì„ ì–¸í•˜ê¸°
    from langchain_openai import OpenAIEmbeddings
    embedding = OpenAIEmbeddings(model='text-embedding-3-large')
    
    # ì–¸ì–´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    from langchain_openai import ChatOpenAI
    llm = ChatOpenAI(model="gpt-4o")
    
    # Load Chroma store
    from langchain_chroma import Chroma
    print("Loading existing Chroma store")
    persist_directory = 'C:/github/gpt_agent_2025_easyspub/chap09/chroma_store'
    
    vectorstore = Chroma(
        persist_directory=persist_directory, 
        embedding_function=embedding
    )
    
    # Create retriever
    retriever = vectorstore.as_retriever(k=3)
    
    # Create document chain
    from langchain.chains.combine_documents import create_stuff_documents_chain
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain_core.output_parsers import StrOutputParser # ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    
    question_answering_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ contextì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ë¼.:\n\n{context}",
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    )
    
    document_chain = create_stuff_documents_chain(llm, question_answering_prompt) | StrOutputParser()
    
    # query augmentation chain
    query_augmentation_prompt = ChatPromptTemplate.from_messages(
        [
            MessagesPlaceholder(variable_name="messages"), # ê¸°ì¡´ ëŒ€í™” ë‚´ìš©
            (
                "system",
                "ê¸°ì¡´ì˜ ëŒ€í™” ë‚´ìš©ì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì•„ë˜ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ëª…ë£Œí•œ í•œ ë¬¸ì¥ì˜ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•˜ë¼. ëŒ€ëª…ì‚¬ë‚˜ ì´, ì €, ê·¸ì™€ ê°™ì€ í‘œí˜„ì„ ëª…í™•í•œ ëª…ì‚¬ë¡œ í‘œí˜„í•˜ë¼. :\n\n{query}",
            ),
        ]
    )
    
    query_augmentation_chain = query_augmentation_prompt | llm | StrOutputParser()
    
    ```
    
    ```python
    import streamlit as st
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
    import retriever
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    llm = ChatOpenAI(model="gpt-4o-mini")
    
    # ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
    def get_ai_response(messages, docs):    
        response = retriever.document_chain.stream({
            "messages": messages,
            "context": docs
        })
    
        for chunk in response:
            yield chunk
    
    # Streamlit ì•±
    st.title("ğŸ’¬ GPT-4o Langchain Chat")
    
    # ìŠ¤íŠ¸ë¦¼ë¦¿ session_stateì— ë©”ì‹œì§€ ì €ì¥
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            SystemMessage("ë„ˆëŠ” ë¬¸ì„œì— ê¸°ë°˜í•´ ë‹µë³€í•˜ëŠ” ë„ì‹œ ì •ì±… ì „ë¬¸ê°€ì•¼ "),  
            AIMessage("How can I help you?")
        ]
    
    # ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
    for msg in st.session_state.messages:
        if msg.content:
            if isinstance(msg, SystemMessage):
                st.chat_message("system").write(msg.content)
            elif isinstance(msg, AIMessage):
                st.chat_message("assistant").write(msg.content)
            elif isinstance(msg, HumanMessage):
                st.chat_message("user").write(msg.content)
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input():
        st.chat_message("user").write(prompt) # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
        st.session_state.messages.append(HumanMessage(prompt)) # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    
        augmented_query = retriever.query_augmentation_chain.invoke({
            "messages": st.session_state["messages"],
            "query": prompt,
        })
        print("augmented_query\t", augmented_query)
    
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        print("ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰")
        docs = retriever.retriever.invoke(f"{prompt}\n{augmented_query}")
    
        for doc in docs:
            print('---------------')
            print(doc)   
        print("===============")
    
        with st.spinner(f"AIê°€ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤... '{augmented_query}'"):
            response = get_ai_response(st.session_state["messages"], docs)
            result = st.chat_message("assistant").write_stream(response) # AI ë©”ì‹œì§€ ì¶œë ¥
        st.session_state["messages"].append(AIMessage(result)) # AI ë©”ì‹œì§€ ì €ì¥    
    
    ```
    
- [ì‹¤ìŠµ] ì¶œì²˜ í‘œê¸°í•˜ê¸°
    
    ```python
    import streamlit as st
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
    import retriever
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    llm = ChatOpenAI(model="gpt-4o-mini")
    
    # ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
    def get_ai_response(messages, docs):    
        response = retriever.document_chain.stream({
            "messages": messages,
            "context": docs
        })
    
        for chunk in response:
            yield chunk
    
    # Streamlit ì•±
    st.title("ğŸ’¬ GPT-4o Langchain Chat")
    
    # ìŠ¤íŠ¸ë¦¼ë¦¿ session_stateì— ë©”ì‹œì§€ ì €ì¥
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            SystemMessage("ë„ˆëŠ” ë¬¸ì„œì— ê¸°ë°˜í•´ ë‹µë³€í•˜ëŠ” ë„ì‹œ ì •ì±… ì „ë¬¸ê°€ì•¼ "),  
            AIMessage("How can I help you?")
        ]
    
    # ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
    for msg in st.session_state.messages:
        if msg.content:
            if isinstance(msg, SystemMessage):
                st.chat_message("system").write(msg.content)
            elif isinstance(msg, AIMessage):
                st.chat_message("assistant").write(msg.content)
            elif isinstance(msg, HumanMessage):
                st.chat_message("user").write(msg.content)
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input():
        st.chat_message("user").write(prompt) # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
        st.session_state.messages.append(HumanMessage(prompt)) # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    
        augmented_query = retriever.query_augmentation_chain.invoke({
            "messages": st.session_state["messages"],
            "query": prompt,
        })
        print("augmented_query\t", augmented_query)
    
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        print("ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰")
        docs = retriever.retriever.invoke(f"{prompt}\n{augmented_query}")
    
        for doc in docs:
            print('---------------')
            print(doc)   
            with st.expander(f"**ë¬¸ì„œ:** {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}"):
                # íŒŒì¼ëª…ê³¼ í˜ì´ì§€ ì •ë³´ í‘œì‹œ
                st.write(f"**page:**{doc.metadata.get('page', '')}")
                st.write(doc.page_content)
        print("===============")
    
        with st.spinner(f"AIê°€ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤... '{augmented_query}'"):
            response = get_ai_response(st.session_state["messages"], docs)
            result = st.chat_message("assistant").write_stream(response) # AI ë©”ì‹œì§€ ì¶œë ¥
        st.session_state["messages"].append(AIMessage(result)) # AI ë©”ì‹œì§€ ì €ì¥    
    
    ```
<img width="625" height="421" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-11-16 á„‹á…©á„’á…® 11 54 20" src="https://github.com/user-attachments/assets/f8fea529-69ec-4180-a666-36fdc61c3c88" />

### 10ì¥ ì¸í„°ë„· ê²€ìƒ‰ì„ í™œìš©í•´ ë‹µë³€í•˜ëŠ” ì±—ë´‡ ë§Œë“¤ê¸°

10-1 ì¸í„°ë„· ê²€ìƒ‰ í›„ ë‹µë³€í•˜ê¸° - ë•ë•ê³  ê²€ìƒ‰

ë•ë•ê³ 

- ì‚¬ìš©ìì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë©”ì‹œì§€ë¥¼ ê°•ë ¥íˆ ë‚´ì„¸ìš°ëŠ” ê²€ìƒ‰ ì—”ì§„.
- ì˜¨ë¼ì¸ í™œë™ì˜ í”„ë¼ì´ë²„ì‹œ ë³´ì¥
- API ë¬´ë£Œ ì œê³µ

[ì‹¤ìŠµ] GPTì— ì¸í„°ë„· ê²€ìƒ‰ ê¸°ëŠ¥ ì¶”ê°€í•˜ê¸° 

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini")
model.invoke("ìµœê·¼ ì œë‹ˆê°€ ë°œí‘œí•œ ì‹ ê³¡ì€ ë¬´ì—‡ì¸ê°€ìš”?")

# AIMessage(content='2023ë…„ 10ì›” ê¸°ì¤€ìœ¼ë¡œ ì œë‹ˆê°€ ë°œí‘œí•œ ì‹ ê³¡ì€ "SOLO"ì…ë‹ˆë‹¤. 
# ê·¸ëŸ¬ë‚˜ ê·¸ë…€ì˜ ìµœì‹  í™œë™ì´ë‚˜ ìƒˆë¡œìš´ ê³¡ì— ëŒ€í•œ ì •ë³´ëŠ” ê³„ì† ì—…ë°ì´íŠ¸ë˜ê¸° ë•Œë¬¸ì—, ê³µì‹ ì†Œì…œ ë¯¸ë””ì–´ë‚˜ ë‰´ìŠ¤ì—ì„œ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. 
# ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”!
```

```python
%pip install -U duckduckgo-search langchain_community  ddgs
```

```python
from langchain_community.tools import DuckDuckGoSearchResults 

search = DuckDuckGoSearchResults(results_separator=';\n')
docs = search.invoke("ìµœê·¼ ì œë‹ˆê°€ ë°œí‘œí•œ ì‹ ê³¡ì€ ë¬´ì—‡ì¸ê°€ìš”?")

print(docs)

# snippet: ìµœê·¼ ì¦ê²¨ë§ˆì‹œëŠ” ìŒë£ŒëŠ” ë¬´ì—‡ì¸ê°€ìš” ? ĞŸĞ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´. Report copyright infringement., title: ĞšĞ°Ğº ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ¯Ğ¿Ğ¾Ğ½ÑĞºĞ¸Ğ¹? " ìµœê·¼ ì¦ê²¨ë§ˆì‹œëŠ” ìŒë£ŒëŠ” ë¬´ì—‡ì¸ê°€ìš” ?", link: https://ru.hinative.com/questions/25138075;
#snippet: ìµœê·¼ ì†”ë¡œ ì•¨ë²” 1ì§‘ ë¥¼ ë°œí‘œí•œ í›„ ì–´ë”œê°€ë„, ë¬´ì—‡ì„ í‹€ì–´ë„ ì œë‹ˆì˜ ëª¨ìŠµë§Œ ê°€ë“í•˜ë‹¤. ë§¤ì¼ ë´ë„ ì§ˆë¦¬ì§€ ì•ŠëŠ” ì œë‹ˆì˜ ë§¤ë ¥ì€ íŒ¨ì…˜ ìŠ¤íƒ€ì¼ì—ì„œë„ ë¹›ì„ ë°œí•œë‹¤., title: íŒ¨ì…˜ì— ì§„ì‹¬ì¸ ì œë‹ˆê°€ í’ˆì ˆ ì‹œí‚¨ ëŒ€í‘œ íŒ¨ì…˜ ì•„ì´í…œì€ ì–´ë”” ê±°? | í•˜í¼ìŠ¤..., link: https://www.harpersbazaar.co.kr/article/1878402;
#snippet: Ğ ÑĞµÑ€Ğ²Ğ¸ÑĞµ ĞŸÑ€ĞµÑÑĞµ ĞĞ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ° Ğ¡Ğ²ÑĞ·Ğ°Ñ‚ÑŒÑÑ Ñ Ğ½Ğ°Ğ¼Ğ¸ ĞĞ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼ Ğ ĞµĞºĞ»Ğ°Ğ¼Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑĞ¼..., title: ì‹ ì¹¸ì„¼íƒ€ê³  ì˜¤ì‚¬ì¹´ì—ì„œ ë„ì¿„ã…£ì…êµ­ ì™„í™”ë¥¼ ë°œí‘œí•œ ì¼ë³¸! - YouTube, link: https://www.youtube.com/watch?v=5FQCITl2kEk;
#snippet: í‰ë²”í•œ 20ëŒ€ ì œë‹ˆê°€ ì˜¬í•´ í•´ë³´ê³  ì‹¶ì€ ì¼ì€ ë¬´ì—‡ì¸ê°€ìš” ?One of a Kind ì›”ë“œ íˆ¬ì–´ ì¤‘ ë² ë¥¼ë¦°ì—ì„œ ë§Œë‚œ ì œë‹ˆê°€ ìë‘í•˜ëŠ” ë‹¹ë‹¹í•œ ë§¤ë ¥., title: ì˜¤ì§ ë‹¨ í•˜ë‚˜ì˜ ì œë‹ˆ | ë³´ê·¸ ì½”ë¦¬ì•„ (Vogue Korea), link: https://www.vogue.co.kr/2023/01/17/ì˜¤ì§-ë‹¨-í•˜ë‚˜ì˜-ì œë‹ˆ/
```

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

question_answering_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ contextì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ë¼.:\n\n{context}",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

document_chain = question_answering_prompt | model
```

```python
from langchain_openai import ChatOpenAI
from langchain_classic.memory import ConversationBufferMemory
from langchain_classic.chains import ConversationChain

# LLM ì„ ì–¸
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.3
)

# ë©”ëª¨ë¦¬ ê°ì²´ ìƒì„±
memory = ConversationBufferMemory(memory_key="history", return_messages=True)

conversation = ConversationChain(
llm=llm,
memory=memory,
verbose=True
)

answer = conversation.run("2025ë…„ í˜„ëŒ€ ìë™ì°¨ ì „ë§ì„ ì•Œë ¤ì¤˜")
print(answer)
print(answer)

#  2025ë…„ í˜„ì¬ ì •ë³´ ì•Œë ¤ì¤Œ
```

[ì‹¤ìŠµ] ê²€ìƒ‰ ê¸°ëŠ¥ì— ì˜µì…˜ ì„¤ì •í•˜ê¸°

ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë§Œ ê²€ìƒ‰í•˜ê¸°
DuckDuckGoSearchAPIWrapperë¥¼ í™œìš©í•´ ê²€ìƒ‰ ì§€ì—­ê³¼ ê¸°ê°„ ì„¤ì •

```python
# DuckDuckGo API wrapperë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•  ë•Œ ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê¸° ìœ„í•œ í´ë˜ìŠ¤ import
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper

# í•œêµ­ ì§€ì—­("kr-kr")ì„ ê¸°ì¤€, ìµœê·¼ ì¼ì£¼ì¼("w") ë‚´ì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ì´ˆê¸°í™”
wrapper = DuckDuckGoSearchAPIWrapper(region="kr-kr", time="w")
```

- w: ìµœê·¼ ì¼ì£¼ì¼
- m: 1ê°œì›”
- d: í•˜ë£¨

```python
# ê²€ìƒ‰ ê¸°ëŠ¥ì„ ìœ„í•œ DuckDuckGoSearchResults ì´ˆê¸°í™”
search = DuckDuckGoSearchResults(
    api_wrapper=wrapper,      # ì•ì—ì„œ ì •ì˜í•œ API wrapperë¥¼ ì‚¬ìš©
    source="news",            # ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œë§Œ ê²€ìƒ‰í•˜ë„ë¡ ì§€ì •
    results_separator=';\n'   # ê²°ê³¼ í•­ëª© ì‚¬ì´ì— êµ¬ë¶„ì ì‚¬ìš© (ì„¸ë¯¸ì½œë¡ ê³¼ ì¤„ë°”ê¿ˆ)
)
```

```python
# ê²€ìƒ‰í•œ ë’¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ docsì— ì €ì¥
docs = search.invoke("2025ë…„ í˜„ëŒ€ìë™ì°¨ ë¯¸êµ­ ì‹œì¥ ì „ë§ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?")

# ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
print(docs)

```

íŠ¹ì • ì›¹ ì‚¬ì´íŠ¸ì—ì„œ ê²€ìƒ‰í•˜ê¸°

```python
# DuckDuckGoë¥¼ ì´ìš©í•´ ytn.co.kr ì‚¬ì´íŠ¸ì—ì„œ í˜„ëŒ€ìë™ì°¨ ë¯¸êµ­ ì‹œì¥ ì „ë§ì— ëŒ€í•œ ë‚´ìš©ì„ ê²€ìƒ‰
docs = search.invoke("site:ytn.co.kr 2025ë…„ í˜„ëŒ€ìë™ì°¨ ë¯¸êµ­ ì‹œì¥ ì „ë§ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?")
docs

```

[ì‹¤ìŠµ] ê¸°ì‚¬ ë§í¬ ê°€ì ¸ì˜¤ê¸°

```python
# ê²€ìƒ‰ ê²°ê³¼ì˜ ë§í¬ë“¤ì„ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
links = []

# ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì„¸ë¯¸ì½œë¡ ê³¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³ , ê° ê²°ê³¼ í•­ëª©ì—ì„œ ë§í¬ë¥¼ ì¶”ì¶œ
for doc in docs.split(";\n"):
    print(doc)  # ê° ê²€ìƒ‰ ê²°ê³¼ í•­ëª©ì„ ì¶œë ¥í•˜ì—¬ í™•ì¸
    link = doc.split("link:")[1].strip()  # ê° í•­ëª©ì—ì„œ 'link:' ì´í›„ì˜ URL ë¶€ë¶„ë§Œ ì¶”ì¶œ
    links.append(link)  # ì¶”ì¶œí•œ ë§í¬ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

# ëª¨ë“  ë§í¬ë¥¼ ì¶œë ¥
print(links)
```

links ë¦¬ìŠ¤íŠ¸ë¥¼ webBaseLoaderì˜ web_pathë¡œ ì§€ì •í•˜ê³  ê° ì›¹ í˜ì´ì§€ì˜ ë‚´ìš©ì„ ì½ì–´ ì˜´.
ì—¬ëŸ¬ ì›¹ í˜ì´ì§€ë¥¼ ë™ì‹œì— ì½ì–´ ì˜¤ê¸° ìœ„í•´ ë¹„ë™ê¸° í•¨ìˆ˜ alazy_load() ì‚¬ìš©

```python
# Langchainì˜ WebBaseLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ í˜ì´ì§€ì˜ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
from langchain_community.document_loaders import WebBaseLoader

# WebBaseLoader ê°ì²´ë¥¼ ìƒì„±. 'links'ëŠ” ì›¹ í˜ì´ì§€ì˜ URL ëª©ë¡ì„ ë‹´ê³  ìˆëŠ” ë³€ìˆ˜
# bs_get_text_kwargsëŠ” BeautifulSoupì˜ get_text() ë©”ì†Œë“œì— ì „ë‹¬ë  ì¶”ê°€ ì¸ì
loader = WebBaseLoader(
    web_paths=links,  # ì›¹ í˜ì´ì§€ì˜ ë§í¬ ëª©ë¡ì„ ì§€ì •
    bs_get_text_kwargs={
        "strip": True  # ì›¹ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¬ ë•Œ ì•ë’¤ì˜ ê³µë°±ì„ ì œê±°
    },
)

# ë¹„ë™ê¸°ë¡œ ì›¹ í˜ì´ì§€ì˜ ë‚´ìš©ì„ ë¡œë“œí•˜ê³ , ê° ë¬¸ì„œë¥¼ page_contents ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
page_contents = []  # ê° ì›¹ í˜ì´ì§€ì˜ ë‚´ìš©ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
async for doc in loader.alazy_load():
    page_contents.append(doc)  # ë¶ˆëŸ¬ì˜¨ ë¬¸ì„œë¥¼ page_contents ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

# page_contentsì— ìˆëŠ” ê° ì›¹ í˜ì´ì§€ì˜ ë‚´ìš©ì„ ì¶œë ¥
for content in page_contents:
    print(content)  # ì›¹ í˜ì´ì§€ì˜ ë‚´ìš©ì„ ì¶œë ¥
    print('--------------')  # í˜ì´ì§€ êµ¬ë¶„ì„ ìœ„í•´ êµ¬ë¶„ì„ ì„ ì¶œë ¥
```

[ì‹¤ìŠµ] ë·°í‹°í’€ìˆ˜í”„ë¥¼ ì´ìš©í•´ íŠ¹ì • ì˜ì—­ë§Œ ê°€ì ¸ì˜¤ê¸°

- ë·°í‹°í’€ìˆ˜í”„
    - íŒŒì´ì¬ì—ì„œ HTMLì´ë‚˜ XMLë¬¸ì„œë¥¼ ì‰½ê²Œ íŒŒì‹±í•  ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
    - ì›¹ í˜ì´ì§€ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  íŠ¹ì • íƒœê·¸ì˜ í…ìŠ¤íŠ¸ ë“± ì›í•˜ëŠ” ìš”ì†Œë¥¼ ì‰½ê²Œ ì¶”ì¶œí•  ìˆ˜ ìˆê²Œ í•´ì¤Œ
    - ì›¹ ìŠ¤í¬ë˜í•‘í•  ë•Œ ë§ì´ ì‚¬ìš©

```python
import requests
from bs4 import BeautifulSoup

# ì£¼ì–´ì§„ URLì—ì„œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_article_text(url):
    try:
        # URLì— GET ìš”ì²­ì„ ë³´ëƒ„
        response = requests.get(url)
        # ìš”ì²­ì´ ì„±ê³µí•˜ì§€ ëª»í•˜ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚´
        response.raise_for_status()
        
        # BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ HTML ë‚´ìš©ì„ íŒŒì‹±
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # í´ë˜ìŠ¤ê°€ 'story-news article'ì¸ <article> íƒœê·¸ë¥¼ ì°¾ìŒ
        article = soup.find('article', class_='story-news article')
        
        # ê¸°ì‚¬ë¥¼ ì°¾ì•˜ë‹¤ë©´ ê·¸ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
        if article:
            return article.get_text(strip=True)
        else:
            try:
                if soup.find('article'):
                    return soup.find('article').get_text(strip=True)
                elif soup.find('div', id="CmAdContent"):
                    return soup.find('div', id="CmAdContent").get_text(strip=True)
            except:
                return "ê¸°ì‚¬ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
    # ìš”ì²­ì´ ì‹¤íŒ¨í•  ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
    except requests.exceptions.RequestException as e:
        return f"URLì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
```

```python
# URL ëª©ë¡ì˜ ê° ë§í¬ë¥¼ ë°˜ë³µí•˜ë©´ì„œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥
articles = []    # ê°€ì ¸ì˜¨ ë‚´ìš©ì„ ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸° ìœ„í•œ ë³€ìˆ˜ ì„ ì–¸
for link in links:
    print(f"URL: {link}\n")
    article_text = get_article_text(link)
    print(f"Content:\n{article_text}")
    print("--------------------------------------------------")
    articles.append(article_text)
```

```python
chat_history.add_message("\n".join(articles))
chat_history.add_user_message("2025ë…„ í˜„ëŒ€ìë™ì°¨ ë¯¸êµ­ ì‹œì¥ ì „ë§ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?") 

# ë¬¸ì„œ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±
answer = document_chain.invoke(
    {
        "messages": chat_history.messages,
        "context": docs,
    }
)

# ìƒì„±ëœ ë‹µë³€ ë©”ëª¨ë¦¬ì— ì €ì¥
chat_history.add_ai_message(answer) 
print(answer)

```

10-2 ìë£Œ ì¡°ì‚¬ í›„ ê¸°ì‚¬ ì“°ê¸° - íƒ€ë¹Œë¦¬ ê²€ìƒ‰
- íƒ€ë¹Œë¦¬ ê²€ìƒ‰: ìœ ë¡œ êµ¬ë… ì„œë¹„ìŠ¤, ì•ˆì •ì„± ì¸¡ë©´ì—ì„œëŠ” ë•ë•ê³ ë³´ë‹¤ ì¢‹ìŒ.

íƒ€ë¹Œë¦¬ ì‚¬ì´íŠ¸ì—ì„œ APIí‚¤ ë°œê¸‰ í›„ envì— ì €ì¥

```python
%pip install tavily-python
```

```python
from langchain_community.tools import TavilySearchResults

tavily_search = TavilySearchResults(
    max_results=5
)
```

```python
res = tavily_search.invoke({"query": "2025 í•œêµ­ ê²½ì œ ì „ë§"})
for r in res:
    print(r)
```

```python
from IPython.display import JSON
JSON(res).data

```

TavilyClientë¥¼ ì‚¬ìš©í•´ì„œ ì›¹í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°

    include_raw_content=True í•´ì¤˜ì•¼ í•¨!

```python
from tavily import TavilyClient

client = TavilyClient()

content = client.search(
    "2025ë…„ í•œêµ­ ê²½ì œ ì „ë§",  
    search_depth="advanced",
    include_raw_content=True,
)

JSON(content).data

```

[ì‹¤ìŠµ] ì¸í„°ë„·ì—ì„œ ìë£Œ ì¡°ì‚¬ í›„ ê¸°ì‚¬ ì“°ëŠ” ê¸°ì ë§Œë“¤ê¸°

```python
from langchain.adapters.openai import convert_openai_messages
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o")

# Step 1. ì§ˆë¬¸ì— ëŒ€í•œ ìë£Œë¥¼ TavilyClientë¡œ ê°€ì ¸ì˜¤ê¸°
query = "í˜„ëŒ€ìë™ì°¨ ë¯¸êµ­ ì‹œì¥ 2025ë…„ ì „ë§"

content = client.search(
    query, 
    include_raw_content=True,
    search_depth="advanced"
)["results"]

content

```

f-stringì„ ì´ìš©í•´ queryì™€ ê²€ìƒ‰ ê²°ê³¼ì¸ contextë¥¼ ë„£ì–´ì„œ ë©”ì‹œì§€ë¥¼ ë§Œë“¦

```python
# Step 2. í”„ë¡¬í”„íŠ¸ ì„¤ì •
query = "í˜„ëŒ€ìë™ì°¨ ë¯¸êµ­ ì‹œì¥ 2025ë…„ ì „ë§"
prompt = [{
    "role": "system",
    "content":  (
        "ë‹¹ì‹ ì€ ì‹ ë¬¸ê¸°ì‚¬ë¥¼ ì“°ëŠ” ê¸°ì AIì…ë‹ˆë‹¤.  \n"
        "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì ì´ê³  ì²´ê³„ì ìœ¼ë¡œ ì‘ì„±ëœ ê¸°ì‚¬ë¥¼ ì¨ì•¼ í•©ë‹ˆë‹¤. \n"
    )
}, {
    "role": "user",
    "content": (
        f'ì •ë³´: """{content}"""\n\n' 
        f'ìœ„ì˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬, ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ìì„¸í•œ ë³´ê³ ì„œë¥¼ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”: "{query}" \n'
        'â€”ì‹ ë¬¸ê¸°ì‚¬ í˜•ì‹ì„ ì‚¬ìš©í•˜ë˜, MLAë¥¼ ì¤€ìˆ˜í•˜ëŠ” markdown ë¬¸ë²•ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.'
        'â€”í™œìš©í•œ ìë£ŒëŠ” ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.'
    )
    
}]
```

```python
# Step 3. ì˜¤í”ˆAIë¥¼ ë­ì²´ì¸ì¸ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ì¶œë ¥
report = llm.invoke(prompt).content
print(report)
```

10-3 ìœ íŠ¸ë¸Œ ì˜ìƒ ìš”ì•½í•˜ê¸°

10-4 ì›¹ê³¼ ìœ íŠœë¸Œ ê²€ìƒ‰ì„ í™œìš©í•œ ì±—ë´‡ ë§Œë“¤ê¸°

11-1 ë”¥ì‹œí¬ ëª¨ë¸ ì•Œì•„ë³´ê¸°

ì†Œê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ë“±ì¥
- GPTë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì§ˆì˜í•  ë•Œë§ˆë‹¤ í† í° í¬ê¸°ì— ë”°ë¥¸ ë¹„ìš© ë°œìƒ
- ë‚´ ì»´í“¨í„° ì•ˆì— ì €ì¥ëœ ìë£Œë¥¼ ì˜¤í”ˆ AI ì„œë²„ì— ì „ì†¡í•´ì•¼í•˜ëŠ” ë³´ì•ˆ ë¬¸ì œ
- í† í°ë‹¹ ë¶€ê³¼ë˜ëŠ” ìš”ê¸ˆì œë¡œ ì¸í•´ ê¸°ì—…, ê°œì¸ì—ê²Œ ê³¨ì¹«ê±°ë¦¬ì˜€ìŒ
- ì´ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì¼ë°˜ PCë‚˜ ìŠ¤ë§ˆíŠ¸í°ì—ì„œë„ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì†Œê·œëª¨ ì–¸ì–´ ëª¨ë¸ ìˆ˜ìš”ê°€ ëŠ˜ ìˆì—ˆìŒ
- LLMì„ ì €ì‚¬ì–‘PCì—ì„œë„ ì‘ë™í•  ìˆ˜ ìˆë„ë¡ ê²½ëŸ‰í™”í•œ ëª¨ë¸ ë“±ì¥

ë”¥ì‹œí¬-R1 ëª¨ë¸

- ë”¥ì‹œí¬ì—ì„œ ê°œë°œí•œ ì—¬ëŸ¬ ë²„ì „ì˜ ì–¸ì–´ ëª¨ë¸ì„ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ê³µê°œ
- ì¼ë°˜ PCì—ì„œ LLM ì‹¤í–‰ì€ ì–´ë µê³  LLMì„ ê²½ëŸ‰í™”í•œ ëª¨ë¸ë“¤ë„  í•¨ê»˜ ê³µê°œë¨

[ì‹¤ìŠµ] ì˜¬ë¼ë§ˆì™€ ë”¥ì‹œí¬-R1 ëª¨ë¸ ì„¤ì¹˜í•˜ê¸°

- [ì˜¬ë¼ë§ˆ(Ollama)](https://ollama.com/download)
    - ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë°°í¬/ì‹¤í–‰í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬
    - ë¡œì»¬í™˜ê²½ì—ì„œ ì–¸ì–´ ëª¨ë¸ ì‰½ê²Œ ì„¤ì¹˜& ì‹¤í–‰ì— ìµœì í™”

```python
ollama run deepseek-r1:8b
```

ê²½ëŸ‰í™”ëœ ì–¸ì–´ ëª¨ë¸ì¸ë°ë„ ë‹µë³€ í’ˆì§ˆì´ ê½¤ ì¢‹ì§€ë§Œ, ì¢…ì¢… ì•„ëì–´, ì¼ë³¸ì–´ë‚˜ íƒœêµ­ì–´ë¥¼ ì„ì–´ì„œ ë‹µë³€í•œë‹¤ëŠ” ë¬¸ì œì ì´ ìˆìŒ

11-2 ë­ì²´ì¸ì—ì„œ ë”¥ì‹œí¬ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°

```python
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOllama(model="deepseek-r1:8b") 

messages = [
    SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."),
]

while True:
    user_input = input("ì‚¬ìš©ì: ")

    if user_input == "exit":
        break
    
    messages.append( 
        HumanMessage(user_input)
    )  
    
    ai_response = llm.invoke(messages)
    messages.append(
        ai_response
    )  

    print("AI: " + ai_response.content)

```

```python
pip install langchain-ollama
```

ìŠ¤íŠ¸ë¦¼ ì¶œë ¥ì´ ë˜ì§€ ì•Šì•„ ê²°ê³¼ ë°›ê¸°ê¹Œì§€ ì˜¤ë˜ ê±¸ë¦¼
<img width="825" height="443" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-12-01 á„‹á…©á„’á…® 10 53 34" src="https://github.com/user-attachments/assets/2ad67680-b4a9-4f9d-ae02-1f13e891847e" />

ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ì¶œë ¥ë˜ê²Œ ì½”ë“œ ìˆ˜ì •

```python
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

llm = ChatOllama(model="deepseek-r1:8b") 

messages = [
    SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."),
]

while True:
    user_input = input("ì‚¬ìš©ì: ")

    if user_input == "exit":
        break
    
    messages.append( 
        HumanMessage(user_input)
    )  
    
    response = llm.stream(messages) # ìŠ¤íŠ¸ë¦¼ ì¶œë ¥
    # responseë¡œ ìŠ¤íŠ¸ë¦¼ ì¶œë ¥ë˜ëŠ” ë¶€ë¶„ì„ ë°›ì•„ í„°ë¯¸ë„ ì°½ì— ì°¨ë¡€ë¡œ ì¶œë ¥
    ai_message = None
    for chunk in response:
        print(chunk.content, end="")
        if ai_message is None:
            ai_message = chunk
        else:
            ai_message += chunk
    print('')
    # ì±…ì—ì„œëŠ” </think> ì´í›„ë§Œ ì¶œë ¥ë˜ê²Œ split ì²˜ë¦¬í–ˆëŠ”ë°, í˜„ì¬ëŠ” ë¶ˆí•„ìš”í•¨
    message_only = ai_message.content 
    messages.append(AIMessage(message_only))

    # print("AI: " + response.content)

```

11-3 ë”¥ì‹œí¬ì— ê¸°ë°˜í•œ RAG ë§Œë“¤ê¸°

[ì‹¤ìŠµ] ë”¥ì‹œí¬ë¡œ RAG ë§Œë“¤ê¸°

```python
# rag_deepseek.py

import streamlit as st
from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
import retriever

# ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOllama(model="deepseek-r1:14b")

# ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
def get_ai_response(messages, docs):    
    response = retriever.document_chain.stream({
        "messages": messages,
        "context": docs
    })

    for chunk in response:
        yield chunk

# Streamlit ì•±
st.title("ğŸ’¬ DeepSeek-R1 Langchain Chat")

# ìŠ¤íŠ¸ë¦¼ë¦¿ session_stateì— ë©”ì‹œì§€ ì €ì¥
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        SystemMessage("ë„ˆëŠ” ë¬¸ì„œì— ê¸°ë°˜í•´ ë‹µë³€í•˜ëŠ” ë„ì‹œ ì •ì±… ì „ë¬¸ê°€ì•¼ "),  
        AIMessage("How can I help you?")
    ]

# ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    if msg.content:
        if isinstance(msg, SystemMessage):
            st.chat_message("system").write(msg.content)
        elif isinstance(msg, AIMessage):
            st.chat_message("assistant").write(msg.content)
        elif isinstance(msg, HumanMessage):
            st.chat_message("user").write(msg.content)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input():
    st.chat_message("user").write(prompt) # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
    st.session_state.messages.append(HumanMessage(prompt)) # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥

    augmented_query = retriever.query_augmentation_chain.invoke({
        "messages": st.session_state["messages"],
        "query": prompt,
    })
    print("augmented_query\t", augmented_query)

    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    print("ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰")
    docs = retriever.retriever.invoke(f"{prompt}\n{augmented_query}")

    for doc in docs:
        print('---------------')
        print(doc)   
        with st.expander(f"**ë¬¸ì„œ:** {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}"):
            # íŒŒì¼ëª…ê³¼ í˜ì´ì§€ ì •ë³´ í‘œì‹œ
            st.write(f"**page:**{doc.metadata.get('page', '')}")
            st.write(doc.page_content)
    print("===============")

    with st.spinner(f"AIê°€ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤... '{augmented_query}'"):
        response = get_ai_response(st.session_state["messages"], docs)
        result = st.chat_message("assistant").write_stream(response) # AI ë©”ì‹œì§€ ì¶œë ¥
    st.session_state["messages"].append(AIMessage(result)) # AI ë©”ì‹œì§€ ì €ì¥    

```

```python
# retriever.py
# ì„ë² ë”© ëª¨ë¸ ì„ ì–¸í•˜ê¸°
from langchain_openai import OpenAIEmbeddings
embedding = OpenAIEmbeddings(model='text-embedding-3-large')

# ì–¸ì–´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
from langchain_ollama import ChatOllama
llm = ChatOllama(model="deepseek-r1:14b")

# Load Chroma store
from langchain_chroma import Chroma
print("Loading existing Chroma store")
persist_directory = 'C:/github/gpt_agent_2025_easyspub/chap09/chroma_store'

vectorstore = Chroma(
    persist_directory=persist_directory, 
    embedding_function=embedding
)

# Create retriever
retriever = vectorstore.as_retriever(k=3)

# Create document chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser # ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

question_answering_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ contextì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ë¼.:\n\n{context}",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

document_chain = create_stuff_documents_chain(llm, question_answering_prompt) | StrOutputParser()

# query augmentation chain
query_augmentation_prompt = ChatPromptTemplate.from_messages(
    [
        MessagesPlaceholder(variable_name="messages"), # ê¸°ì¡´ ëŒ€í™” ë‚´ìš©
        (
            "system",
            "ê¸°ì¡´ì˜ ëŒ€í™” ë‚´ìš©ì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì•„ë˜ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ëª…ë£Œí•œ í•œ ë¬¸ì¥ì˜ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•˜ë¼. ëŒ€ëª…ì‚¬ë‚˜ ì´, ì €, ê·¸ì™€ ê°™ì€ í‘œí˜„ì„ ëª…í™•í•œ ëª…ì‚¬ë¡œ í‘œí˜„í•˜ë¼. :\n\n{query}",
        ),
    ]
)

query_augmentation_chain = query_augmentation_prompt | llm | StrOutputParser()

```

Q) ë­ì²´ì¸ìœ¼ë¡œ ë§Œë“  ëª¨ë“  ì• í”Œë¦¬ì¼€ì´ì…˜ì„ GPTê°€ ì•„ë‹Œ ë”¥ì‹œí¬-R1 ëª¨ë¸ë¡œ ë³€ê²½ ê°€ëŠ¥í•œ ì§€?
ë“±ì¥í•œ ì§€ ì–¼ë§ˆ ì•ˆë˜ì„œ ëª‡ ê°€ì§€ ì œì•½ì´ ìˆìŒ.
ë„êµ¬ í˜¸ì¶œí•˜ëŠ” ê¸°ëŠ¥ì€ ì§€ì› ì•ˆë¨.
ì´í›„ ë” ë‚˜ì€ ëª¨ë¸ì´ ê³µê°œëì„ ìˆ˜ ìˆìœ¼ë‹ˆ, ì ì ˆí•œ ëª¨ë¸ì„ ì„ íƒí•´ì„œ í™œìš©í•˜ê¸¸ ë°”ëŒ.    

### 12ì¥ ë­ê·¸ë˜í”„ì™€ ì¹œí•´ì§€ê¸°

12-1 ë­ê·¸ë˜í”„ë¡œ ë§Œë“œëŠ” ê¸°ë³¸ ì±—ë´‡

ë­ê·¸ë˜í”„ë€?

- ë­ì²´ì¸ì—ì„œ í•œë°œ ë” ë‚˜ì•„ê°€ ì—¬ëŸ¬ AI ì—ì´ì „íŠ¸ë¥¼ ì—°ê²°í•˜ì—¬ ìƒí™©ì— ë§ê²Œ ë‹¤ìŒ ì‘ì—…ì„ í•˜ë„ë¡ êµ¬ì„±í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í”„ë ˆì„ ì›Œí¬

ë­ê·¸ë˜í”„ì˜ ê¸°ë³¸ ê°œë… - ë…¸ë“œ, ì—£ì§€, ìƒíƒœ

- ë…¸ë“œ: í•˜ë‚˜ì˜ ì‘ì—…ì´ë‚˜ ë‹¨ê³„
- ì—£ì§€: ë…¸ë“œì˜ ì—°ê²°ì„ ë‚˜íƒ€ë‚´ëŠ” í™”ì‚´í‘œ
- ìƒíƒœ: ë…¸ë“œê°€ ì‘ì—…í•œ ê²°ê³¼ë¥¼ ê¸°ë¡í•´ ë‘ëŠ” ì‘ì—… ì¼ì§€

![image.png](attachment:1ab9cf83-c4c1-455a-95f8-7a9c88db2c62:image.png)

[ì‹¤ìŠµ] ë­ê·¸ë˜í”„ë¡œ ê°„ë‹¨í•œ ì±—ë´‡ ë§Œë“¤ê¸°

```python

// ë­ê·¸ë˜í”„ ì„¤ì¹˜
%pip install langgraph

// GPT ëª¨ë¸ ì„¤ì •í•˜ê¸°
from langchain_openai import ChatOpenAI

# ëª¨ë¸ ì´ˆê¸°í™”
model = ChatOpenAI(model="gpt-4o-mini")
model.invoke('ì•ˆë…•í•˜ì„¸ìš”!')

```

[ì‹¤ìŠµ] ìƒíƒœ ì •ì˜í•˜ê¸°

- ë­ê·¸ë˜í”„ì—ì„œ ìƒíƒœëŠ” ì–¸ì–´ ëª¨ë¸ì´ ì„ë¬´ë¥¼ ìˆ˜í–‰í•˜ë©´ì„œ í˜„ì¬ ìƒíƒœë¥¼ ëª…í™•íˆ ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ìš”ì†Œ

```python
from typing import Annotated # annotatedëŠ” íƒ€ì… íŒíŠ¸ë¥¼ ì‚¬ìš©í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
from typing_extensions import TypedDict # TypedDictëŠ” ë”•ì…”ë„ˆë¦¬ íƒ€ì…ì„ ì •ì˜í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

class State(TypedDict):	# ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ê´€ë¦¬
    """
    State í´ë˜ìŠ¤ëŠ” TypedDictë¥¼ ìƒì†ë°›ìŠµë‹ˆë‹¤.

    ì†ì„±:
        messages (Annotated[list[str], add_messages]): ë©”ì‹œì§€ë“¤ì€ "list" íƒ€ì…ì„ ê°€ì§‘ë‹ˆë‹¤.   #â‘¡
       'add_messages' í•¨ìˆ˜ëŠ” ì´ ìƒíƒœ í‚¤ê°€ ì–´ë–»ê²Œ ì—…ë°ì´íŠ¸ë˜ì–´ì•¼ í•˜ëŠ”ì§€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.  #â‘¢
        (ì´ ê²½ìš°, ë©”ì‹œì§€ë¥¼ ë®ì–´ì“°ëŠ” ëŒ€ì‹  ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤)
    """
    messages: Annotated[list[str], add_messages]	#â‘¡

# StateGraph í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ State íƒ€ì…ì˜ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
graph_builder = StateGraph(State) #â‘£
```

[ì‹¤ìŠµ] ë…¸ë“œ ìƒì„±í•˜ê¸°
ê¸°ì¡´ì˜ ëŒ€í™” ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ GPTë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ ë§Œë“¤ê¸°

```python
def generate(state: State):	
    """
    ì£¼ì–´ì§„ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì±—ë´‡ì˜ ì‘ë‹µ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
    state (State): í˜„ì¬ ëŒ€í™” ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°ì²´ë¡œ, ì´ì „ ë©”ì‹œì§€ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
		
    ë°˜í™˜ê°’:
    dict: ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µ ë©”ì‹œì§€ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬. 
          í˜•ì‹ì€ {"messages": [ì‘ë‹µ ë©”ì‹œì§€]}ì…ë‹ˆë‹¤.
    """ 
    return {"messages": [model.invoke(state["messages"])]}

graph_builder.add_node("generate_v2", generate)	
```

[ì‹¤ìŠµ] ì—£ì§€ ì„¤ì •í•˜ê¸°

```
// graph ì„ ì–¸í•˜ê¸° 
graph_builder.add_edge(START, "generate")
graph_builder.add_edge("generate", END)    

graph = graph_builder.compile()

// ê·¸ë˜í”„ ë„ì‹í™”í•˜ê¸°
from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception: 
    pass

// ë¦¬ìŠ¤íŠ¸ì— ë¬¸ì¥ ì¶”ê°€í•˜ê¸°
response = graph.invoke({"messages": ["ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ë¹„ë¹„ì…ë‹ˆë‹¤"]})

print(type(response))
response

// ì´ì „ ëŒ€í™” ë‚´ìš©ì— ìƒˆ ë©”ì‹œì§€ ì¶”ê°€í•˜ê¸°
response["messages"].append("ì œ ì´ë¦„ì„ ì•„ì‹œë‚˜ìš”?")
graph.invoke(response)
```

[ì‹¤ìŠµ] ìŠ¤íŠ¸ë¦¼ ì¶œë ¥í•˜ê¸°

```python
inputs = {"messages": [("human", "í•œêµ­ê³¼ ì¼ë³¸ì˜ ê´€ê³„ì— ëŒ€í•´ ìì„¸íˆ ì•Œë ¤ì¤˜")]}
for chunk, _ in graph.stream(inputs, stream_mode="messages"):
    print(chunk.content, end="")
```

12-2 ëŒ€í™” ë‚´ìš©ì„ ì €ì¥í•˜ëŠ” ë©”ëª¨ë¦¬
[ì‹¤ìŠµ] ë­ê·¸ë˜í”„ì˜ ë©”ëª¨ë¦¬ ê¸°ëŠ¥ í™œìš©í•˜ê¸°

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini")

from typing import Annotated # annotatedëŠ” íƒ€ì… íŒíŠ¸ë¥¼ ì‚¬ìš©í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
from typing_extensions import TypedDict # TypedDictëŠ” ë”•ì…”ë„ˆë¦¬ íƒ€ì…ì„ ì •ì˜í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

class State(TypedDict):
    """
    State í´ë˜ìŠ¤ëŠ” TypedDictë¥¼ ìƒì†ë°›ìŠµë‹ˆë‹¤.

    ì†ì„±:
        messages (Annotated[list[str], add_messages]): ë©”ì‹œì§€ë“¤ì€ "list" íƒ€ì…ì„ ê°€ì§‘ë‹ˆë‹¤.
       'add_messages' í•¨ìˆ˜ëŠ” ì´ ìƒíƒœ í‚¤ê°€ ì–´ë–»ê²Œ ì—…ë°ì´íŠ¸ë˜ì–´ì•¼ í•˜ëŠ”ì§€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
        (ì´ ê²½ìš°, ë©”ì‹œì§€ë¥¼ ë®ì–´ì“°ëŠ” ëŒ€ì‹  ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤)
    """
    messages: Annotated[list[str], add_messages]

# StateGraph í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ State íƒ€ì…ì˜ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
graph_builder = StateGraph(State)

def generate(state: State):
    """
    ì£¼ì–´ì§„ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì±—ë´‡ì˜ ì‘ë‹µ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
    state (State): í˜„ì¬ ëŒ€í™” ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°ì²´ë¡œ, ì´ì „ ë©”ì‹œì§€ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
		
    ë°˜í™˜ê°’:
    dict: ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µ ë©”ì‹œì§€ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬. 
          í˜•ì‹ì€ {"messages": [ì‘ë‹µ ë©”ì‹œì§€]}ì…ë‹ˆë‹¤.
    """ 
    return {"messages": [model.invoke(state["messages"])]}

graph_builder.add_node("generate", generate)

graph_builder.add_edge(START, "generate")
graph_builder.add_edge("generate", END)    

graph = graph_builder.compile()

#------------ ì—¬ê¸°ì„œë¶€í„° ë‹¬ë¼ì§„ ì½”ë“œê°€ ìˆìŠµë‹ˆë‹¤.   
from langchain.schema import HumanMessage

while True:
    user_input = input("You\t:")
    
    if user_input in ["exit", "quit", "q"]:
        break
	#â‘¡
    for event in graph.stream({"messages": [HumanMessage(user_input)]}, stream_mode="values"):
        event["messages"][-1].pretty_print()

    print(f'\ní˜„ì¬ ë©”ì‹œì§€ ê°¯ìˆ˜: {len(event["messages"])}\n-------------------\n')

```

ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•œ ìƒíƒœë¡œ ëŒ€í™” ì´ì–´ê°€ê²Œ ì„¤ì •

```
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini")

from typing import Annotated # annotatedëŠ” íƒ€ì… íŒíŠ¸ë¥¼ ì‚¬ìš©í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
from typing_extensions import TypedDict # TypedDictëŠ” ë”•ì…”ë„ˆë¦¬ íƒ€ì…ì„ ì •ì˜í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

class State(TypedDict):
    """
    State í´ë˜ìŠ¤ëŠ” TypedDictë¥¼ ìƒì†ë°›ìŠµë‹ˆë‹¤.

    ì†ì„±:
        messages (Annotated[list[str], add_messages]): ë©”ì‹œì§€ë“¤ì€ "list" íƒ€ì…ì„ ê°€ì§‘ë‹ˆë‹¤.
       'add_messages' í•¨ìˆ˜ëŠ” ì´ ìƒíƒœ í‚¤ê°€ ì–´ë–»ê²Œ ì—…ë°ì´íŠ¸ë˜ì–´ì•¼ í•˜ëŠ”ì§€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
        (ì´ ê²½ìš°, ë©”ì‹œì§€ë¥¼ ë®ì–´ì“°ëŠ” ëŒ€ì‹  ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤)
    """
    messages: Annotated[list[str], add_messages]

# StateGraph í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ State íƒ€ì…ì˜ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
graph_builder = StateGraph(State)

def generate(state: State):
    """
    ì£¼ì–´ì§„ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì±—ë´‡ì˜ ì‘ë‹µ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
    state (State): í˜„ì¬ ëŒ€í™” ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°ì²´ë¡œ, ì´ì „ ë©”ì‹œì§€ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
		
    ë°˜í™˜ê°’:
    dict: ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µ ë©”ì‹œì§€ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬. 
          í˜•ì‹ì€ {"messages": [ì‘ë‹µ ë©”ì‹œì§€]}ì…ë‹ˆë‹¤.
    """ 
    return {"messages": [model.invoke(state["messages"])]}

graph_builder.add_node("generate", generate)

graph_builder.add_edge(START, "generate")
graph_builder.add_edge("generate", END)    

from langgraph.checkpoint.memory import MemorySaver
memory = MemorySaver()

config = {"configurable": {"thread_id": "abcd"}}

graph = graph_builder.compile(checkpointer=memory)

#------------ ì—¬ê¸°ì„œë¶€í„° ë‹¬ë¼ì§„ ì½”ë“œê°€ ìˆìŠµë‹ˆë‹¤.   
from langchain.schema import HumanMessage

while True:
    user_input = input("You\t:")
    
    if user_input in ["exit", "quit", "q"]:
        break
	#â‘¡
    for event in graph.stream({
        "messages": [HumanMessage(user_input)]}, 
        config,
        stream_mode="values"
    ):
        event["messages"][-1].pretty_print()

    print(f'\ní˜„ì¬ ë©”ì‹œì§€ ê°¯ìˆ˜: {len(event["messages"])}\n-------------------\n')

```

12-3 ì¸í„°ë„· ê²€ìƒ‰ í›„ ê¸°ì‚¬ë¥¼ ì‘ì„±í•˜ëŠ” ì±—ë´‡ ë§Œë“¤ê¸°

[ì‹¤ìŠµ] ì‹ ë¬¸ê¸°ì ì±—ë´‡ ë§Œë“¤ê¸°

```python
# ì–¸ì–´ ëª¨ë¸ ì„¤ì •í•˜ê¸°
from langchain_openai import ChatOpenAI

# ëª¨ë¸ ì´ˆê¸°í™”
model = ChatOpenAI(model="gpt-4o", temperature=0.01)
model.invoke('ì•ˆë…•í•˜ì„¸ìš”!')
```

```python
# ìƒíƒœ ì„¤ì •í•˜ê¸°
from typing import Annotated # annotatedëŠ” íƒ€ì… íŒíŠ¸ë¥¼ ì‚¬ìš©í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
from typing_extensions import TypedDict # TypedDictëŠ” ë”•ì…”ë„ˆë¦¬ íƒ€ì…ì„ ì •ì˜í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

class State(TypedDict):
    """
    State í´ë˜ìŠ¤ëŠ” TypedDictë¥¼ ìƒì†ë°›ìŠµë‹ˆë‹¤.

    ì†ì„±:
        messages (Annotated[list[str], add_messages]): ë©”ì‹œì§€ë“¤ì€ "list" íƒ€ì…ì„ ê°€ì§‘ë‹ˆë‹¤. 
        ì£¼ì„ì— ìˆëŠ” 'add_messages' í•¨ìˆ˜ëŠ” ì´ ìƒíƒœ í‚¤ê°€ ì–´ë–»ê²Œ ì—…ë°ì´íŠ¸ë˜ì–´ì•¼ í•˜ëŠ”ì§€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
        (ì´ ê²½ìš°, ë©”ì‹œì§€ë¥¼ ë®ì–´ì“°ëŠ” ëŒ€ì‹  ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤)
    """
    messages: Annotated[list[str], add_messages]

# StateGraph í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ State íƒ€ì…ì˜ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
graph_builder = StateGraph(State) 
```

```python
#ë„êµ¬ ë“±ë¡í•˜ê¸°
from langchain_core.tools import tool
from datetime import datetime
import pytz
from langchain_community.tools import DuckDuckGoSearchResults
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper

import bs4
from langchain_community.document_loaders import WebBaseLoader

# ë„êµ¬ í•¨ìˆ˜ ì •ì˜
@tool
def get_current_time(timezone: str, location: str) -> str:
    """í˜„ì¬ ì‹œê°ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜."""
    try:
        tz = pytz.timezone(timezone)
        now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        result = f'{timezone} ({location}) í˜„ì¬ì‹œê° {now}'
        # print(result)
        return result
    except pytz.UnknownTimeZoneError:
        return f"ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì„ì¡´: {timezone}"
    
@tool
def get_web_search(query: str, search_period: str='m') -> str:
    """
    ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜.

    Args:
        query (str): ê²€ìƒ‰ì–´
        search_period (str): ê²€ìƒ‰ ê¸°ê°„ (e.g., "w" for past week (default), "m" for past month, "y" for past year, "d" for past day)

    Returns:
        str: ê²€ìƒ‰ ê²°ê³¼
    """
    wrapper = DuckDuckGoSearchAPIWrapper(
        # region="kr-kr", 
        time=search_period
    )

    print('\n-------- WEB SEARCH --------')
    print(query)
    print(search_period)

    search = DuckDuckGoSearchResults(
        api_wrapper=wrapper,
        # source="news",
        results_separator=';\n'
    )

    searched = search.invoke(query)
    
    for i, result in enumerate(searched.split(';\n')):
        print(f'{i+1}. {result}')
    
    return searched

# ë„êµ¬ ë°”ì¸ë”©
tools = [get_current_time, get_web_search]
```

```python
# í˜„ì¬ ì‹œê°ì„ ì–»ëŠ” ë„êµ¬ get_current_time ì‹¤í–‰ í…ŒìŠ¤íŠ¸í•˜ê¸°
tools[0].invoke({"timezone": "Asia/Seoul", "location": "ì„œìš¸"})
```

```python
# ì›¹ ê²€ìƒ‰ ë„êµ¬ get_web_search ì‹¤í–‰ í…ŒìŠ¤íŠ¸í•˜ê¸°
tools[1].invoke({"query": "íŒŒì´ì¬", "search_period": "m"})
```

```python
# ë„êµ¬ ëª©ë¡ í™•ì¸í•˜ê¸°
for tool in tools:
    print(tool.name, tool)
```

```python
# ì–¸ì˜¤ ëª¨ë¸ì— .bind_toolë¡œ ì‚¬ìš©í•  ë„êµ¬ ì—°ê²°í•˜ê¸°
model_with_tools = model.bind_tools(tools) # GPT ì–¸ì–´ëª¨ë¸ì— ë„êµ¬ ì—°ê²°

def generate(state: State):
    """
    ì£¼ì–´ì§„ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì±—ë´‡ì˜ ì‘ë‹µ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
    state (State): í˜„ì¬ ëŒ€í™” ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°ì²´ë¡œ, ì´ì „ ë©”ì‹œì§€ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

    ë°˜í™˜ê°’:
    dict: ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µ ë©”ì‹œì§€ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬. 
          í˜•ì‹ì€ {"messages": [ì‘ë‹µ ë©”ì‹œì§€]}ì…ë‹ˆë‹¤.
    """
    return {"messages": model_with_tools.invoke(state["messages"])}

graph_builder.add_node("generate", generate)
```

```python
# ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ë…¸ë“œ í´ë˜ìŠ¤ ìƒì„±í•˜ê¸°
import json
from langchain_core.messages import ToolMessage

class BasicToolNode:
    """
    ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ëŠ” ë…¸ë“œ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ë§ˆì§€ë§‰ AIMessageì—ì„œ ìš”ì²­ëœ ë„êµ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    Attributes:
        tools_by_name (dict): ë„êµ¬ ì´ë¦„ì„ í‚¤ë¡œ í•˜ê³  ë„êµ¬ ê°ì²´ë¥¼ ê°’ìœ¼ë¡œ ê°€ì§€ëŠ” ì‚¬ì „ì…ë‹ˆë‹¤.
    Methods:
        __init__(tools: list): ë„êµ¬ ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        __call__(inputs: dict): ì…ë ¥ ë©”ì‹œì§€ë¥¼ ë°›ì•„ì„œ ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    """A node that runs the tools requested in the last AIMessage."""

    def __init__(self, tools: list) -> None:    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë„êµ¬ ì´ë¦„ê³¼ ë„êµ¬ ìì²´ë¥¼ ì €ì¥
        self.tools_by_name = {tool.name: tool for tool in tools}

    def __call__(self, inputs: dict):    # inputì€ ë­ ê·¸ë˜í”„ì—ì„œ ìƒíƒœ ê´€ë¦¬ stateê°€ ë”•ì…”ë„ˆë¦¬ë¡œ ì „ë‹¬ë¨.
        if messages := inputs.get("messages", []):
            # inputsì— messagesê°€ ìˆìœ¼ë©´ messagesë¥¼ ê°€ì ¸ì˜¤ê³  ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            message = messages[-1]
        else:
            raise ValueError("No message found in input")
        outputs = []
        for tool_call in message.tool_calls:    # 
            tool_result = self.tools_by_name[tool_call["name"]].invoke(
                tool_call["args"]
            )
            outputs.append(
                ToolMessage(
                    content=json.dumps(tool_result),
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                )
            )
        return {"messages": messages + outputs}

tool_node = BasicToolNode(tools=tools)
graph_builder.add_node("tools", tool_node) # BasicToolNode í´ë˜ìŠ¤ë¥¼ tools ë…¸ë“œë¡œ graph_builderì— ì¶”ê°€
```

[ì‹¤ìŠµ] ë¼ìš°í„° ì„¤ì •í•˜ê¸°

- AI ì—ì´ì „íŠ¸ê°€ ë­ê·¸ë˜í”„ ë‚´ì—ì„œ ìŠ¤ìŠ¤ë¡œ ë‹¤ìŒ ê²½ë¡œë¥¼ ì„ íƒí•´ì•¼ í•  ë•Œ ë¼ìš°í„°ë¥¼ í™œìš©í•¨
- ë¼ìš°íŒ…: ìƒí™©ì— ë”°ë¼ ë°©í–¥ì„ ê²°ì •í•˜ëŠ” ê²ƒ
- ì¡°ê±´ì— ë”°ë¼ í™œì„±í™” or ë¹„í™œì„±í™”ë˜ëŠ” ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ì‚¬ìš©

```python
# ì–¸ì–´ ëª¨ë¸ì´ ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ë„ë¡ ë¼ìš°í„° ì„¤ì •
def route_tools(state: State):
    """
    ë§ˆì§€ë§‰ ë©”ì‹œì§€ì— ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ” ê²½ìš° ToolNodeë¡œ ë¼ìš°íŒ…í•˜ê³ ,
    ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° ëìœ¼ë¡œ ë¼ìš°íŒ…í•˜ê¸° ìœ„í•´ conditional_edgeì—ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    if isinstance(state, list):
        ai_message = state[-1]
    elif messages := state.get("messages", []):
        ai_message = messages[-1]
    else:
        raise ValueError(f"tool_edge ì…ë ¥ ìƒíƒœì—ì„œ ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {state}")
    if hasattr(ai_message, "tool_calls") and len(ai_message.tool_calls) > 0:
        return "tools"
    return END

graph_builder.add_edge(START, "generate")

graph_builder.add_conditional_edges(
    "generate",
    route_tools,
    {"tools": "tools", END: END},
)
# ë„êµ¬ê°€ í˜¸ì¶œë  ë•Œë§ˆë‹¤ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ ì±—ë´‡ìœ¼ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.
graph_builder.add_edge("tools", "generate")
graph = graph_builder.compile()

```

```python
#ê·¸ë˜í”„ ì¶œë ¥í•˜ê¸°
from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    pass
```

[ì‹¤ìŠµ] ë„êµ¬ í…ŒìŠ¤íŠ¸í•˜ê³  ê¸°ì‚¬ ì‘ì„±í•˜ê¸°

```python
from langchain_core.messages import AIMessageChunk, HumanMessage

inputs = [HumanMessage(content="ì§€ê¸ˆ ì„œìš¸ ëª‡ì‹œì•¼?")]

gathered = None

for msg, metadata in graph.stream({"messages": inputs}, stream_mode="messages"):
    if isinstance(msg, AIMessageChunk):
        print(msg.content, end='')

        if gathered is None:
            gathered = msg
        else:
            gathered = gathered + msg

gathered
```

```python
#í”„ë¡¬í”„íŠ¸ ì„¤ì •í•˜ê³  ê¸°ì‚¬ ì‘ì„±í•˜ê¸°
from langchain_core.messages import AIMessageChunk, SystemMessage

about = "ì„œìš¸ì›”ë“œì»µ ê²½ê¸°ì¥ ì”ë”” ë¬¸ì œ"

inputs = [SystemMessage(content=f"""
ë„ˆëŠ” ì‹ ë¬¸ê¸°ìì´ë‹¤. 
ìµœê·¼ {about}ì— ëŒ€í•´ ë¹„íŒí•˜ëŠ” ì‹¬ì¸µ ë¶„ì„ ê¸°ì‚¬ë¥¼ ì“°ë ¤ê³  í•œë‹¤.  

- ìµœê·¼ ì–´ë–¤ ì´ìŠˆê°€ ìˆëŠ”ì§€ ê²€ìƒ‰í•˜ê³ , ì‚¬ëŒë“¤ì´ ì œì¼ ê´€ì‹¬ìˆì–´ í• ë§Œí•œ ì£¼ì œë¥¼ ì„ ì •í•˜ê³ , ì™œ ì„ ì •í–ˆëŠ”ì§€ ë§í•´ì¤˜. 
- ê·¸ ë‚´ìš©ìœ¼ë¡œ ì›ê³ ë¥¼ ì‘ì„±í•˜ê¸° ìœ„í•œ ëª©ì°¨ë¥¼ ë§Œë“¤ê³ , ëª©ì°¨ ë‚´ìš©ì„ ì±„ìš°ê¸° ìœ„í•´ ì¶”ê°€ë¡œ ê²€ìƒ‰í•  ë‚´ìš©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•´ë´. 
- ê²€ìƒ‰í•  ë¦¬ìŠ¤íŠ¸ë¥¼ í† ëŒ€ë¡œ ì¬ê²€ìƒ‰ì„ í•œë‹¤. 
- ëª©ì°¨ì— ìˆëŠ” ë‚´ìš©ì„ ì‘ì„±í•˜ê¸° ìœ„í•´ ë” ê²€ìƒ‰ì´ í•„ìš”í•œ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ìˆë‹¤ë©´ ì¶”ê°€ë¡œ ê²€ìƒ‰í•´ë¼.
- ê²€ìƒ‰ëœ ê²°ê³¼ì— ì›í•˜ëŠ” ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì¬ê²€ìƒ‰í•´ë„ ì¢‹ë‹¤. 

ë” ì´ìƒ ê²€ìƒ‰í•  ë‚´ìš©ì´ ì—†ë‹¤ë©´, ì¡°ì„ ì¼ë³´ ì‹ ë¬¸ ê¸°ì‚¬ í˜•ì‹ìœ¼ë¡œ ìµœì¢… ê¸°ì‚¬ë¥¼ ì‘ì„±í•˜ë¼.
ì œëª©, ë¶€ì œ, ë¦¬ë“œë¬¸, ë³¸ë¬¸ ì˜ êµ¬ì„±ìœ¼ë¡œ ì‘ì„±í•˜ë¼. ë³¸ë¬¸ ë‚´ìš©ì€ ì‹¬ì¸µ ë¶„ì„ ê¸°ì‚¬ì— ë§ê²Œ êµ¬ì²´ì ì´ê³  ê¹Šì´ ìˆê²Œ ì‘ì„±í•´ì•¼ í•œë‹¤. 
    
""")]

for msg, metadata in graph.stream({"messages": inputs}, stream_mode="messages"):
    if isinstance(msg, AIMessageChunk):
        print(msg.content, end='')

```

### 13ì¥ ë­ê·¸ë˜í”„ë¥¼ í™œìš©í•œ ë©€í‹°ì—ì´ì „íŠ¸ RAG ë§Œë“¤ê¸°
13-1 ë­ê·¸ë˜í”„ ê¸°ë°˜ RAGë¥¼ ìœ„í•œ ì‚¬ì „ ì‘ì—…

ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œê³¼ ì •í™•í•œ ê°€ì´ë“œ

- í•˜ë‚˜ì˜ AI ì—ì´ì „íŠ¸ì—ê²Œ ëª¨ë“  ì‘ì—…ì„ ë§¡ê¸°ëŠ” ê²ƒë³´ë‹¤ ê°ê°ì˜ AI ì—ì´ì „íŠ¸ì—ê²Œ ë‹¨ìˆœí•œ ì—…ë¬´ ë‹¨ìœ„ë¡œ ëª…í™•í•œ ì§€ì‹œë¥¼ ë‚´ë ¤ì„œ ì›Œí¬ í”Œë¡œë¥¼ ë§Œë“¤ë©´ ê° ë¶„ì•¼ì˜ ì „ë¬¸ê°€ë“¤ì´ í˜‘ì—…í•˜ëŠ” ê²ƒì²˜ëŸ¼ ì‹œë„ˆì§€ë¥¼ ë‚¼ ìˆ˜ ìˆìŒ
- ë©€í‹°ì—ì´ì „íŠ¸:  AI ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—…ì„ ë¶„ë°°í•˜ê³  ì„œë¡œ í˜‘ë ¥í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ

RAGì˜ í•œê³„ ê°œì„ í•˜ê¸°

- ì¼ìƒì ì¸ ì§ˆë¬¸ì—ë„ ë¦¬íŠ¸ë¦¬ë²„ë¥¼ í™œìš©í•´ ê´€ë ¨ ë¬¸ì„œë“¤ì„ ì°¾ì•„ì™€ì„œ ë‹µë³€í•˜ë¯€ë¡œ ì‹œê°„ê³¼ í† í° ë¹„ìš© ë‚­ë¹„ ë°œìƒ
- ë­ê·¸ë˜í”„ë¥¼ ì´ìš©í•´ì„œ RAG ì‚¬ìš© ì—¬ë¶€ë¥¼ íŒë‹¨í•´ì„œ í•„ìš”í•  ë•Œë§Œ ê²€ìƒ‰í•˜ê²Œ í•¨

[ì‹¤ìŠµ] PDF ì „ì²˜ë¦¬í•˜ê³  ë²¡í„° DB ë§Œë“¤ê¸°

```python
from glob import glob 

for g in glob('../data/*.pdf'):
    print(g)
```

```python
# read_pdf_and_split_text í•¨ìˆ˜ ë§Œë“¤ê¸°
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

def read_pdf_and_split_text(pdf_path, chunk_size=1000, chunk_overlap=100):
    """
    ì£¼ì–´ì§„ PDF íŒŒì¼ì„ ì½ê³  í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
    ë§¤ê°œë³€ìˆ˜:
        pdf_path (str): PDF íŒŒì¼ì˜ ê²½ë¡œ.
        chunk_size (int, ì„ íƒì ): ê° í…ìŠ¤íŠ¸ ì²­í¬ì˜ í¬ê¸°. ê¸°ë³¸ê°’ì€ 1000ì…ë‹ˆë‹¤.
        chunk_overlap (int, ì„ íƒì ): ì²­í¬ ê°„ì˜ ì¤‘ì²© í¬ê¸°. ê¸°ë³¸ê°’ì€ 100ì…ë‹ˆë‹¤.
    ë°˜í™˜ê°’:
        list: ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ì˜ ë¦¬ìŠ¤íŠ¸.
    """
    print(f"PDF: {pdf_path} -----------------------------")

    pdf_loader = PyPDFLoader(pdf_path)
    data_from_pdf = pdf_loader.load()

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )

    splits = text_splitter.split_documents(data_from_pdf)
    
    print(f"Number of splits: {len(splits)}\n")
    return splits

```

```python
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
import os

##### Vectorstore ì„¤ì • #####
embedding = OpenAIEmbeddings(model='text-embedding-3-large')

persist_directory='../chroma_store'

if os.path.exists(persist_directory):
    print("Loading existing Chroma store")
    vectorstore = Chroma(
        persist_directory=persist_directory, 
        embedding_function=embedding
    )
else:
    print("Creating new Chroma store")
    
    vectorstore = None
    for g in glob('../data/*.pdf'):
        chunks = read_pdf_and_split_text(g)
        # 100ê°œì”© ë‚˜ëˆ ì„œ ì €ì¥
        for i in range(0, len(chunks), 100):
            if vectorstore is None:
                vectorstore = Chroma.from_documents(
                    documents=chunks[i:i+100],
                    embedding=embedding,
                    persist_directory=persist_directory
                )
            else:
                vectorstore.add_documents(
                    documents=chunks[i:i+100]
                )

```

```python
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

chunks = retriever.invoke("ì„œìš¸ ì˜¨ì‹¤ê°€ìŠ¤ ì €ê° ê³„íš")

for chunk in chunks:
    print(chunk.metadata)
    print(chunk.page_content)

```

```python
from langchain_openai import ChatOpenAI

# ëª¨ë¸ ì´ˆê¸°í™”
model = ChatOpenAI(model="gpt-4o-mini")
model.invoke('ì•ˆë…•í•˜ì„¸ìš”!')
```

13-2 ë¼ìš°í„° ì•Œì•„ë³´ê¸°

ë¼ìš°í„°

- ì…ë ¥í•œ ë‚´ìš©ì— ë”°ë¼ ì—¬ëŸ¬ê°œì˜ ì‹¤í–‰ ê²½ë¡œ ì¤‘ì—ì„œ ì ì ˆí•œ ê²½ë¡œë¥¼ ê²°ì •í•´ ë‹¤ìŒ ë…¸ë“œë¥¼ ì„ íƒí•˜ëŠ” ê¸°ëŠ¥ì„ í•¨
- ì–¸ì–´ ëª¨ë¸ì˜ ì‘ë‹µì´ë‚˜ íŠ¹ì • ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ë™ì‘í•´ì•¼ í•  ë•Œ ë¼ìš°í„°ë¥¼ ì‚¬ìš©í•¨

[ì‹¤ìŠµ]ì±—ë´‡ì— ë¼ìš°í„° ì„¤ì •í•˜ê¸°

```python
# Router ì„¤ì •
from langchain_core.prompts import ChatPromptTemplate
from typing import Literal # ë¬¸ìì—´ ë¦¬í„°ëŸ´ íƒ€ì…ì„ ì§€ì›í•˜ëŠ” typing ëª¨ë“ˆì˜ í´ë˜ìŠ¤
from pydantic import BaseModel, Field

# Data model
class RouteQuery(BaseModel):
    """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë°ì´í„° ì†ŒìŠ¤ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤."""
    
    datasource: Literal["vectorstore", "casual_talk"] = Field(
        ...,
        description="""
        ì‚¬ìš©ì ì§ˆë¬¸ì— ë”°ë¼ casual_talk ë˜ëŠ” vectorstoreë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
        - casual_talk: ì¼ìƒ ëŒ€í™”ë¥¼ ìœ„í•œ ë°ì´í„° ì†ŒìŠ¤. ì‚¬ìš©ìê°€ ì¼ìƒì ì¸ ì§ˆë¬¸ì„ í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        - vectorstore: ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ RAGë¡œ vectorstore ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° ì‚¬ìš©í•©ë‹ˆë‹¤.
        """,
    )
```

```python
# íŠ¹ì • ëª¨ë¸ì„ structured output (êµ¬ì¡°í™”ëœ ì¶œë ¥)ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì„¤ì •
structured_llm_router = model.with_structured_output(RouteQuery)

router_system = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ vectorstore ë˜ëŠ” casual_talkìœ¼ë¡œ ë¼ìš°íŒ…í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
- vectorstoreì—ëŠ” ì„œìš¸, ë‰´ìš•ì˜ ë°œì „ê³„íšê³¼ ê´€ë ¨ëœ ë¬¸ì„œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ì£¼ì œì— ëŒ€í•œ ì§ˆë¬¸ì—ëŠ” vectorstoreë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì¼ìƒ ëŒ€í™”ì— ê´€ë ¨ëœ ê²½ìš° casual_talkì„ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
"""

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ í¬í•¨í•˜ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
route_prompt = ChatPromptTemplate.from_messages([
    ("system", router_system),
    ("human", "{question}"),
])

# ë¼ìš°í„° í”„ë¡¬í”„íŠ¸ì™€ êµ¬ì¡°í™”ëœ ì¶œë ¥ ëª¨ë¸ì„ ê²°í•©í•œ ê°ì²´
question_router = route_prompt | structured_llm_router
```

```python
print(
    question_router.invoke({
        "question": "ì„œìš¸ ì˜¨ì‹¤ê°€ìŠ¤ ì €ê° ê³„íšì€ ë¬´ì—‡ì¸ê°€ìš”?"
    })
)

print(question_router.invoke({"question": "ì˜ ì§€ëƒˆì–´?"}))
```

13-3 ë­ê·¸ë˜í”„ë¡œ RAG ì—ì´ì „íŠ¸ ë§Œë“¤ê¸°

[ì‹¤ìŠµ] ê´€ë ¨ìˆëŠ” ì²­í¬ë§Œ í•„í„°ë§í•˜ê¸°

```python
from langchain_core.prompts import PromptTemplate

class GradeDocuments(BaseModel):
    """ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„± ìˆëŠ”ì§€ yes ë˜ëŠ” noë¡œ í‰ê°€í•©ë‹ˆë‹¤."""

    binary_score: Literal["yes", "no"] = Field(
        description="ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ 'yes' ë˜ëŠ” 'no'ë¡œ í‰ê°€í•©ë‹ˆë‹¤."
    )

structured_llm_grader = model.with_structured_output(GradeDocuments)
```

```python
grader_prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ëŠ” í‰ê°€ìì…ë‹ˆë‹¤. \n 
ë¬¸ì„œì— ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í‚¤ì›Œë“œ ë˜ëŠ” ì˜ë¯¸ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´, í•´ë‹¹ ë¬¸ì„œë¥¼ ê´€ë ¨ì„±ì´ ìˆë‹¤ê³  í‰ê°€í•˜ì‹­ì‹œì˜¤. \n
ì—„ê²©í•œ í…ŒìŠ¤íŠ¸ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëª©í‘œëŠ” ì˜ëª»ëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê±¸ëŸ¬ë‚´ëŠ” ê²ƒì…ë‹ˆë‹¤. \n
ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ 'yes' ë˜ëŠ” 'no'ë¡œ ì´ì§„ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì‹­ì‹œì˜¤.
                                             
Retrieved document: \n {document} \n\n 
User question: {question}
""")

retrieval_grader = grader_prompt | structured_llm_grader
question = "ì„œìš¸ì‹œ ììœ¨ì£¼í–‰ ê´€ë ¨ ê³„íš"
documents = retriever.invoke(question)

for doc in documents:
    print(doc)
```

```python
# ê´€ë ¨ëœ ì²­í¬ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ê¸°
filtered_docs = []

for i, doc in enumerate(documents):
    print(f"Document {i + 1}:")
    is_relevant = retrieval_grader.invoke({"question": question, "document": doc.page_content})
    print(is_relevant)
    print(doc.page_content[:200])
    print("=================================\n\n")

    if is_relevant.binary_score == "yes":
        filtered_docs.append(doc)

print(f"Filtered documents: {len(filtered_docs)}")
```

[ì‹¤ìŠµ] RAG ë‹µë³€ ìƒì„±í•˜ê¸°

```
### Generate
# PromptTemplateì„ ì‚¬ìš©í•˜ì—¬ RAGë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

rag_generate_system = """
ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ contextì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ëŠ” ë„ì‹œ ê³„íš ì „ë¬¸ê°€ì´ë‹¤. 
ì£¼ì–´ì§„ contextëŠ” vectorstoreì—ì„œ ê²€ìƒ‰ëœ ê²°ê³¼ì´ë‹¤. 
ì£¼ì–´ì§„ contextë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ questionì— ëŒ€í•´ ë‹µë³€í•˜ë¼.

=================================
question: {question}
context: {context}
"""

# PromptTemplateì„ ìƒì„±í•˜ì—¬ questionê³¼ contextë¥¼ í¬ë§·íŒ…
rag_prompt = PromptTemplate(
    input_variables=["question", "context"],
    template=rag_generate_system
)

# rag chain
rag_chain = rag_prompt | model 

# ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ RAGë¥¼ ì‹¤í–‰
question = "ì„œìš¸ì‹œ ììœ¨ì£¼í–‰ ê´€ë ¨ ê³„íš"

rag_chain.invoke({"question": question, "context": filtered_docs})
```

13-4 ê·¸ë˜í”„ ì •ì˜í•˜ê¸°

[ì‹¤ìŠµ] ê·¸ë˜í”„ ìƒíƒœ ì„ ì–¸í•˜ê³  ë…¸ë“œ ì •ì˜í•˜ê¸°

```python
from typing import List
from typing_extensions import TypedDict

class GraphState(TypedDict):
    question: str   # ì‚¬ìš©ì ì§ˆë¬¸
    generation: str # LLM ìƒì„± ê²°ê³¼
    documents: List[str] # ê²€ìƒ‰ëœ ë¬¸ì„œ
```

```python
def route_question(state): 
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ vectorstore ë˜ëŠ” casual_talkë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
    
    Args:
        state (dict): í˜„ì¬ graph state

    return:
        state (dict): ë¼ìš°íŒ…ëœ ë°ì´í„° ì†ŒìŠ¤ì™€ ì‚¬ìš©ì ì§ˆë¬¸ì„ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ graph state
    """
    print('------ROUTE------')
    question = state['question']
    route = question_router.invoke({"question": question})

    
    print(f"---Routing to {route.datasource}---")
    return route.datasource 
```

```python
def retrieve(state): 
    """
    vectorstoreì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        state (dict): í˜„ì¬ graph state

    return:
        state (dict): ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì‚¬ìš©ì ì§ˆë¬¸ì„ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ graph state
    """
    print('------RETRIEVE------')
    question = state['question']

    # Retrieve documents
    documents = retriever.invoke(question)
    return {"documents": documents, "question": question}
```

```python
def grade_documents(state):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í‰ê°€í•˜ì—¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

    Args:
        state (dict): í˜„ì¬ graph state

    return:
        state (dict): ê´€ë ¨ì„±ì´ ìˆëŠ” ë¬¸ì„œì™€ ì‚¬ìš©ì ì§ˆë¬¸ì„ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ graph state
    """
    print('------GRADE------')
    question = state['question']
    documents = state['documents']
    filtered_docs = []

    for i, doc in enumerate(documents):
        is_relevant = retrieval_grader.invoke({"question": question, "document": doc.page_content})
        if is_relevant.binary_score == "yes":
            filtered_docs.append(doc)
    return {"documents": filtered_docs, "question": question}  
```

```python
def generate(state):
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œì™€ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        state (dict): í˜„ì¬ graph state

    return:
        state (dict): LLM ìƒì„± ê²°ê³¼ì™€ ì‚¬ìš©ì ì§ˆë¬¸ì„ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ graph state
    """
    print('------GENERATE------')
    question = state['question']
    documents = state['documents']
    generation = rag_chain.invoke({"question": question, "context": documents})
    return {
        "documents": documents,
        "question": question,
        "generation": generation
    }
```

```python
def casual_talk(state):
    """
    ì¼ìƒ ëŒ€í™”ë¥¼ ìœ„í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        state (dict): í˜„ì¬ graph state

    return:
        state (dict): ì¼ìƒ ëŒ€í™” ê²°ê³¼ì™€ ì‚¬ìš©ì ì§ˆë¬¸ì„ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ graph state
    """
    print('------CASUAL TALK------')
    question = state['question']
    generation = model.invoke(question)
    return {
        "question": question,
        "generation": generation
    }
```

[ì‹¤ìŠµ] StateGraph ë§Œë“¤ê¸°

```python
from langgraph.graph import START, StateGraph, END

workflow = StateGraph(GraphState)
```

```python
# ë…¸ë“œë¥¼ ì •ì˜ 
workflow.add_node("retrieve", retrieve)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("generate", generate)
workflow.add_node("casual_talk", casual_talk)
```

```python
# graphë¥¼ ì •ì˜
workflow.add_conditional_edges(
    START, 
    route_question,
    {
        "vectorstore": "retrieve",
        "casual_talk": "casual_talk"
    }
)
workflow.add_edge("casual_talk", END)
workflow.add_edge("retrieve", "grade_documents")
workflow.add_edge("grade_documents", "generate")
workflow.add_edge("generate", END)

app = workflow.compile() # workflowë¥¼ ì»´íŒŒì¼
```

```python
from IPython.display import Image, display

try:
    display(Image(app.get_graph().draw_mermaid_png()))
except Exception:
    #  ì‹¤íŒ¨ ì‹œ í†µê³¼
    pass

```

[ì‹¤ìŠµ] ë©€í‹°ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸í•˜ê¸°

```python
inputs = {
    "question": "ì„œìš¸ì‹œ ììœ¨ì£¼í–‰ ê³„íš"
}

app.invoke(inputs) # workflowë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
```

```python
inputs = {
    "question": "ì˜ ì§€ë‚´ê³  ìˆì–´?"
}

app.invoke(inputs) # workflowë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
```

```python
inputs = {
    "question": "ì„œìš¸ì‹œì˜ ììœ¨ì£¼í–‰ ì°¨ëŸ‰ ê´€ë ¨ ê³„íšì€ ë¬´ì—‡ì´ ìˆë‚˜ìš”?"
}

for msg, meta in app.stream(inputs, stream_mode='messages'):
    print(msg.content, end='')

```

### 14ì¥ ë­ê·¸ë˜í”„ë¡œ ëª©ì°¨ë¥¼ ì‘ì„±í•˜ëŠ” ë©€í‹°ì—ì´ì „íŠ¸ ë§Œë“¤ê¸°
